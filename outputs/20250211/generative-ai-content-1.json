[
  {
    "title": "Connect AI agent tools to external services | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/external-connection-tools.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCreate custom tools\nStructured retrieval examples\nUnstructured retrieval examples\nConnect tools to external services\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  AI agent tools  Connect AI agent tools to external services\nConnect AI agent tools to external services\n\nFebruary 10, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nLearn how to connect AI agent tools to external applications like Slack, Google Calendar, or any service with an API using HTTP requests. Agents can use externally connected tools to automate tasks, send messages, and retrieve data from third-party platforms.\n\nTo learn more about agent tools, see AI agent tools.\n\nRequirements\n\nTo connect agent tools to external services, you must meet the following requirements:\n\nSetup authentication to the external service using one of the following methods:\n\nBearer token: Obtain a bearer token for simple token-based authentication.\n\nOAuth 2.0 Machine-to-Machine: Create and configure an app to enable machine-to-machine authentication.\n\nOAuth 2.0 User-to-Machine Shared: Authenticate with user interaction to share access between service identity and machine.\n\nYour workspace must be Unity Catalog enabled.\n\nYou must have network connectivity from a Databricks compute resource to the external service. See Networking recommendations for Lakehouse Federation.\n\nYou must use a compute with single-user access mode on Databricks Runtime 15.4 and above.\n\nYou must have a pro or serverless SQL warehouse. See SQL warehouse types.\n\nAuthentication methods for external services\n\nBearer token: A bearer token is a simple token-based authentication mechanism where a token is issued to a client and used to access resources without requiring additional credentials. The token is included in the request header and grants access as long as it is valid.\n\nOAuth Machine to Machine (recommended): OAuth Machine-to-Machine (M2M) authentication is used when two systems or applications communicate without direct user involvement. Tokens are issued to a registered machine client, which uses its own credentials to authenticate. This is ideal for server-to-server communication, microservices, and automation tasks where no user context is needed. Databricks recommends using OAuth Machine-to-Machine when it is available.\n\nOAuth User to Machine Shared: OAuth User-to-Machine Shared authentication allows a single user identity to authenticate and share the same set of credentials across multiple clients or users. All users share the same access token. This approach is suitable for shared devices or environments where a consistent user identity is sufficient, but it reduces individual accountability and tracking. In cases where identity login is required, select User to Machine Shared.\n\nCreate a connection to the external service\n\nFirst, create a Unity Catalog Connection to the external service that specifies a path and credentials to access the service.\n\nBenefits of using a Unity Catalog Connection include the following:\n\nSecure credential management: Secrets and tokens are securely stored and managed in Unity Catalog, ensuring they are never exposed to users.\n\nGranular access control: Unity Catalog allows fine-grained control over who can use or manage connections with the USE_CONNECTION and MANAGE_CONNECTION privileges.\n\nHost-specific token enforcement: Tokens are restricted to the host_name specified during connection creation, ensuring they cannot be used with unauthorized hosts.\n\nPermissions required: Metastore admin or user with the CREATE CONNECTION privilege.\n\nCreate a connection using one of the following methods:\n\nUse the Catalog Explorer UI.\n\nExecute the CREATE CONNECTION SQL command in a Databricks notebook or the Databricks SQL query editor.\n\nUse the Databricks REST API or the Databricks CLI to create a connection. See POST /api/2.1/unity-catalog/connections and Unity Catalog commands.\n\nCatalog Explorer\nSQL\n\nUse the Catalog Explorer UI to create a connection.\n\nIn your Databricks workspace, click  Catalog.\n\nAt the top of the Catalog pane, click the  Add icon and select Add a connection from the menu.\n\nAlternatively, from the Quick access page, click the External data > button, go to the Connections tab, and click Create connection.\n\nClick Create connection.\n\nEnter a user-friendly Connection name.\n\nSelect a Connection type of HTTP.\n\nSelect an Auth type from the following options:\n\nBearer token\n\nOAuth Machine to Machine\n\nOAuth User to Machine Shared\n\nOn the Authentication page, enter the following connection properties for the HTTP connection.\n\nFor a bearer token:\n\nHost: For example, https://databricks.com\n\nPort: For example, 443\n\nBearer Token: For example, bearer-token\n\nBase Path: For example, /api/\n\nFor OAuth Machine to Machine token:\n\nClient ID: Unique identifier for the application you created.\n\nClient secret: Secret or password generated for the application that you created.\n\nOAuth scope: Scope to grant during user authorization. The scope parameter is expressed as a list of space-delimited, case-sensitive strings. For example, channels:read channels:history chat:write\n\nToken endpoint: Used by the client to obtain an access token by presenting its authorization grant or refresh token. Usually in the format https://authorization-server.com/oauth/token\n\nFor OAuth User to Machine Shared token:\n\nClient ID: Unique identifier for the application you created.\n\nClient secret: Secret or password generated for the application that you created.\n\nOAuth scope: Scope to grant during user authorization. The scope parameter is expressed as a list of space-delimited, case-sensitive strings. For example, channels:read channels:history chat:write\n\nAuthorization endpoint: To authenticate with the resource owner via user-agent redirection, usually in the format https://authorization-server.com/oauth/authorize\n\nToken endpoint: Used by the client to obtain an access token by presenting its authorization grant or refresh token. Usually in the format https://authorization-server.com/oauth/token\n\nNote\n\nFor OAuth User to Machine Shared, you are prompted to sign in with HTTP using your OAuth credentials.\n\nClick Create connection.\n\nSend an HTTP request to the external system\n\nNow that you have a connection, learn how to send HTTP requests to the service using the http_request built-in SQL function.\n\nPermissions required: USE CONNECTION on the connection object.\n\nRun the following SQL command in a notebook or the Databricks SQL editor. Replace the placeholder values:\n\nCopy\nSQL\nSELECT http_request(\n  conn => <connection-name>,\n  method => <http-method>,\n  path => <path>,\n  json => to_json(named_struct(\n    'text', text\n  )),\n  headers => map(\n    'Accept', \"application/vnd.github+json\"\n  )\n);\n\n\nconnection-name: The connection object that specifies the host, port, base_path, and access credentials.\n\nhttp-method: The HTTP request method used to make the call. For example: GET, POST, PUT, DELETE\n\npath: The path to concatenate after the base_path to invoke the service resource.\n\njson: The JSON body to send with the request.\n\nheaders: A map to specify the request headers.\n\nCreate a Unity Catalog function tool\n\nAfter validating that the connection works properly, create a Unity Catalog function that uses the connection. The following example creates a Unity Catalog function tool that an agent can use to post a message to Slack:\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.slack_post_message(\n  text STRING COMMENT 'message content'\n)\nRETURNS STRING\nCOMMENT 'Sends a Slack message by passing in the message and returns the response received from the external service.'\nRETURN (http_request(\n  conn => 'test_sql_slack',\n  method => 'POST',\n  path => '/api/chat.postMessage',\n  json => to_json(named_struct(\n    'channel', \"C032G2DAH3\",\n    'text', text\n  ))\n)).text\n\nCreate a tool in agent code\n\nTo send HTTP requests to external services with Python, use the http_request function from the databricks-sdk library. This function sends an HTTP request to an external service using a Unity Catalog connection to authenticate.\n\nPermissions required: USE CONNECTION on the connection object.\n\nThe following example makes an external HTTP request from inside agent code.\n\nCopy\nPython\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.serving import ExternalFunctionRequestHttpMethod\n\n\nWorkspaceClient().serving_endpoints.http_request(\n  conn=\"connection_name\",\n  method=ExternalFunctionRequestHttpMethod.POST,\n  path=\"/api/v1/resource\",\n  json={\"key\": \"value\"},\n  headers={\"extra_header_key\": \"extra_header_value\"},\n)\n\n\nconn: The connection object that specifies the host, port, base_path, and access credentials.\n\nmethod: The HTTP request method used to make the call. For example: GET, POST, PUT, DELETE\n\npath: The path to concatenate after the base_path to invoke the service resource.\n\njson: The JSON body to send with the request.\n\nheaders: A Map to specify the request headers.\n\nExample notebooks\n\nThe following notebooks demonstrate creating AI agent tools that connect to Slack, OpenAI, and Azure AI search.\n\nSlack messaging agent tool\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nMicrosoft Graph API agent tool\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nAzure AI Search agent tool\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nAuthentication methods for external services\nCreate a connection to the external service\nSend an HTTP request to the external system\nCreate a Unity Catalog function tool\nCreate a tool in agent code\nExample notebooks\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Unstructured retrieval AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/unstructured-retrieval-tools.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCreate custom tools\nStructured retrieval examples\nUnstructured retrieval examples\nConnect tools to external services\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  AI agent tools  Unstructured retrieval AI agent tools\nUnstructured retrieval AI agent tools\n\nJanuary 31, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to create AI agent tools for unstructured data retrieval using the Mosaic AI Agent Framework. Unstructured retrievers enable agents to query unstructured data sources, such as a document corpus, using vector search indexes.\n\nTo learn more about agent tools, see AI agent tools.\n\nLocally develop Vector Search retriever tools with AI Bridge\n\nThe easiest way to start developing a Databricks Vector Search retriever tool is locally. Use Databricks AI Bridge packages like databricks-langchain and databricks-openai to quickly add retrieval capabilities to an agent and experiment with query parameters. This approach enables fast iteration during initial development.\n\nOnce your local tool is ready, you can directly productionize it as part of your agent code, or migrate it to a Unity Catalog function, which provides better discoverability and governance but has certain limitations. See Vector Search retriever tool with Unity Catalog functions.\n\nLangChain/LangGraph\nOpenAI\n\nThe following code prototypes a retriever tool and binds it to an LLM locally so you can chat with the agent to test its tool-calling behavior.\n\nInstall the latest version of databricks-langchain which includes Databricks AI Bridge.\n\nCopy\n%pip install --upgrade databricks-langchain\n\n\nNote\n\nWhen initializing the VectorSearchRetrieverTool, the text_column and embedding arguments are required for Delta Sync Indexes with self-managed embeddings and Direct Vector Access Indexes. See options for providing embeddings.\n\nCopy\nfrom databricks_langchain import VectorSearchRetrieverTool, ChatDatabricks\n\n# Initialize the retriever tool\nvs_tool = VectorSearchRetrieverTool(index_name=\"catalog.schema.my_index_name\")\n\n# Run a query against the vector search index locally for testing\nvs_tool.invoke(\"Databricks Agent Framework?\")\n\n# Bind the retriever tool to your Langchain LLM of choice\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\nllm_with_tools = llm.bind_tools([vs_tool])\n\n# Chat with your LLM to test the tool calling functionality\nllm_with_tools.invoke(\"Based on the Databricks documentation, what is Databricks Agent Framework?\")\n\n\nTo customize tool calling, pass additional arguments to VectorSearchRetrieverTool:\n\nCopy\nfrom databricks_langchain import VectorSearchRetrieverTool\n\nvs_tool = VectorSearchRetrieverTool(\n  index_name, # Index name in the format 'catalog.schema.index'\n  num_results, # Max number of documents to return\n  columns, # List of columns to include in the search\n  filters, # Filters to apply to the query\n  query_type, # Query type (\"ANN\" or \"HYBRID\").\n  tool_name, # Used by the LLM to understand the purpose of the tool\n  tool_description, # Used by the LLM to understand the purpose of the tool\n  text_column, # Specify text column for embeddings. Required for direct-access index or delta-sync index with self-managed embeddings.\n  embedding # The embedding model. Required for direct-access index or delta-sync index with self-managed embeddings.\n)\n\nVector Search retriever tool with Unity Catalog functions\n\nThe following example creates retriever tool using a Unity Catalog function to query data from a Mosaic AI Vector Search index.\n\nThe Unity Catalog function databricks_docs_vector_search queries a hypothetical Vector Search index containing Databricks documentation. This function wraps the Databricks SQL function vector_search() and aligns its output with the MLflow retriever schema. by using the page_content and metadata aliases.\n\nNote\n\nTo conform to the MLflow retriever schema, any additional metadata columns must be added to the metadata column using the SQL map function, rather than as top-level output keys.\n\nRun the following code in a notebook or SQL editor to create the function:\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.databricks_docs_vector_search (\n  -- The agent uses this comment to determine how to generate the query string parameter.\n  query STRING\n  COMMENT 'The query string for searching Databricks documentation.'\n) RETURNS TABLE\n-- The agent uses this comment to determine when to call this tool. It describes the types of documents and information contained within the index.\nCOMMENT 'Executes a search on Databricks documentation to retrieve text documents most relevant to the input query.' RETURN\nSELECT\n  chunked_text as page_content,\n  map('doc_uri', url, 'chunk_id', chunk_id) as metadata\nFROM\n  vector_search(\n    -- Specify your Vector Search index name here\n    index => 'catalog.schema.databricks_docs_index',\n    query => query,\n    num_results => 5\n  )\n\n\nTo use this retriever tool in your AI agent, wrap it with UCFunctionToolkit. This enables automatic tracing through MLflow.\n\nMLflow Tracing captures detailed execution information for gen AI applications. It logs inputs, outputs, and metadata for each step, helping you debug issues and analyze performance.\n\nWhen using UCFunctionToolkit, retrievers automatically generate RETRIEVER span types in MLflow logs if their output conforms to the MLflow retriever schema. See MLflow Tracing Schema.\n\nFor more information about UCFunctionToolkit see Unity Catalog documentation.\n\nCopy\nPython\nfrom unitycatalog.ai.langchain.toolkit import UCFunctionToolkit\n\ntoolkit = UCFunctionToolkit(\n    function_names=[\n        \"main.default.databricks_docs_vector_search\"\n    ]\n)\ntools = toolkit.tools\n\n\nThis retriever tool has the following caveats:\n\nSQL clients may limit the maximum number of rows or bytes returned. To prevent data truncation, you should truncate column values returned by the UDF. For example, you could use substring(chunked_text, 0, 8192) to reduce the size of large content columns and avoid row truncation during execution.\n\nSince this tool is a wrapper for the vector_search() function, it is subject to the same limitations as the vector_search() function. See Limitations.\n\nIf this example is unsuitable for your use case, create a vector search retriever tool using custom agent code instead.\n\nVector Search retriever with agent code (PyFunc)\n\nThe following example creates a Vector Search retriever for a PyFunc-flavored agent in agent code.\n\nThis example uses databricks-vectorsearch to create a basic retriever that performs a Vector Search similarity search with filters. It uses MLflow decorators to enable agent tracing.\n\nNote\n\nTo conform to the MLflow retriever schema, the retriever function should return a Document type and use the metadata field in the Document class to add additional attributes to the returned document, like like doc_uri and similarity_score.\n\nUse the following code in the agent module or agent notebook.\n\nCopy\nPython\nimport mlflow\nimport json\n\nfrom mlflow.entities import Document\nfrom typing import List, Dict, Any\nfrom dataclasses import asdict\nfrom databricks.vector_search.client import VectorSearchClient\n\nclass VectorSearchRetriever:\n    \"\"\"\n    Class using Databricks Vector Search to retrieve relevant documents.\n    \"\"\"\n    def __init__(self):\n        self.vector_search_client = VectorSearchClient(disable_notice=True)\n        # TODO: Replace this with the list of column names to return in the result when querying Vector Search\n        self.columns = [\"chunk_id\", \"text_column\", \"doc_uri\"]\n        self.vector_search_index = self.vector_search_client.get_index(\n            index_name=\"catalog.schema.chunked_docs_index\"\n        )\n        mlflow.models.set_retriever_schema(\n            name=\"vector_search\",\n            primary_key=\"chunk_id\",\n            text_column=\"text_column\",\n            doc_uri=\"doc_uri\"\n        )\n\n    @mlflow.trace(span_type=\"RETRIEVER\", name=\"vector_search\")\n    def __call__(\n        self,\n        query: str,\n        filters: Dict[Any, Any] = None,\n        score_threshold = None\n    ) -> List[Document]:\n        \"\"\"\n        Performs vector search to retrieve relevant chunks.\n        Args:\n            query: Search query.\n            filters: Optional filters to apply to the search. Filters must follow the Databricks Vector Search filter spec\n            score_threshold: Score threshold to use for the query.\n\n        Returns:\n            List of retrieved Documents.\n        \"\"\"\n\n        results = self.vector_search_index.similarity_search(\n            query_text=query,\n            columns=self.columns,\n            filters=filters,\n            num_results=5,\n            query_type=\"ann\"\n        )\n\n        documents = self.convert_vector_search_to_documents(\n            results, score_threshold\n        )\n        return [asdict(doc) for doc in documents]\n\n    @mlflow.trace(span_type=\"PARSER\")\n    def convert_vector_search_to_documents(\n        self, vs_results, score_threshold\n    ) -> List[Document]:\n\n        docs = []\n        column_names = [column[\"name\"] for column in vs_results.get(\"manifest\", {}).get(\"columns\", [])]\n        result_row_count = vs_results.get(\"result\", {}).get(\"row_count\", 0)\n\n        if result_row_count > 0:\n            for item in vs_results[\"result\"][\"data_array\"]:\n                metadata = {}\n                score = item[-1]\n\n                if score >= score_threshold:\n                    metadata[\"similarity_score\"] = score\n                    for i, field in enumerate(item[:-1]):\n                        metadata[column_names[i]] = field\n\n                    page_content = metadata.pop(\"text_column\", None)\n\n                    if page_content:\n                        doc = Document(\n                            page_content=page_content,\n                            metadata=metadata\n                        )\n                        docs.append(doc)\n\n        return docs\n\n\nTo run the retriever, run the following Python code. You can optionally include Vector Search filters in the request to filter results.\n\nCopy\nPython\nretriever = VectorSearchRetriever()\nquery = \"What is Databricks?\"\nfilters={\"text_column LIKE\": \"Databricks\"},\nresults = retriever(query, filters=filters, score_threshold=0.1)\n\nSet retriever schema\n\nTo ensure that retrievers are traced properly and render correctly in downstream applications, call mlflow.models.set_retriever_schema when you define your agent. Use set_retriever_schema to map the column names in the returned table to MLflow’s expected fields such as primary_key, text_column, and doc_uri.\n\nCopy\nPython\n# Define the retriever's schema by providing your column names\nmlflow.models.set_retriever_schema(\n    name=\"vector_search\",\n    primary_key=\"chunk_id\",\n    text_column=\"text_column\",\n    doc_uri=\"doc_uri\"\n    # other_columns=[\"column1\", \"column2\"],\n)\n\n\nYou can also specify additional columns in your retriever’s schema by providing a list of column names with the other_columns field.\n\nIf you have multiple retrievers, you can define multiple schemas by using unique names for each retriever schema.\n\nThe retriever schema set during agent creation affects downstream applications and workflows, such as the review app and evaluation sets. Specifically, the doc_uri column serves as the primary identifier for documents returned by the retriever.\n\nThe review app displays the doc_uri to help reviewers assess responses and trace document origins. See Review App UI.\n\nEvaluation sets use doc_uri to compare retriever results against predefined evaluation datasets to determine the retriever’s recall and precision. See Evaluation sets.\n\nTrace the retriever\n\nMLflow tracing adds observability by capturing detailed information about your agent’s execution. It provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to pinpoint the source of bugs and unexpected behaviors quickly.\n\nThis example uses the @mlflow.trace decorator to create a trace for the retriever and parser. For other options for setting up trace methods, see MLflow Tracing for agents.\n\nThe decorator creates a span that starts when the function is invoked and ends when it returns. MLflow automatically records the function’s input and output and any exceptions raised.\n\nNote\n\nLangChain, LlamaIndex, and OpenAI library users can use MLflow auto logging instead of manually defining traces with the decorator. See Use autologging to add traces to your agents.\n\nCopy\nPython\n...\n@mlflow.trace(span_type=\"RETRIEVER\", name=\"vector_search\")\ndef __call__(self, query: str) -> List[Document]:\n  ...\n\n\nTo ensure downstream applications such as Agent Evaluation and the AI Playground render the retriever trace correctly, make sure the decorator meets the following requirements:\n\nUse span_type=\"RETRIEVER\" and ensure the function returns List[Document] object. See Retriever spans.\n\nThe trace name and the retriever_schema name must match to configure the trace correctly.\n\nFilter Vector Search results\n\nYou can limit the search scope to a subset of data using a Vector Search filter.\n\nThe filters parameter in VectorSearchRetriever defines the filter conditions using the Databricks Vector Search filter specification.\n\nCopy\nPython\nfilters = {\"text_column LIKE\": \"Databricks\"}\n\n\nInside the __call__ method, the filters dictionary is passed directly to the similarity_search function:\n\nCopy\nPython\nresults = self.vector_search_index.similarity_search(\n    query_text=query,\n    columns=self.columns,\n    filters=filters,\n    num_results=5,\n    query_type=\"ann\"\n)\n\n\nAfter initial filtering, the score_threshold parameter provides additional filtering by setting a minimum similarity score.\n\nCopy\nPython\nif score >= score_threshold:\n    metadata[\"similarity_score\"] = score\n\n\nThe final result includes documents that meet the filters and score_threshold conditions.\n\nNext steps\n\nAfter you create a Unity Catalog function agent tool, add the tool to an AI agent. See Add Unity Catalog tools to agents.\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nLocally develop Vector Search retriever tools with AI Bridge\nVector Search retriever tool with Unity Catalog functions\nVector Search retriever with agent code (PyFunc)\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Structured retrieval AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/structured-retrieval-tools.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCreate custom tools\nStructured retrieval examples\nUnstructured retrieval examples\nConnect tools to external services\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  AI agent tools  Structured retrieval AI agent tools\nStructured retrieval AI agent tools\n\nDecember 13, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to create AI agents for structured data retrieval using the Mosaic AI Agent Framework. Structured retrievers enable agents to query structured data sources such as SQL tables.\n\nTo learn more about agent tools, see AI agent tools.\n\nTable query tool\n\nThe following example creates a tool that allows an agent to query structured customer data from a Unity Catalog table.\n\nIt defines a UC function called lookup_customer_info, which allows an AI agent to retrieve structured data from a hypothetical customer_data table.\n\nRun the following code in a SQL editor.\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.lookup_customer_info(\n  customer_name STRING COMMENT 'Name of the customer whose info to look up'\n)\nRETURNS STRING\nCOMMENT 'Returns metadata about a particular customer given the customer name, including the customer's email and ID. The\ncustomer ID can be used for other queries.'\nRETURN SELECT CONCAT(\n    'Customer ID: ', customer_id, ', ',\n    'Customer Email: ', customer_email\n  )\n  FROM main.default.customer_data\n  WHERE customer_name = customer_name\n  LIMIT 1;\n\nNext steps\n\nAfter you create an agent tool, add the tool to an AI agent. See Add Unity Catalog tools to agents.\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nTable query tool\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Legacy input and output agent schema | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/agent-legacy-schema.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAuthor agents using Python\nPrototype agents in AI Playground\nLegacy input and output schema\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent  Legacy input and output agent schema\nLegacy input and output agent schema\n\nJanuary 24, 2025\n\nNote\n\nThe SplitChatMessageRequest input schema and StringResponse output schema have been deprecated. If you are using either of these legacy schemas, Databricks recommends that you migrate to the recommended ChatModel schema. See Author AI agents in code.\n\nAI agents must adhere to specific input and output schema requirements to be compatible with other features on Databricks. This article explains how to use the SplitChatMessageRequest input schema and StringResponse output schema to ensure compatibility with Databricks features.\n\nSplitChatMessageRequest input schema (deprecated)\n\nSplitChatMessagesRequest allows you to pass the current query and history separately as agent input.\n\nCopy\nPython\n  question = {\n      \"query\": \"What is MLflow\",\n      \"history\": [\n          {\n              \"role\": \"user\",\n              \"content\": \"What is Retrieval-augmented Generation?\"\n          },\n          {\n              \"role\": \"assistant\",\n              \"content\": \"RAG is\"\n          }\n      ]\n  }\n\nStringResponse output schema (deprecated)\n\nStringResponse allows you to return the agent’s response as an object with a single string content field:\n\nCopy\n{\"content\": \"This is an example string response\"}\n\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nSplitChatMessageRequest input schema (deprecated)\nStringResponse output schema (deprecated)\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Create custom AI agent tools with Unity Catalog functions | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/create-custom-tool.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCreate custom tools\nStructured retrieval examples\nUnstructured retrieval examples\nConnect tools to external services\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  AI agent tools  Create custom AI agent tools with Unity Catalog functions\nCreate custom AI agent tools with Unity Catalog functions\n\nJanuary 31, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nLearn how to create AI agent tools using Unity Catalog functions to execute custom Python code for task-specific operations.\n\nWhat is UCFunctionToolkit\n\nDatabricks recommends using UCFunctionToolkit to create and use agent tools defined with Unity Catalog functions.\n\nUCFunctionToolkit is a key component of the Unity Catalog AI Core Library that simplifies the process of creating and using Unity Catalog functions as tools in AI frameworks like LangChain. It serves as a bridge between Unity Catalog functions and AI agents.\n\nOnce set up, you can create Unity Catalog functions, wrap them with UCFunctionToolkit, and use them as tools in your AI agents. This abstraction allows for consistent tool creation across authoring libraries and enables features like auto-tracing of retriever functions, making it easier to view retrieved documents and related data.\n\nSee Unity Catalog’s AI Core library quickstart for more information and examples.\n\nDefine a Unity Catalog function\n\nTo use UCFunctionToolkit, you must install the Unity Catalog AI Core Library and any necessary integration packages.\n\nCopy\nPython\n## Install UCToolkit for LangChain with the Databricks extra\n# Install the core package for databricks\n%pip install unitycatalog-ai[databricks]\n\n# Install the LangChain integration\n%pip install unitycatalog-langchain[databricks]\n\n\nNote\n\nThis example uses LangChain, you must install the correct package for your agent library. For example, OpenAI agents should use %pip install unitycatalog-openai[databricks]. See Unity Catalog AI integrations.\n\nCreate a Unity Catalog function containing the tool’s logic.\n\nCopy\nPython\nfrom unitycatalog.ai.core.databricks import DatabricksFunctionClient\n\n\nCATALOG = \"my_catalog\"\nSCHEMA = \"my_schema\"\n\ndef add_numbers(number_1: float, number_2: float) -> float:\n  \"\"\"\n  A function that accepts two floating point numbers adds them,\n  and returns the resulting sum as a float.\n\n  Args:\n      number_1 (float): The first of the two numbers to add.\n      number_2 (float): The second of the two numbers to add.\n\n  Returns:\n      float: The sum of the two input numbers.\n  \"\"\"\n  return number_1 + number_2\n\nfunction_info = client.create_python_function(\n  func=add_numbers,\n  catalog=CATALOG,\n  schema=SCHEMA,\n)\n\nUse the function as a tool\n\nOnce you’ve created a function, you can make it accessible to AI authoring libraries by wrapping it into a toolkit. The following example uses LangChain, but a similar approach can be applied to other frameworks:\n\nCopy\nPython\nfrom unitycatalog.ai.langchain.toolkit import UCFunctionToolkit\n\n# Define the UC function to be used as a tool\nfunc_name = f\"{CATALOG}.{SCHEMA}.add_numbers\"\n\n# Create a toolkit with the UC function\ntoolkit = UCFunctionToolkit(function_names=[func_name], client=client)\n\ntools = toolkit.tools\n\n\nFor more information on UCFunctionToolKit, see Unity Catalog documentation.\n\nBuilt-in Python executor tool\n\nDatabricks also provides a built-in Unity Catalog function allowing AI agents to dynamically execute Python code within queries dynamically.\n\nsystem.ai.python_exec is available by default and can be used like any other Unity Catalog function-based tool.\n\nThe following example shows you how to use the python_exec function in a SQL query:\n\nCopy\nSQL\nSELECT python_exec(\"\"\"\nimport random\nnumbers = [random.random() for _ in range(10)]\nprint(numbers)\n\"\"\")\n\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat is UCFunctionToolkit\nDefine a Unity Catalog function\nUse the function as a tool\nBuilt-in Python executor tool\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Prototype tool-calling agents in AI Playground | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/ai-playground-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAuthor agents using Python\nPrototype agents in AI Playground\nLegacy input and output schema\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent  Prototype tool-calling agents in AI Playground\nPrototype tool-calling agents in AI Playground\n\nJanuary 24, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to prototype a tool-calling AI agent with the AI Playground.\n\nUse the AI Playground quickly create a tool-calling agent and chat with it live to see how it behaves. Then, export the agent for deployment or further development in Python code.\n\nTo author agents using a code-first approach, see Author AI agents in code.\n\nRequirements\n\nYour workspace must have the following features enabled to prototype agents using AI Playground:\n\nUnity Catalog\n\nServerless compute\n\nMosaic AI Agent Framework\n\nEither pay-per-token foundation models or external models. See Model serving feature availability\n\nPrototype tool-calling agents in AI Playground\n\nTo prototype a tool-calling agent:\n\nFrom Playground, select a model with the Tools enabled label.\n\nSelect Tools and select a tool to give to the agent. For this guide, select the built-in Unity Catalog function, system.ai.python_exec. This function gives your agent the ability to run arbitrary Python code. To learn how to create agent tools, see AI agent tools.\n\nChat to test out the current combination of LLM, tools, and system prompt and try variations.\n\nExport and deploy AI Playground agents\n\nAfter prototyping the AI agent in AI Playground, export it to Python notebooks to deploy it to a model serving endpoint.\n\nClick Export to generate Python notebooks that define and deploy the AI agent.\n\nAfter exporting the agent code, three files are saved to your workspace. These files follow MLflow’s Models from Code methodology, which defines agents directly in code rather than relying on serialized artifacts. To learn more, see MLflow’s Model from Code Guide:\n\nagent notebook: Contains Python code defining your agent using LangChain.\n\ndriver notebook: Contains Python code to log, trace, register, and deploy the AI agent using Mosaic AI Agent Framework.\n\nconfig.yml: Contains configuration information about your agent, including tool definitions.\n\nOpen the agent notebook to see the LangChain code defining your agent.\n\nRun driver notebook to log and deploy your agent to a Model Serving endpoint.\n\nNote\n\nThe exported code might behave differently from your AI Playground session. Databricks recommends running the exported notebooks to iterate and debug further, evaluate agent quality, and then deploy the agent to share with others.\n\nDevelop agents in code\n\nUse the exported notebooks to test and iterate programmatically. Use the notebook to do things like add tools or adjust the agent’s parameters.\n\nWhen developing programmatically, agents must meet specific requirements to be compatible with other Databricks agent features. To learn how to author agents using a code-first approach, see Author AI agents in code\n\nNext steps\n\nCreate your own agent tools.\n\nLog an AI agent.\n\nAdd traces to an AI agent.\n\nDeploy an AI agent.\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nPrototype tool-calling agents in AI Playground\nExport and deploy AI Playground agents\nDevelop agents in code\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Author AI agents in code | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/create-chat-model.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAuthor agents using Python\nPrototype agents in AI Playground\nLegacy input and output schema\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent  Author AI agents in code\nAuthor AI agents in code\n\nJanuary 31, 2025\n\nThis article shows how to author an AI agent in code, using MLflow ChatModel. Databricks leverages MLflow ChatModel to ensure compatibility with Databricks AI agent features like evaluation, tracing, and deployment.\n\nWhat is ChatModel?\n\nChatModel is an MLflow class designed to simplify the creation of conversational AI agents. It provides a standardized interface for building models compatible with OpenAI’s ChatCompletion API.\n\nChatModel extends OpenAI’s ChatCompletion schema. This approach allows you to maintain broad compatibility with platforms supporting the ChatCompletion standard, while also adding your own custom functionality.\n\nBy using ChatModel, developers can create agents that are compatible with Databricks and MLflow tools for agent tracking, evaluation, and lifecycle management, which are essential for deploying production-ready models.\n\nSee MLflow: Getting Started with ChatModel.\n\nRequirements\n\nDatabricks recommends installing the latest version of the MLflow Python client when developing agents.\n\nTo author and deploy agents using the approach in this article, you must meet the following requirements:\n\nInstall databricks-agents version 0.15.0 and above\n\nInstall mlflow version 2.20.0 and above\n\nCopy\nBash\n%pip install -U -qqqq databricks-agents>=0.15.0 mlflow>=2.20.0\n\nCreate a ChatModel agent\n\nYou can author your agent as a subclass of mlflow.pyfunc.ChatModel. This method provides the following benefits:\n\nAllows you to write agent code compatible with the ChatCompletion schema using typed Python classes.\n\nMLflow will automatically infer a chat completion-compatible signature when logging the agent, even without an input_example. This simplifies the process of registering and deploying the agent. See Infer Model Signature during logging.\n\nThe following code is best executed in a Databricks notebook. Notebooks provide a convenient environment for developing, testing, and iterating on your agent.\n\nThe MyAgent class extends mlflow.pyfunc.ChatModel, implementing the required predict method. This ensures compatibility with Mosaic AI Agent Framework.\n\nThe class also includes the optional methods _create_chat_completion_chunk and predict_stream to handle streaming outputs.\n\nCopy\nPython\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, List, Generator\nfrom mlflow.pyfunc import ChatModel\nfrom mlflow.types.llm import (\n    # Non-streaming helper classes\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ChatCompletionChunk,\n    ChatMessage,\n    ChatChoice,\n    ChatParams,\n    # Helper classes for streaming agent output\n    ChatChoiceDelta,\n    ChatChunkChoice,\n)\n\nclass MyAgent(ChatModel):\n    \"\"\"\n    Defines a custom agent that processes ChatCompletionRequests\n    and returns ChatCompletionResponses.\n    \"\"\"\n    def predict(self, context, messages: list[ChatMessage], params: ChatParams) -> ChatCompletionResponse:\n        last_user_question_text = messages[-1].content\n        response_message = ChatMessage(\n            role=\"assistant\",\n            content=(\n                f\"I will always echo back your last question. Your last question was: {last_user_question_text}. \"\n            )\n        )\n        return ChatCompletionResponse(\n            choices=[ChatChoice(message=response_message)]\n        )\n\n    def _create_chat_completion_chunk(self, content) -> ChatCompletionChunk:\n        \"\"\"Helper for constructing a ChatCompletionChunk instance for wrapping streaming agent output\"\"\"\n        return ChatCompletionChunk(\n                choices=[ChatChunkChoice(\n                    delta=ChatChoiceDelta(\n                        role=\"assistant\",\n                        content=content\n                    )\n                )]\n            )\n\n    def predict_stream(\n        self, context, messages: List[ChatMessage], params: ChatParams\n    ) -> Generator[ChatCompletionChunk, None, None]:\n        last_user_question_text = messages[-1].content\n        yield self._create_chat_completion_chunk(f\"Echoing back your last question, word by word.\")\n        for word in last_user_question_text.split(\" \"):\n            yield self._create_chat_completion_chunk(word)\n\nagent = MyAgent()\nmodel_input = ChatCompletionRequest(\n    messages=[ChatMessage(role=\"user\", content=\"What is Databricks?\")]\n)\nresponse = agent.predict(context=None, model_input=model_input)\nprint(response)\n\n\nWhile the agent class MyAgent is defined in one notebook, you should create a separate driver notebook. The driver notebook logs the agent to Model Registry and deploys the agent using Model Serving.\n\nThis separation follows Databricks’ recommended workflow for logging models using MLflow’s Models from Code methodology.\n\nExample: Wrap LangChain in ChatModel\n\nIf you have an existing LangChain model and want to integrate it with other Mosaic AI agent features, you can wrap it in an MLflow ChatModel to ensure compatibility.\n\nThis code sample does the following steps to wrap a LangChain runnable as a ChatModel:\n\nWrap the final output of the LangChain with mlflow.langchain.output_parsers.ChatCompletionOutputParser to produce a chat completion output signature\n\nThe LangchainAgent class extends mlflow.pyfunc.ChatModel and implements two key methods:\n\npredict: Handles synchronous predictions by invoking the chain and returning a formatted response.\n\npredict_stream: Handles streaming predictions by invoking the chain and yielding chunks of responses.\n\nCopy\nPython\nfrom mlflow.langchain.output_parsers import ChatCompletionOutputParser\nfrom mlflow.pyfunc import ChatModel\nfrom typing import Optional, Dict, List, Generator\nfrom mlflow.types.llm import (\n    ChatCompletionResponse,\n    ChatCompletionChunk\n)\n\nchain = (\n    <your chain here>\n    | ChatCompletionOutputParser()\n)\n\nclass LangchainAgent(ChatModel):\n    def _prepare_messages(self, messages: List[ChatMessage]):\n        return {\"messages\": [m.to_dict() for m in messages]}\n\n    def predict(\n        self, context, messages: List[ChatMessage], params: ChatParams\n    ) -> ChatCompletionResponse:\n        question = self._prepare_messages(messages)\n        response_message = self.chain.invoke(question)\n        return ChatCompletionResponse.from_dict(response_message)\n\n    def predict_stream(\n        self, context, messages: List[ChatMessage], params: ChatParams\n    ) -> Generator[ChatCompletionChunk, None, None]:\n        question = self._prepare_messages(messages)\n        for chunk in chain.stream(question):\n          yield ChatCompletionChunk.from_dict(chunk)\n\nUse parameters to configure the agent\n\nIn the Agent Framework, you can use parameters to control how agents are executed. This allows you to quickly iterate by varying characteristics of your agent without changing the code. Parameters are key-value pairs that you define in a Python dictionary or a .yaml file.\n\nTo configure the code, create a ModelConfig, a set of key-value parameters. ModelConfig is either a Python dictionary or a .yaml file. For example, you can use a dictionary during development and then convert it to a .yaml file for production deployment and CI/CD. For details about ModelConfig, see the MLflow documentation.\n\nAn example ModelConfig is shown below.\n\nCopy\nYAML\nllm_parameters:\n  max_tokens: 500\n  temperature: 0.01\nmodel_serving_endpoint: databricks-dbrx-instruct\nvector_search_index: ml.docs.databricks_docs_index\nprompt_template: 'You are a hello world bot. Respond with a reply to the user''s\n  question that indicates your prompt template came from a YAML file. Your response\n  must use the word \"YAML\" somewhere. User''s question: {question}'\nprompt_template_input_vars:\n- question\n\n\nTo call the configuration from your code, use one of the following:\n\nCopy\nPython\n# Example for loading from a .yml file\nconfig_file = \"configs/hello_world_config.yml\"\nmodel_config = mlflow.models.ModelConfig(development_config=config_file)\n\n# Example of using a dictionary\nconfig_dict = {\n    \"prompt_template\": \"You are a hello world bot. Respond with a reply to the user's question that is fun and interesting to the user. User's question: {question}\",\n    \"prompt_template_input_vars\": [\"question\"],\n    \"model_serving_endpoint\": \"databricks-dbrx-instruct\",\n    \"llm_parameters\": {\"temperature\": 0.01, \"max_tokens\": 500},\n}\n\nmodel_config = mlflow.models.ModelConfig(development_config=config_dict)\n\n# Use model_config.get() to retrieve a parameter value\nvalue = model_config.get('sample_param')\n\nSet retriever schema\n\nAI agents often use retrievers, a type of agent tool that finds and returns relevant documents using a Vector Search index. For more information on retrievers, see Unstructured retrieval AI agent tools.\n\nTo ensure that retrievers are traced properly, call mlflow.models.set_retriever_schema when you define your agent in code. Use set_retriever_schema to map the column names in the returned table to MLflow’s expected fields such as primary_key, text_column, and doc_uri.\n\nCopy\nPython\n# Define the retriever's schema by providing your column names\n# These strings should be read from a config dictionary\nmlflow.models.set_retriever_schema(\n    name=\"vector_search\",\n    primary_key=\"chunk_id\",\n    text_column=\"text_column\",\n    doc_uri=\"doc_uri\"\n    # other_columns=[\"column1\", \"column2\"],\n)\n\n\nNote\n\nThe doc_uri column is especially important when evaluating the retriever’s performance. doc_uri is the main identifier for documents returned by the retriever, allowing you to compare them against ground truth evaluation sets. See Evaluation sets.\n\nYou can also specify additional columns in your retriever’s schema by providing a list of column names with the other_columns field.\n\nIf you have multiple retrievers, you can define multiple schemas by using unique names for each retriever schema.\n\nCustom inputs and outputs\n\nSome scenarios may require additional agents inputs, such as client_type and session_id, or outputs like retrieval source links that should not be included in the chat history for future interactions.\n\nFor these scenarios, MLflow ChatModel natively support augmenting OpenAI chat completion requests and responses with the ChatParams fields custom_input and custom_output.\n\nSee the following examples to learn how to create custom inputs and outputs for PyFunc and LangGraph agents.\n\nWarning\n\nThe Agent Evaluation review app does not currently support rendering traces for agents with additional input fields.\n\nPyFunc custom schemas\n\nThe following notebooks show a custom schema example using PyFunc.\n\nPyFunc custom schema agent notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nPyFunc custom schema driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLangGraph custom schemas\n\nThe following notebooks show a custom schema example using LangGraph. You can modify the wrap_output function in the notebooks to parse and extract information from the message stream.\n\nLangGraph custom schema agent notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLangGraph custom schema driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nProvide custom_inputs in the AI Playground and agent review app\n\nIf your agent accepts additional inputs using the custom_inputs field, you can manually provide these inputs in both the AI Playground and the agent review app.\n\nIn either the AI Playground or the Agent Review App, select the gear icon .\n\nEnable custom_inputs.\n\nProvide a JSON object that matches your agent’s defined input schema.\n\nStreaming error propagation\n\nMosaic AI propagation any errors encountered while streaming with the last token under databricks_output.error. It is up to the calling client to properly handle and surface this error.\n\nCopy\nBash\n{\n  \"delta\": …,\n  \"databricks_output\": {\n    \"trace\": {...},\n    \"error\": {\n      \"error_code\": BAD_REQUEST,\n      \"message\": \"TimeoutException: Tool XYZ failed to execute\"\n    }\n  }\n}\n\nExample notebooks\n\nThese notebooks create a simple “Hello, world” chain to illustrate creating an agent in Databricks. The first example creates a simple chain, and the second example notebook illustrates how to use parameters to minimize code changes during development.\n\nSimple chain notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nSimple chain driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nParameterized chain notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nParameterized chain driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nNext steps\n\nCreate your own agent tools.\n\nLog an AI agent.\n\nAdd traces to an AI agent.\n\nDeploy an AI agent.\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat is ChatModel?\nRequirements\nCreate a ChatModel agent\nExample: Wrap LangChain in ChatModel\nUse parameters to configure the agent\nSet retriever schema\nCustom inputs and outputs\nPyFunc custom schemas\nLangGraph custom schemas\nProvide custom_inputs in the AI Playground and agent review app\nStreaming error propagation\nExample notebooks\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "What are compound AI systems and AI agents? | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/ai-agents.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  What are compound AI systems and AI agents?\nWhat are compound AI systems and AI agents?\n\nNovember 30, 2024\n\nMosaic AI Agent Framework helps developers overcome the unique challenges of developing AI agents and compound AI systems. Learn what makes an AI application a compound AI system and an AI agent.\n\nCompound AI systems\n\nCompound AI systems are systems that tackle AI tasks by combining multiple interacting components. In contrast, an AI model is simply a statistical model, e.g., a Transformer that predicts the next token in text. Compound AI systems are an increasingly common design pattern for AI applications due to their performance and flexibility.\n\nFor more information, see The Shift from Models to Compound AI Systems .\n\nWhat are AI agents?\n\nThe industry is still defining AI agents, however it generally understood as an AI system where the model makes some or all of the planning decisions in contrast to hard-coded logic. These agents use large language models (LLMs) to make decisions and accomplish their objectives.\n\nMany AI agents applications are made of multiple systems, thus qualifying them as compound AI systems.\n\nAgency is a continuum, the more freedom we provide models to control the behavior of the system, the more agent-like the application becomes.\n\nWhat are tools?\n\nAI agents use tools to perform actions besides language generation, for example to retrieve structured or unstructured data, run code, or talk to remote services like sending an email or Slack message.\n\nOn Databricks, you can use Unity Catalog functions as tools, enabling easy discovery, governance, and sharing of tools. You can also define tools using open source agent authoring libraries like LangChain.\n\nIn typical agentic workflows, the agent LLM is given metadata about tools, which it uses to determine when and how to use the tool. So when defining tools, you must ensure that the tool, its parameters, and its return value are well-documented, so that the agent LLM can best use the tool.\n\nFrom LLMs to AI agents\n\nTo understand AI agents, it’s helpful to consider the evolution of AI systems.\n\nLLMs: Initially, large language models simply responded to prompts based on knowledge from a vast training dataset.\n\nLLMs + tool chains: Then, developers added hardcoded tools to expand the LLM’s capabilities. For example, retrieval augmented generation (RAG) expanded an LLM’s knowledge base with custom documentation sets, while API tools allowed LLMs to perform tasks like create support tickets or send emails.\n\nAI agents: Now, AI agents autonomously create plans and execute tasks based on their understanding of the problem. AI agents still use tools but it’s up to them to decide which tool to use and when. The key distinction is in the level of autonomy and decision-making capabilities compared to compound AI systems.\n\nFrom a development standpoint, AI applications, whether they are individual LLMs, LLMs with toolchains, or full AI agents face similar challenges. Mosaic AI Agent Framework helps developers manage the unique challenges of building and AI applications at all levels of complexity.\n\nExamples of AI agents\n\nHere are some examples of AI agents across industries:\n\nAI/BI: AI-powered chatbots and dashboards accept natural language prompts to perform analysis on a businesses’ data, drawing insights from the full lifecycle of their data. AI/BI agents parse requests, decide which data sources to, and how to communicate findings. AI/BI agents can improve over time through human feedback, offering tools to verify and refine its outputs.\n\nCustomer service: AI-powered chatbots, such as those used by customer service platforms, interact with users, understand natural language, and provide relevant responses or perform tasks. Companies use AI chatbots for customer service by answering queries, providing product information, and assisting with troubleshooting.\n\nManufacturing predictive maintenance: AI agents can go beyond simply predicting equipment failures, autonomously acting on them by ordering replacements, or scheduling maintenance to reduce downtime and increase productivity.\n\nNext steps\n\nLearn how to develop and evaluate AI agents:\n\nCreate an AI agent\n\nWhat is Mosaic AI Agent Evaluation?\n\nHands on AI agent tutorials:\n\nDemo: Mosaic AI Agent Framework and Agent Evaluation\n\nIntroduction: End-to-end generative AI agent tutorial\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nCompound AI systems\nWhat are AI agents?\nWhat are tools?\nExamples of AI agents\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Deploy an agent for generative AI application | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/deploy-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Deploy an agent for generative AI application\nDeploy an agent for generative AI application\n\nJanuary 29, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to deploy your AI agent using the deploy() function from the Agent Framework Python API.\n\nRequirements\n\nMLflow 2.13.1 or above to deploy agents using the the deploy() API from databricks.agents.\n\nRegister an AI agent to Unity Catalog. See Register the agent to Unity Catalog.\n\nDeploying agents from outside a Databricks notebook requires databricks-agents SDK version 0.12.0 or above.\n\nInstall the the databricks-agents SDK.\n\nCopy\nPython\n%pip install databricks-agents\ndbutils.library.restartPython()\n\nDeploy an agent using deploy()\n\nThe deploy() function does the following:\n\nCreates CPU model serving endpoints for your agent that can be integrated into your user-facing application.\n\nTo reduce cost for idle endpoints (at the expense of increased time to serve initial queries), you can enable scale to zero for your serving endpoint by passing scale_to_zero_enabled=True to deploy(). See Endpoint scaling expectations.\n\nEnables inference tables with AI Gateway on the model serving endpoint. See Monitor served models using AI Gateway-enabled inference tables.\n\nNote\n\nFor streaming response logs, only ChatCompletion-compatible fields and traces are aggregated.\n\nDatabricks automatically provides short-lived service principal credentials to agent code running in the endpoint. The credentials have the minimum permissions to access Databricks-managed resources as defined during model logging. Before generating these credentials, Databricks ensures that the endpoint owner has the appropriate permissions to prevent privilege escalation and unauthorized access. See Authentication for dependent resources.\n\nIf you have resource dependencies that are not Databricks-managed, for example, using Pinecone, you can pass in environment variables with secrets to the deploy() API. See Configure access to resources from model serving endpoints.\n\nEnables the Review App for your agent. The Review App lets stakeholders chat with the agent and give feedback using the Review App UI.\n\nLogs every request to the Review App or REST API to an inference table. The data logged includes query requests, responses, and intermediate trace data from MLflow Tracing.\n\nCreates a feedback model with the same catalog and schema as the agent you are trying to deploy. This feedback model is the mechanism that makes it possible to accept feedback from the Review App and log it to an inference table. This model is served in the same CPU model serving endpoint as your deployed agent. Because this serving endpoint has inference tables enabled, it is possible to log feedback from the Review App to an inference table.\n\nNote\n\nDeployments can take up to 15 minutes to complete. Raw JSON payloads take 10 - 30 minutes to arrive, and the formatted logs are processed from the raw payloads about every hour.\n\nCopy\nPython\n\nfrom databricks.agents import deploy\nfrom mlflow.utils import databricks_utils as du\n\ndeployment = deploy(model_fqn, uc_model_info.version)\n\n# query_endpoint is the URL that can be used to make queries to the app\ndeployment.query_endpoint\n\n# Copy deployment.rag_app_url to browser and start interacting with your RAG application.\ndeployment.rag_app_url\n\nAgent-enhanced inference tables\n\nThe deploy() creates three inference tables for each deployment to log requests and responses to and from the agent serving endpoint. Users can expect the data to be in the payload table within an hour of interacting with their deployment.\n\nPayload request logs and assessment logs might take longer to populate, but are ultimately derived from the raw payload table. You can extract request and assessment logs from the payload table yourself. Deletions and updates to the payload table are not reflected in the payload request logs or the payload assessment logs.\n\nTable\n\n\t\n\nExample Unity Catalog table name\n\n\t\n\nWhat is in each table\n\n\n\n\nPayload\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload\n\n\t\n\nRaw JSON request and response payloads\n\n\n\n\nPayload request logs\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload_request_logs\n\n\t\n\nFormatted request and responses, MLflow traces\n\n\n\n\nPayload assessment logs\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload_assessment_logs\n\n\t\n\nFormatted feedback, as provided in the Review App, for each request\n\nThe following shows the schema for the request logs table.\n\nColumn name\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nclient_request_id\n\n\t\n\nString\n\n\t\n\nClient request ID, usually null.\n\n\n\n\ndatabricks_request_id\n\n\t\n\nString\n\n\t\n\nDatabricks request ID.\n\n\n\n\ndate\n\n\t\n\nDate\n\n\t\n\nDate of request.\n\n\n\n\ntimestamp_ms\n\n\t\n\nLong\n\n\t\n\nTimestamp in milliseconds.\n\n\n\n\ntimestamp\n\n\t\n\nTimestamp\n\n\t\n\nTimestamp of the request.\n\n\n\n\nstatus_code\n\n\t\n\nInteger\n\n\t\n\nStatus code of endpoint.\n\n\n\n\nexecution_time_ms\n\n\t\n\nLong\n\n\t\n\nTotal execution milliseconds.\n\n\n\n\nconversation_id\n\n\t\n\nString\n\n\t\n\nConversation id extracted from request logs.\n\n\n\n\nrequest\n\n\t\n\nString\n\n\t\n\nThe last user query from the user’s conversation. This is extracted from the RAG request.\n\n\n\n\nresponse\n\n\t\n\nString\n\n\t\n\nThe last response to the user. This is extracted from the RAG request.\n\n\n\n\nrequest_raw\n\n\t\n\nString\n\n\t\n\nString representation of request.\n\n\n\n\nresponse_raw\n\n\t\n\nString\n\n\t\n\nString representation of response.\n\n\n\n\ntrace\n\n\t\n\nString\n\n\t\n\nString representation of trace extracted from the databricks_options of response Struct.\n\n\n\n\nsampling_fraction\n\n\t\n\nDouble\n\n\t\n\nSampling fraction.\n\n\n\n\nrequest_metadata\n\n\t\n\nMap[String, String]\n\n\t\n\nA map of metadata related to the model serving endpoint associated with the request. This map contains the endpoint name, model name, and model version used for your endpoint.\n\n\n\n\nschema_version\n\n\t\n\nString\n\n\t\n\nInteger for the schema version.\n\nThe following is the schema for the assessment logs table.\n\nColumn name\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nrequest_id\n\n\t\n\nString\n\n\t\n\nDatabricks request ID.\n\n\n\n\nstep_id\n\n\t\n\nString\n\n\t\n\nDerived from retrieval assessment.\n\n\n\n\nsource\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the information on who created the assessment.\n\n\n\n\ntimestamp\n\n\t\n\nTimestamp\n\n\t\n\nTimestamp of request.\n\n\n\n\ntext_assessment\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the data for any feedback on the agent’s responses from the review app.\n\n\n\n\nretrieval_assessment\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the data for any feedback on the documents retrieved for a response.\n\nAuthentication for dependent resources\n\nAI agents often need to authenticate to other resources to complete tasks. For example, an agent may need to access a Vector Search index to query unstructured data.\n\nYour agent can use one of the following methods to authenticate to dependent resources when you serve it behind a Model Serving endpoint:\n\nAutomatic authentication passthrough: Declare Databricks resource dependencies for your agent during logging. Databricks can automatically provision, rotate, and manage short-lived credentials when your agent is deployed to securely access resources. Databricks recommends using automatic authentication passthrough where possible.\n\nManual authentication: Manually specify long-lived credentials during agent deployment. Use manual authentication for Databricks resources that do not support automatic authentication passthrough, or for external API access.\n\nAutomatic authentication passthrough\n\nModel Serving supports automatic authentication passthrough for the most common types of Databricks resources used by agents.\n\nTo enable automatic authentication passthrough, you must specify dependencies during agent logging.\n\nThen, when you serve the agent behind an endpoint, Databricks performs the following steps:\n\nPermission verification: Databricks verifies that the endpoint creator can access all dependencies specified during agent logging.\n\nService principal creation and grants: A service principal is created for the agent model version and is automatically granted read access to agent resources.\n\nNote\n\nThe system-generated service principal does not appear in API or UI listings. If the agent model version is removed from the endpoint, the service principal is also deleted.\n\nCredential provisioning and rotation: Short-lived credentials (an M2M OAuth token) for the service principal are injected into the endpoint, allowing agent code to access Databricks resources. Databricks also rotates the credentials, ensuring that your agent has continued, secure access to dependent resources.\n\nThis authentication behavior is similar to the “Run as owner” behavior for Databricks dashboards - downstream resources like Unity Catalog tables are accessed using the credentials of a service principal with least-privilege access to dependent resources.\n\nThe following table lists the Databricks resources that support automatic authentication passthrough and the permissions the endpoint creator must have when deploying the agent.\n\nNote\n\nUnity Catalog resources also require USE SCHEMA on the parent schema and USE CATALOG on the parent catalog.\n\nResource type\n\n\t\n\nPermission\n\n\n\n\nSQL Warehouse\n\n\t\n\nUse Endpoint\n\n\n\n\nModel Serving endpoint\n\n\t\n\nCan Query\n\n\n\n\nUnity Catalog Function\n\n\t\n\nEXECUTE\n\n\n\n\nGenie space\n\n\t\n\nCan Run\n\n\n\n\nVector Search index\n\n\t\n\nCan Use\n\n\n\n\nUnity Catalog Table\n\n\t\n\nSELECT\n\nManual authentication\n\nYou can also manually provide credentials using secrets-based environment variables. Manual authentication can be helpful in the following scenarios:\n\nThe dependent resource does not support automatic authentication passthrough.\n\nThe agent is accessing an external resource or API.\n\nThe agent needs to use credentials other than those of the agent deployer.\n\nFor example, to use the Databricks SDK in your agent to access other dependent resources, you can set the environment variables described in Databricks client unified authentication.\n\nGet deployed applications\n\nThe following shows how to get your deployed agents.\n\nCopy\nPython\nfrom databricks.agents import list_deployments, get_deployments\n\n# Get the deployment for specific model_fqn and version\ndeployment = get_deployments(model_name=model_fqn, model_version=model_version.version)\n\ndeployments = list_deployments()\n# Print all the current deployments\ndeployments\n\nProvide feedback on a deployed agent (experimental)\n\nWhen you deploy your agent with agents.deploy(), agent framework also creates and deploys a “feedback” model version within the same endpoint, which you can query to provide feedback on your agent application. Feedback entries appear as request rows within the inference table associated with your agent serving endpoint.\n\nNote that this behavior is experimental: Databricks may provide a first-class API for providing feedback on a deployed agent in the future, and future functionality may require migrating to this API.\n\nLimitations of this API include:\n\nThe feedback API lacks input validation - it always responds successfully, even if passed invalid input.\n\nThe feedback API requires passing in the Databricks-generated request_id of the agent endpoint request on which you wish to provide feedback. To get the databricks_request_id, include {\"databricks_options\": {\"return_trace\": True}} in your original request to the agent serving endpoint. The agent endpoint response will then include the databricks_request_id associated with the request so that you can pass that request ID back to the feedback API when providing feedback on the agent response.\n\nFeedback is collected using inference tables. See inference table limitations.\n\nThe following example request provides feedback on the agent endpoint named “your-agent-endpoint-name”, and assumes that the DATABRICKS_TOKEN environment variable is set to a Databricks REST API token.\n\nCopy\nBash\ncurl \\\n  -u token:$DATABRICKS_TOKEN \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '\n      {\n          \"dataframe_records\": [\n              {\n                  \"source\": {\n                      \"id\": \"user@company.com\",\n                      \"type\": \"human\"\n                  },\n                  \"request_id\": \"573d4a61-4adb-41bd-96db-0ec8cebc3744\",\n                  \"text_assessments\": [\n                      {\n                          \"ratings\": {\n                              \"answer_correct\": {\n                                  \"value\": \"positive\"\n                              },\n                              \"accurate\": {\n                                  \"value\": \"positive\"\n                              }\n                          },\n                          \"free_text_comment\": \"The answer used the provided context to talk about Delta Live Tables\"\n                      }\n                  ],\n                  \"retrieval_assessments\": [\n                      {\n                          \"ratings\": {\n                              \"groundedness\": {\n                                  \"value\": \"positive\"\n                              }\n                          }\n                      }\n                  ]\n              }\n          ]\n      }' \\\nhttps://<workspace-host>.databricks.com/serving-endpoints/<your-agent-endpoint-name>/served-models/feedback/invocations\n\n\nYou can pass additional or different key-value pairs in the text_assessments.ratings and retrieval_assessments.ratings fields to provide different types of feedback. In the example, the feedback payload indicates that the agent’s response to the request with ID 573d4a61-4adb-41bd-96db-0ec8cebc3744 was correct, accurate, and grounded in context fetched by a retriever tool.\n\nAdditional resources\n\nWhat is Mosaic AI Agent Evaluation?\n\nGet feedback about the quality of an agentic application\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nDeploy an agent using deploy()\nAgent-enhanced inference tables\nAuthentication for dependent resources\nGet deployed applications\nProvide feedback on a deployed agent (experimental)\nAdditional resources\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Log and register AI agents | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/log-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Log and register AI agents\nLog and register AI agents\n\nJanuary 24, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nLog AI agents using Mosaic AI Agent Framework. Logging an agent is the basis of the development process. Logging captures a “point in time” of the agent’s code and configuration so you can evaluate the quality of the configuration.\n\nRequirements\n\nCreate an AI agent before logging it.\n\nCode-based logging\n\nDatabricks recommends using MLflow’s Models from Code functionality when logging agents.\n\nIn this approach, the agent’s code is captured as a Python file, and the Python environment is captured as a list of packages. When the agent is deployed, the Python environment is restored, and the agent’s code is executed to load the agent into memory so it can be invoked when the endpoint is called.\n\nYou can couple this approach with the use of pre-deployment validation APIs like mlflow.models.predict() to ensure that the agent runs reliably when deployed for serving.\n\nThe code that logs the agent or agent must be in a separate notebook from the agent code. This notebook is called a driver notebook. For an example notebook, see Example notebooks.\n\nInfer Model Signature during logging\n\nDuring logging, you must define an MLflow Model Signature, which specifies the agent’s input and output schema. The signature validates inputs and outputs to ensure that the agent interacts correctly with downstream tools like AI Playground and the review app. It also guides other applications on how to use the agent effectively.\n\nDatabricks recommends using MLflow’s Model Signature inferencing capabilities to automatically generate the agent’s signature based on an input example you provide. This approach is more convenient than manually defining the signature.\n\nThe LangChain and PyFunc examples below use Model Signature inferencing.\n\nIf you would rather explicitly define a Model Signature yourself at logging time, see MLflow docs - How to log models with signatures.\n\nCode-based logging with LangChain\n\nThe following instructions and code sample show you how to log an agent with LangChain.\n\nCreate a notebook or Python file with your code. For this example, the notebook or file is named agent.py. The notebook or file must contain a LangChain agent, referred to here as lc_agent.\n\nInclude mlflow.models.set_model(lc_agent) in the notebook or file.\n\nCreate a new notebook to serve as the driver notebook (called driver.py in this example).\n\nIn the driver notebook, use the following code to run agent.py and log the results to an MLflow model:\n\nCopy\nPython\nmlflow.langchain.log_model(lc_model=\"/path/to/agent.py\", resources=list_of_databricks_resources)\n\n\nThe resources parameter declares Databricks-managed resources needed to serve the agent, such as a vector search index or serving endpoint that serves a foundation model. For more information, see Specify resources for automatic authentication passthrough.\n\nDeploy the model. See Deploy an agent for generative AI application.\n\nWhen the serving environment is loaded, agent.py is executed.\n\nWhen a serving request comes in, lc_agent.invoke(...) is called.\n\nCopy\nPython\n\nimport mlflow\n\ncode_path = \"/Workspace/Users/first.last/agent.py\"\nconfig_path = \"/Workspace/Users/first.last/config.yml\"\n\n# Input example used by MLflow to infer Model Signature\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Retrieval-augmented Generation?\",\n        }\n    ]\n}\n\n# example using langchain\nwith mlflow.start_run():\n  logged_agent_info = mlflow.langchain.log_model(\n    lc_model=code_path,\n    model_config=config_path, # If you specify this parameter, this configuration is used by agent code. The development_config is overwritten.\n    artifact_path=\"agent\", # This string is used as the path inside the MLflow model where artifacts are stored\n    input_example=input_example, # Must be a valid input to the agent\n    example_no_conversion=True, # Required\n  )\n\nprint(f\"MLflow Run: {logged_agent_info.run_id}\")\nprint(f\"Model URI: {logged_agent_info.model_uri}\")\n\n# To verify that the model has been logged correctly, load the agent and call `invoke`:\nmodel = mlflow.langchain.load_model(logged_agent_info.model_uri)\nmodel.invoke(example)\n\nCode-based logging with PyFunc\n\nThe following instructions and code sample show you how to log an agent with PyFunc.\n\nCreate a notebook or Python file with your code. For this example, the notebook or file is named agent.py. The notebook or file must contain a PyFunc class, named PyFuncClass.\n\nInclude mlflow.models.set_model(PyFuncClass) in the notebook or file.\n\nCreate a new notebook to serve as the driver notebook (called driver.py in this example).\n\nIn the driver notebook, use the following code to run agent.py and log the results to an MLflow model:\n\nCopy\nPython\nmlflow.pyfunc.log_model(python_model=\"/path/to/agent.py\", resources=list_of_databricks_resources)\n\n\nThe resources parameter declares Databricks-managed resources needed to serve the agent, such as a vector search index or serving endpoint that serves a foundation model. For more information, see Specify resources for automatic authentication passthrough.\n\nDeploy the model. See Deploy an agent for generative AI application.\n\nWhen the serving environment is loaded, agent.py is executed.\n\nWhen a serving request comes in, PyFuncClass.predict(...) is called.\n\nCopy\nPython\nimport mlflow\nfrom mlflow.models.resources import (\n    DatabricksServingEndpoint,\n    DatabricksVectorSearchIndex,\n)\n\ncode_path = \"/Workspace/Users/first.last/agent.py\"\nconfig_path = \"/Workspace/Users/first.last/config.yml\"\n\n# Input example used by MLflow to infer Model Signature\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Retrieval-augmented Generation?\",\n        }\n    ]\n}\n\nwith mlflow.start_run():\n  logged_agent_info = mlflow.pyfunc.log_model(\n    python_model=agent_notebook_path,\n    artifact_path=\"agent\",\n    input_example=input_example,\n    resources=resources_path,\n    example_no_conversion=True,\n    resources=[\n      DatabricksServingEndpoint(endpoint_name=\"databricks-mixtral-8x7b-instruct\"),\n      DatabricksVectorSearchIndex(index_name=\"prod.agents.databricks_docs_index\"),\n    ]\n  )\n\nprint(f\"MLflow Run: {logged_agent_info.run_id}\")\nprint(f\"Model URI: {logged_agent_info.model_uri}\")\n\n# To verify that the model has been logged correctly, load the agent and call `invoke`:\nmodel = mlflow.pyfunc.load_model(logged_agent_info.model_uri)\nmodel.invoke(example)\n\nSpecify resources for automatic authentication passthrough\n\nAI agents often need to authenticate to other resources to complete tasks. For example, an agent may need to access a Vector Search index to query unstructured data.\n\nAs described in Authentication for dependent resources, Model Serving supports authenticating to both Databricks-managed and external resources when you deploy the agent.\n\nFor the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront during logging. This enables automatic authentication passthrough when you deploy the agent - Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n\nTo enable automatic authentication passthrough, specify dependent resources using the resources parameter of the log_model() API, as shown in the following code.\n\nCopy\nPython\nimport mlflow\nfrom mlflow.models.resources import (\n    DatabricksVectorSearchIndex,\n    DatabricksServingEndpoint,\n    DatabricksSQLWarehouse,\n    DatabricksFunction,\n    DatabricksGenieSpace,\n    DatabricksTable,\n)\n\nwith mlflow.start_run():\n  logged_agent_info = mlflow.pyfunc.log_model(\n    python_model=agent_notebook_path,\n    artifact_path=\"agent\",\n    input_example=input_example,\n    example_no_conversion=True,\n    # Specify resources for automatic authentication passthrough\n    resources=[\n      DatabricksVectorSearchIndex(index_name=\"prod.agents.databricks_docs_index\"),\n      DatabricksServingEndpoint(endpoint_name=\"databricks-mixtral-8x7b-instruct\"),\n      DatabricksServingEndpoint(endpoint_name=\"databricks-bge-large-en\"),\n      DatabricksSQLWarehouse(warehouse_id=\"your_warehouse_id\"),\n      DatabricksFunction(function_name=\"ml.tools.python_exec\"),\n      DatabricksGenieSpace(genie_space_id=\"your_genie_space_id\"),\n      DatabricksTable(table_name=\"your_table_name\"),\n    ]\n  )\n\n\nDatabricks recommends you manually specify resources for all agent flavors.\n\nNote\n\nIf you do not specify resources when logging LangChain agents using mlflow.langchain.log_model(...), MLflow performs best-effort automatic inference of resources. However, this may not capture all dependencies, resulting in authorization errors when serving or querying the agent.\n\nThe following table lists the Databricks resources that support automatic authentication passthrough and the minimum mlflow version required to log the resource.\n\nResource type\n\n\t\n\nMinimum mlflow version required to log the resource\n\n\n\n\nVector Search index\n\n\t\n\nRequires mlflow 2.13.1 or above\n\n\n\n\nModel serving endpoint\n\n\t\n\nRequires mlflow 2.13.1 or above\n\n\n\n\nSQL warehouse\n\n\t\n\nRequires mlflow 2.16.1 or above\n\n\n\n\nUnity Catalog function\n\n\t\n\nRequires mlflow 2.16.1 or above\n\n\n\n\nGenie space\n\n\t\n\nRequires mlflow 2.17.1 or above\n\n\n\n\nUnity Catalog table\n\n\t\n\nRequires mlflow 2.18.0 or above\n\nRegister the agent to Unity Catalog\n\nBefore you deploy the agent, you must register the agent to Unity Catalog. Registering the agent packages it as a model in Unity Catalog. As a result, you can use Unity Catalog permissions for authorization for resources in the agent.\n\nCopy\nPython\nimport mlflow\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog_name = \"test_catalog\"\nschema_name = \"schema\"\nmodel_name = \"agent_name\"\n\nmodel_name = catalog_name + \".\" + schema_name + \".\" + model_name\nuc_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=model_name)\n\nNext steps\n\nAdd traces to an AI agent.\n\nDeploy an AI agent.\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nCode-based logging\nInfer Model Signature during logging\nCode-based logging with LangChain\nCode-based logging with PyFunc\nSpecify resources for automatic authentication passthrough\nRegister the agent to Unity Catalog\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCreate custom tools\nStructured retrieval examples\nUnstructured retrieval examples\nConnect tools to external services\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  AI agent tools\nAI agent tools\n\nJanuary 31, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article provides an overview of building AI agent tools using the Mosaic AI Agent Framework.\n\nAI agent tools enable agents to perform tasks beyond language generation, such as retrieving structured or unstructured data and executing custom code.\n\nFor an introduction to AI agents, see What are compound AI systems and AI agents?.\n\nUnity Catalog function tools vs. agent code tools\n\nTo create a tool with Mosaic AI Agent Framework, you can use any combination of the following methods:\n\nMethod\n\n\t\n\nDescription\n\n\n\n\nUnity Catalog functions\n\n\t\n\nDefined and managed in Unity Catalog with built-in security and compliance features\n\nGrants easier discoverability, governance, and reuse\n\nIdeal for applying transformations and aggregations on large datasets\n\n\n\n\nAgent code tools\n\n\t\n\nDefined in the AI agent’s code\n\nUseful for calling REST APIs, using arbitrary code, or executing low-latency tools\n\nLacks built-in governance and discoverability of functions\n\nBoth methods are compatible with custom Python agents or agent-authoring libraries like LangGraph.\n\nCreate AI agent tools\n\nLearn how to create AI agent tools that let your agents execute custom Python code. See Create custom AI agent tools with Unity Catalog functions.\n\nAgent tool examples\n\nSee the following articles for examples of agent tools:\n\nStructured data retrieval tools allow your agent to query structured data sources like SQL tables.\n\nUnstructured data retrieval tools allow your agent to query unstructured data sources like a text corpora to perform retrieval augmented generation.\n\nExternal connection tools connect agent tools to external services and APIs.\n\nAdd Unity Catalog tools to agents\n\nUnlike agent code tools, which are defined in the agent’s code, Unity Catalog tools must be explicitly added to agents to make them available for use.\n\nDatabricks recommends using UCFunctionToolkit to integrate Unity Catalog tools with agent authoring frameworks and SDKs. See Create custom AI agent tools with Unity Catalog functions.\n\nYou can also use the AI Playground to quickly add Unity Catalog tools to agents to prototype behavior. See Prototype tool-calling agents in AI Playground.\n\nImprove tool-calling with clear documentation\n\nWell-documented tools help AI agents understand when and how to use tools effectively. Follow these best practices when documenting tool parameters and return values:\n\nFor Unity Catalog functions, use COMMENT to describe tool functionality and parameters.\n\nClearly define expected inputs and outputs.\n\nProvide meaningful descriptions to improve usability.\n\nExample: Effective tool documentation\n\nThe following example shows effective COMMENT strings for a Unity Catalog function tool that queries a structured table.\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.lookup_customer_info(\n  customer_name STRING COMMENT 'Name of the customer whose info to look up.'\n)\nRETURNS STRING\nCOMMENT 'Returns metadata about a specific customer including their email and ID.'\nRETURN SELECT CONCAT(\n    'Customer ID: ', customer_id, ', ',\n    'Customer Email: ', customer_email\n  )\n  FROM main.default.customer_data\n  WHERE customer_name = customer_name\n  LIMIT 1;\n\nExample: Ineffective tool documentation\n\nThe following example lacks important details, making it harder for the AI agent to use the tool effectively:\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.lookup_customer_info(\n  customer_name STRING COMMENT 'Name of the customer.'\n)\nRETURNS STRING\nCOMMENT 'Returns info about a customer.'\nRETURN SELECT CONCAT(\n    'Customer ID: ', customer_id, ', ',\n    'Customer Email: ', customer_email\n  )\n  FROM main.default.customer_data\n  WHERE customer_name = customer_name\n  LIMIT 1;\n\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nUnity Catalog function tools vs. agent code tools\nCreate AI agent tools\nAdd Unity Catalog tools to agents\nImprove tool-calling with clear documentation\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Create an AI agent | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/create-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAuthor agents using Python\nPrototype agents in AI Playground\nLegacy input and output schema\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent\nCreate an AI agent\n\nJanuary 24, 2025\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article introduces the process of creating AI agents on Databricks and outlines the available methods for creating agents.\n\nTo learn more about agents, see What are compound AI systems and AI agents?.\n\nAuthor an agent in code\n\nMosaic AI Agent Framework and MLflow provide tools to help you author enterprise-ready agents in Python.\n\nDatabricks supports authoring agents using third-party agent authoring libraries like LangChain, LlamaIndex, or custom Python implementations.\n\nTo learn how to create AI agents on Databricks, see Author AI agents in code.\n\nPrototype agents with AI Playground\n\nThe AI Playground is the easiest way to get started creating an agent on Databricks. AI Playground lets you select from various LLMs and quickly add tools to the LLM using a low-code UI. You can then chat with the agent to test its responses and then export the agent to code for deployment or further development.\n\nSee Prototype tool-calling agents in AI Playground.\n\nUnderstand model signatures to ensure compatibility with Databricks features\n\nMosaic AI uses MLflow Model Signatures to define the input and output schema requirements for agents. The model signature tells internal and external components how to interact with your agent and validates that they adhere to the schema.\n\nTo ensure compatibility with Databricks and MLflow features, your agent must conform to OpenAI’s ChatCompletionRequest and ChatCompletionResponse signatures, which are open-source and widely compatible with other platforms.\n\nAdditionally, Databricks supports custom input and output fields that extend these schemas. To facilitate this, MLflow offers ChatModel, which adds custom fields by default.\n\nThis ensures that any agent created using the ChatModel interface will be automatically compliant with the chat completion interface and the broader Databricks toolset.\n\nSee MLflow: Getting Started with ChatModel.\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nAuthor an agent in code\nPrototype agents with AI Playground\nUnderstand model signatures to ensure compatibility with Databricks features\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Introduction to building gen AI apps on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/build-genai-apps.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks\nIntroduction to building gen AI apps on Databricks\n\nJanuary 24, 2025\n\nMosaic AI provides a comprehensive platform to build, deploy, and manage GenAI applications. This article guides you through the essential components and processes involved in developing GenAI applications on Databricks.\n\nDeploy and query gen AI models\n\nFor simple use cases, you can directly serve and query gen AI models, including high quality open-source models, as well as third-party models from LLM providers such as OpenAI and Anthropic.\n\nMosaic AI Model Serving supports serving and querying generative AI models using the following capabilities:\n\nFoundation Model APIs. This functionality makes state-of-the-art open models and fine-tuned model variants available to your model serving endpoint. These models are curated foundation model architectures that support optimized inference. Base models, like DBRX Instruct, Meta-Llama-3.1-70B-Instruct, GTE-Large, and Mistral-7B are available for immediate use with pay-per-token pricing, and workloads that require performance guarantees, like fine-tuned model variants, can be deployed with provisioned throughput.\n\nExternal models. These are generative AI models that are hosted outside of Databricks. Endpoints that serve external models can be centrally governed and customers can establish rate limits and access control for them. Examples include foundation models like OpenAI’s GPT-4, Anthropic’s Claude, and others.\n\nSee Create foundation model serving endpoints.\n\nMosaic AI Agent Framework\n\nMosaic AI Agent Framework comprises a set of tools on Databricks designed to help developers build, deploy, and evaluate production-quality agents like Retrieval Augmented Generation (RAG) applications.\n\nIt is compatible with third-party frameworks like LangChain and LlamaIndex, allowing you to develop with your preferred framework and while leveraging Databricks’ managed Unity Catalog, Agent Evaluation Framework, and other platform benefits.\n\nQuickly iterate on agent development using the following features:\n\nCreate AI agents using any agent authoring library and MLflow. Parameterize your agents to experiment and iterate on agent development quickly.\n\nCreate agent tools to extend the LLMs capabilities beyond language generation.\n\nAgent tracing lets you log, analyze, and compare traces across your agent code to debug and understand how your agent responds to requests.\n\nAgent Evaluation helps evaluate the quality, cost, and latency of agents to identify quality issues and determine the root cause of those issues.\n\nImprove agent quality using DSPy. DSPy can automate prompt engineering and fine-tuning to improve the quality of your GenAI agents.\n\nDeploy agents to production with native support for token streaming and request/response logging, plus a built-in review app to get user feedback for your agent.\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nDeploy and query gen AI models\nMosaic AI Agent Framework\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "AI and machine learning on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/index.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nRead with external systems\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Feb 10, 2025\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks\nAI and machine learning on Databricks\n\nFebruary 05, 2025\n\nThis article describes the tools that Mosaic AI (formerly Databricks Machine Learning) provides to help you build AI and ML systems. The diagram shows how various products on Databricks platform help you implement your end to end workflows to build and deploy AI and ML systems\n\nGenerative AI on Databricks\n\nMosaic AI unifies the AI lifecycle from data collection and preparation, to model development and LLMOps, to serving and monitoring. The following features are specifically optimized to facilitate the development of generative AI applications:\n\nUnity Catalog for governance, discovery, versioning, and access control for data, features, models, and functions.\n\nMLflow for model development tracking.\n\nMosaic AI Gateway for governing and monitoring access to supported generative AI models and their associated model serving endpoints.\n\nMosaic AI Model Serving for deploying LLMs. You can configure a model serving endpoint specifically for accessing generative AI models:\n\nState-of-the-art open LLMs using Foundation Model APIs.\n\nThird-party models hosted outside of Databricks. See External models in Mosaic AI Model Serving.\n\nMosaic AI Vector Search provides a queryable vector database that stores embedding vectors and can be configured to automatically sync to your knowledge base.\n\nLakehouse Monitoring for data monitoring and tracking model prediction quality and drift using automatic payload logging with inference tables.\n\nAI Playground for testing generative AI models from your Databricks workspace. You can prompt, compare and adjust settings such as system prompt and inference parameters.\n\nFoundation Model Fine-tuning (now part of Mosaic AI Model Training) for customizing a foundation model using your own data to optimize its performance for your specific application.\n\nMosaic AI Agent Framework for building and deploying production-quality agents like Retrieval Augmented Generation (RAG) applications.\n\nMosaic AI Agent Evaluation for evaluating the quality, cost, and latency of generative AI applications, including RAG applications and chains.\n\nWhat is generative AI?\n\nGenerative AI is a type of artificial intelligence focused on the ability of computers to use models to create content like images, text, code, and synthetic data.\n\nGenerative AI applications are built on top of generative AI models: large language models (LLMs) and foundation models.\n\nLLMs are deep learning models that consume and train on massive datasets to excel in language processing tasks. They create new combinations of text that mimic natural language based on their training data.\n\nGenerative AI models or foundation models are large ML models pre-trained with the intention that they are to be fine-tuned for more specific language understanding and generation tasks. These models are used to discern patterns within the input data.\n\nAfter these models have completed their learning processes, together they generate statistically probable outputs when prompted and they can be employed to accomplish various tasks, including:\n\nImage generation based on existing ones or utilizing the style of one image to modify or create a new one.\n\nSpeech tasks such as transcription, translation, question/answer generation, and interpretation of the intent or meaning of text.\n\nImportant\n\nWhile many LLMs or other generative AI models have safeguards, they can still generate harmful or inaccurate information.\n\nGenerative AI has the following design patterns:\n\nPrompt Engineering: Crafting specialized prompts to guide LLM behavior\n\nRetrieval Augmented Generation (RAG): Combining an LLM with external knowledge retrieval\n\nFine-tuning: Adapting a pre-trained LLM to specific data sets of domains\n\nPre-training: Training an LLM from scratch\n\nSupport for multimodal generative AI models\n\nMultimodel generative AI models process and generate outputs across various data types, like text, images, audio, and video. Databricks supports a range of multimodal generative AI models that can be deployed via API or in batch mode, ensuring flexibility and scalability across all deployment scenarios:\n\nMultimodal models: Use hosted multimodal models like Llama 3.2 and external models like GPT-4o. See Supported foundation models on Mosaic AI Model Serving.\n\nFine-tuned and customized models: Fine-tune models to optimize them for specific use cases. See Foundation Model Fine-tuning.\n\nMachine learning on Databricks\n\nWith Mosaic AI, a single platform serves every step of ML development and deployment, from raw data to inference tables that save every request and response for a served model. Data scientists, data engineers, ML engineers and DevOps can do their jobs using the same set of tools and a single source of truth for the data.\n\nMosaic AI unifies the data layer and ML platform. All data assets and artifacts, such as models and functions, are discoverable and governed in a single catalog. Using a single platform for data and models makes it possible to track lineage from the raw data to the production model. Built-in data and model monitoring saves quality metrics to tables that are also stored in the platform, making it easier to identify the root cause of model performance problems. For more information about how Databricks supports the full ML lifecycle and MLOps, see MLOps workflows on Databricks and MLOps Stacks: model development process as code.\n\nSome of the key components of the data intelligence platform are:\n\nTasks\n\n\t\n\nComponent\n\n\n\n\nGovern and manage data, features, models, and functions. Also discovery, versioning, and lineage.\n\n\t\n\nUnity Catalog\n\n\n\n\nTrack changes to data, data quality, and model prediction quality\n\n\t\n\nLakehouse Monitoring, Inference tables for custom models\n\n\n\n\nFeature development and management\n\n\t\n\nFeature engineering and serving.\n\n\n\n\nTrain models\n\n\t\n\nAutoML, Databricks notebooks\n\n\n\n\nTrack model development\n\n\t\n\nMLflow tracking\n\n\n\n\nServe custom models\n\n\t\n\nMosaic AI Model Serving.\n\n\n\n\nBuild automated workflows and production-ready ETL pipelines\n\n\t\n\nDatabricks Jobs\n\n\n\n\nGit integration\n\n\t\n\nDatabricks Git folders\n\nDeep learning on Databricks\n\nConfiguring infrastructure for deep learning applications can be difficult. Databricks Runtime for Machine Learning takes care of that for you, with clusters that have built-in compatible versions of the most common deep learning libraries like TensorFlow, PyTorch, and Keras.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. It also supports libraries like Ray to parallelize compute processing for scaling ML workflows and ML applications.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. Mosaic AI Model Serving enables creation of scalable GPU endpoints for deep learning models with no extra configuration.\n\nFor machine learning applications, Databricks recommends using a cluster running Databricks Runtime for Machine Learning. See Create a cluster using Databricks Runtime ML.\n\nTo get started with deep learning on Databricks, see:\n\nBest practices for deep learning on Databricks\n\nDeep learning on Databricks\n\nReference solutions for deep learning\n\nNext steps\n\nTo get started, see:\n\nTutorials: Get started with AI and machine learning\n\nFor a recommended MLOps workflow on Databricks Mosaic AI, see:\n\nMLOps workflows on Databricks\n\nTo learn about key Databricks Mosaic AI features, see:\n\nWhat is AutoML?\n\nFeature engineering and serving\n\nDeploy models using Mosaic AI Model Serving\n\nLakehouse Monitoring\n\nManage model lifecycle\n\nMLflow experiment tracking\n\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nGenerative AI on Databricks\nSupport for multimodal generative AI models\nMachine learning on Databricks\nDeep learning on Databricks\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  }
]