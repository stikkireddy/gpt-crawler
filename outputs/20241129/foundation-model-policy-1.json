[
  {
    "title": "Conduct your own LLM endpoint benchmarking | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/foundation-models/prov-throughput-run-benchmark.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nPre-trained models\nAI Gateway\nModel serving\nCustom Python models\nFoundation Model APIs\nProvisioned throughput Foundation Model APIs\nWhat do tokens per second ranges in provisioned throughput mean?\nConduct your own LLM endpoint benchmarking\nSupported models for pay-per-token\nFunction calling on Databricks\nStructured outputs on Databricks\nFoundation model REST API reference\nQuery generative AI models\nBatch inference\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Nov 27, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Serve models with Databricks  Databricks Foundation Model APIs  Provisioned throughput Foundation Model APIs  Conduct your own LLM endpoint benchmarking\nConduct your own LLM endpoint benchmarking\n\nFebruary 26, 2024\n\nThis article provides a Databricks recommended notebook example for benchmarking an LLM endpoint. It also includes a brief introduction to how Databricks performs LLM inference and calculates latency and throughput as endpoint performance metrics.\n\nLLM inference on Databricks measures tokens per second for provisioned throughput mode for Foundation Model APIs. See What do tokens per second ranges in provisioned throughput mean?.\n\nBenchmarking example notebook\n\nYou can import the following notebook into your Databricks environment and specify the name of your LLM endpoint to run a load test.\n\nBenchmarking an LLM endpoint\n\nOpen notebook in new tab\n Copy link for import\n\nExpand notebook ▼\nLLM inference introduction\n\nLLMs perform inference in a two-step process:\n\nPrefill, where the tokens in the input prompt are processed in parallel.\n\nDecoding, where text is generated one token at a time in an auto-regressive manner. Each generated token is appended to the input and fed back into the model to generate the next token. Generation stops when the LLM outputs a special stop token or when a user-defined condition is met.\n\nMost production applications have a latency budget, and Databricks recommends you maximize throughput given that latency budget.\n\nThe number of input tokens has a substantial impact on the required memory to process requests.\n\nThe number of output tokens dominates overall response latency.\n\nDatabricks divides LLM inference into the following sub-metrics:\n\nTime to first token (TTFT): This is how quickly users start seeing the model’s output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token.\n\nTime per output token (TPOT): Time to generate an output token for each user that is querying the system. This metric corresponds with how each user perceives the “speed” of the model. For example, a TPOT of 100 milliseconds per token would be 10 tokens per second, or ~450 words per minute, which is faster than a typical person can read.\n\nBased on these metrics, total latency and throughput can be defined as follows:\n\nLatency = TTFT + (TPOT) * (the number of tokens to be generated)\n\nThroughput = number of output tokens per second across all concurrency requests\n\nOn Databricks, LLM serving endpoints are able to scale to match the load sent by clients with multiple concurrent requests. There is a trade-off between latency and throughput. This is because, on LLM serving endpoints, concurrent requests can be and are processed at the same time. At low concurrent request loads, latency is the lowest possible. However, if you increase the request load, latency might go up, but throughput likely also goes up. This is because two requests worth of tokens per second can be processed in less than double the time.\n\nTherefore, controlling the number of parallel requests into your system is core to balancing latency with throughput. If you have a low latency use case, you want to send fewer concurrent requests to the endpoint to keep latency low. If you have a high throughput use case, you want to saturate the endpoint with lots of concurrency requests, since higher throughput is worth it even at the expense of latency.\n\nDatabricks benchmarking harness\n\nThe previously shared benchmarking example notebook is Databricks’ benchmarking harness. The notebook displays the latency and throughput metrics, and plots the throughput versus latency curve across different numbers of parallel requests. Databricks endpoint autoscaling is based on a “balanced” strategy between latency and throughput. In the notebook, you observe that as more concurrent users are querying the endpoint at the same time latency goes up as well as throughput.\n\nMore details on the Databricks philosophy about LLM performance benchmarking is described in the LLM Inference Performance Engineering: Best Practices blog.\n\nWas this article helpful?\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nBenchmarking example notebook\nLLM inference introduction\nDatabricks benchmarking harness\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Foundation model REST API reference | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/foundation-models/api-reference.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nPre-trained models\nAI Gateway\nModel serving\nCustom Python models\nFoundation Model APIs\nProvisioned throughput Foundation Model APIs\nSupported models for pay-per-token\nFunction calling on Databricks\nStructured outputs on Databricks\nFoundation model REST API reference\nQuery generative AI models\nBatch inference\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Nov 27, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Serve models with Databricks  Databricks Foundation Model APIs  Foundation model REST API reference\nFoundation model REST API reference\n\nOctober 16, 2024\n\nThis article provides general API information for Databricks Foundation Model APIs and the models they support. The Foundation Model APIs are designed to be similar to OpenAI’s REST API to make migrating existing projects easier. Both the pay-per-token and provisioned throughput endpoints accept the same REST API request format.\n\nEndpoints\n\nEach pay-per-token model has a single endpoint, and users can interact with these endpoints using HTTP POST requests. Provisioned throughput endpoints can be created using the API or the Serving UI. These endpoints also support multiple models per endpoint for A/B testing, as long as both served models expose the same API format. For example, both models are chat models.\n\nRequests and responses use JSON, the exact JSON structure depends on an endpoint’s task type. Chat and completion endpoints support streaming responses.\n\nPay-per-token workloads support certain models, see Supported models for pay-per-token for those models and accepted API formats.\n\nUsage\n\nResponses include a usage sub-message which reports the number of tokens in the request and response. The format of this sub-message is the same across all task types.\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\ncompletion_tokens\n\n\t\n\nInteger\n\n\t\n\nNumber of generated tokens. Not included in embedding responses.\n\n\n\n\nprompt_tokens\n\n\t\n\nInteger\n\n\t\n\nNumber of tokens from the input prompt(s).\n\n\n\n\ntotal_tokens\n\n\t\n\nInteger\n\n\t\n\nNumber of total tokens.\n\nFor models like llama-2-70b-chat a user prompt is transformed using a prompt template before being passed into the model. For pay-per-token endpoints, a system prompt might also be added. prompt_tokens includes all text added by our server.\n\nChat task\n\nChat tasks are optimized for multi-turn conversations with a model. Each request describes the conversation so far, where the messages field must alternate between user and assistant roles, ending with a user message. The model response provides the next assistant message in the conversation.\n\nChat request\n\nField\n\n\t\n\nDefault\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nmessages\n\n\t\t\n\nChatMessage list\n\n\t\n\nRequired. A list of messages representing the current conversation.\n\n\n\n\nmax_tokens\n\n\t\n\nnull\n\n\t\n\nnull, which means no limit, or an integer greater than zero\n\n\t\n\nThe maximum number of tokens to generate.\n\n\n\n\nstream\n\n\t\n\ntrue\n\n\t\n\nBoolean\n\n\t\n\nStream responses back to a client in order to allow partial results for requests. If this parameter is included in the request, responses are sent using the Server-sent events standard.\n\n\n\n\ntemperature\n\n\t\n\n1.0\n\n\t\n\nFloat in [0,2]\n\n\t\n\nThe sampling temperature. 0 is deterministic and higher values introduce more randomness.\n\n\n\n\ntop_p\n\n\t\n\n1.0\n\n\t\n\nFloat in (0,1]\n\n\t\n\nThe probability threshold used for nucleus sampling.\n\n\n\n\ntop_k\n\n\t\n\nnull\n\n\t\n\nnull, which means no limit, or an integer greater than zero\n\n\t\n\nDefines the number of k most likely tokens to use for top-k-filtering. Set this value to 1 to make outputs deterministic.\n\n\n\n\nstop\n\n\t\n\n[]\n\n\t\n\nString or List[String]\n\n\t\n\nModel stops generating further tokens when any one of the sequences in stop is encountered.\n\n\n\n\nn\n\n\t\n\n1\n\n\t\n\nInteger greater than zero\n\n\t\n\nThe API returns n independent chat completions when n is specified. Recommended for workloads that generate multiple completions on the same input for additional inference efficiency and cost savings. Only available for provisioned throughput endpoints.\n\n\n\n\ntool_choice\n\n\t\n\nnone\n\n\t\n\nString or ToolChoiceObject\n\n\t\n\nUsed only in conjunction with the tools field. tool_choice supports a variety of keyword strings such as auto, required, and none. auto means that you are letting the model decide which (if any) tool is relevant to use. With auto if the model doesn’t believe any of the tools in tools are relevant, the model generates a standard assistant message instead of a tool call. required means that the model picks the most relevant tool in tools and must generate a tool call. none means that the model does not generate any tool calls and instead must generate a standard assistant message. To force a tool call with a specific tool defined in tools, use a ToolChoiceObject. By default, if the tools field is populated tool_choice = \"auto\". Else, the tools field defaults to tool_choice = \"none\"\n\n\n\n\ntools\n\n\t\n\nnull\n\n\t\n\nToolObject\n\n\t\n\nA list of tools that the model can call. Currently, function is the only supported tool type and a max of 32 functions are supported.\n\n\n\n\nresponse_format\n\n\t\n\nnull\n\n\t\n\nResponseFormatObject\n\n\t\n\nAn object specifying the format that the model must output. Accepted types are text, json_schema or json_object\n\nSetting to { \"type\": \"json_schema\", \"json_schema\": {...} } enables structured outputs which ensures the model follows your supplied JSON schema.\n\nSetting to { \"type\": \"json_object\" } ensures the responses the model generates is valid JSON, but does not ensure that responses follow a specific schema.\n\n\n\n\nlogprobs\n\n\t\n\nfalse\n\n\t\n\nBoolean\n\n\t\n\nThis parameter indicates whether to provide the log probability of a token being sampled.\n\n\n\n\ntop_logprobs\n\n\t\n\nnull\n\n\t\n\nInteger\n\n\t\n\nThis parameter controls the number of most likely token candidates to return log probabilities for at each sampling step. Can be 0-20. logprobs must be true if using this field.\n\nChatMessage\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nrole\n\n\t\n\nString\n\n\t\n\nRequired. The role of the author of the message. Can be \"system\", \"user\", \"assistant\" or \"tool\".\n\n\n\n\ncontent\n\n\t\n\nString\n\n\t\n\nThe content of the message. Required for chat tasks that do not involve tool calls.\n\n\n\n\ntool_calls\n\n\t\n\nToolCall list\n\n\t\n\nThe list of tool_calls that the model generated. Must have role as \"assistant\" and no specification for the content field.\n\n\n\n\ntool_call_id\n\n\t\n\nString\n\n\t\n\nWhen role is \"tool\", the ID associated with the ToolCall that the message is responding to. Must be empty for other role options.\n\nThe system role can only be used once, as the first message in a conversation. It overrides the model’s default system prompt.\n\nToolCall\n\nA tool call action suggestion by the model. See Function calling on Databricks.\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nid\n\n\t\n\nString\n\n\t\n\nRequired. A unique identifier for this tool call suggestion.\n\n\n\n\ntype\n\n\t\n\nString\n\n\t\n\nRequired. Only \"function\" is supported.\n\n\n\n\nfunction\n\n\t\n\nFunctionCallCompletion\n\n\t\n\nRequired. A function call suggested by the model.\n\nFunctionCallCompletion\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nString\n\n\t\n\nRequired. The name of the function the model recommended.\n\n\n\n\narguments\n\n\t\n\nObject\n\n\t\n\nRequired. Arguments to the function as a serialized JSON dictionary.\n\nToolChoiceObject\n\nSee Function calling on Databricks.\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\ntype\n\n\t\n\nString\n\n\t\n\nRequired. The type of the tool. Currently, only \"function\" is supported.\n\n\n\n\nfunction\n\n\t\n\nObject\n\n\t\n\nRequired. An object defining which tool to call of the form {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} where \"my_function is the name of a FunctionObject in the tools field.\n\nToolObject\n\nSee Function calling on Databricks.\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\ntype\n\n\t\n\nString\n\n\t\n\nRequired. The type of the tool. Currently, only function is supported.\n\n\n\n\nfunction\n\n\t\n\nFunctionObject\n\n\t\n\nRequired. The function definition associated with the tool.\n\nFunctionObject\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nString\n\n\t\n\nRequired. The name of the function to be called.\n\n\n\n\ndescription\n\n\t\n\nObject\n\n\t\n\nRequired. The detailed description of the function. The model uses this description to understand the relevance of the function to the prompt and generate the tool calls with higher accuracy.\n\n\n\n\nparameters\n\n\t\n\nObject\n\n\t\n\nThe parameters the function accepts, described as a valid JSON schema object. If the tool is called, then the tool call is fit to the JSON schema provided. Omitting parameters defines a function without any parameters. The number of properties is limited to 15 keys.\n\n\n\n\nstrict\n\n\t\n\nBoolean\n\n\t\n\nWhether to enable strict schema adherence when generating the function call. If set to true, the model follows the exact schema defined in the schema field. Only a subset of JSON schema is supported when strict is true\n\nResponseFormatObject\n\nSee Structured outputs on Databricks.\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\ntype\n\n\t\n\nString\n\n\t\n\nRequired. The type of response format being defined. Either text for unstructured text, json_object for unstructured JSON objects, or json_schema for JSON objects adhering to a specific schema.\n\n\n\n\njson_schema\n\n\t\n\nJsonSchemaObject\n\n\t\n\nRequired. The JSON schema to adhere to if type is set to json_schema\n\nJsonSchemaObject\n\nSee Structured outputs on Databricks.\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nString\n\n\t\n\nRequired. The name of the response format.\n\n\n\n\ndescription\n\n\t\n\nString\n\n\t\n\nA description of what the response format is for, used by the model to determine how to respond in the format.\n\n\n\n\nschema\n\n\t\n\nObject\n\n\t\n\nRequired. The schema for the response format, described as a JSON schema object.\n\n\n\n\nstrict\n\n\t\n\nBoolean\n\n\t\n\nWhether to enable strict schema adherence when generating the output. If set to true, the model follows the exact schema defined in the schema field. Only a subset of JSON schema is supported when strict is true\n\nChat response\n\nFor non-streaming requests, the response is a single chat completion object. For streaming requests, the response is a text/event-stream where each event is a completion chunk object. The top-level structure of completion and chunk objects is almost identical: only choices has a different type.\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nid\n\n\t\n\nString\n\n\t\n\nUnique identifier for the chat completion.\n\n\n\n\nchoices\n\n\t\n\nList[ChatCompletionChoice] or List[ChatCompletionChunk] (streaming)\n\n\t\n\nList of chat completion texts. n choices are returned if the n parameter is specified.\n\n\n\n\nobject\n\n\t\n\nString\n\n\t\n\nThe object type. Equal to either \"chat.completions\" for non-streaming or \"chat.completion.chunk\" for streaming.\n\n\n\n\ncreated\n\n\t\n\nInteger\n\n\t\n\nThe time the chat completion was generated in seconds.\n\n\n\n\nmodel\n\n\t\n\nString\n\n\t\n\nThe model version used to generate the response.\n\n\n\n\nusage\n\n\t\n\nUsage\n\n\t\n\nToken usage metadata. May not be present on streaming responses.\n\nChatCompletionChoice\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nindex\n\n\t\n\nInteger\n\n\t\n\nThe index of the choice in the list of generated choices.\n\n\n\n\nmessage\n\n\t\n\nChatMessage\n\n\t\n\nA chat completion message returned by the model. The role will be assistant.\n\n\n\n\nfinish_reason\n\n\t\n\nString\n\n\t\n\nThe reason the model stopped generating tokens.\n\nChatCompletionChunk\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nindex\n\n\t\n\nInteger\n\n\t\n\nThe index of the choice in the list of generated choices.\n\n\n\n\ndelta\n\n\t\n\nChatMessage\n\n\t\n\nA chat completion message part of generated streamed responses from the model. Only the first chunk is guaranteed to have role populated.\n\n\n\n\nfinish_reason\n\n\t\n\nString\n\n\t\n\nThe reason the model stopped generating tokens. Only the last chunk will have this populated.\n\nCompletion task\n\nText completion tasks are for generating responses to a single prompt. Unlike Chat, this task supports batched inputs: multiple independent prompts can be sent in one request.\n\nCompletion request\n\nField\n\n\t\n\nDefault\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nprompt\n\n\t\t\n\nString or List[String]\n\n\t\n\nRequired. The prompt(s) for the model.\n\n\n\n\nmax_tokens\n\n\t\n\nnull\n\n\t\n\nnull, which means no limit, or an integer greater than zero\n\n\t\n\nThe maximum number of tokens to generate.\n\n\n\n\nstream\n\n\t\n\ntrue\n\n\t\n\nBoolean\n\n\t\n\nStream responses back to a client in order to allow partial results for requests. If this parameter is included in the request, responses are sent using the Server-sent events standard.\n\n\n\n\ntemperature\n\n\t\n\n1.0\n\n\t\n\nFloat in [0,2]\n\n\t\n\nThe sampling temperature. 0 is deterministic and higher values introduce more randomness.\n\n\n\n\ntop_p\n\n\t\n\n1.0\n\n\t\n\nFloat in (0,1]\n\n\t\n\nThe probability threshold used for nucleus sampling.\n\n\n\n\ntop_k\n\n\t\n\nnull\n\n\t\n\nnull, which means no limit, or an integer greater than zero\n\n\t\n\nDefines the number of k most likely tokens to use for top-k-filtering. Set this value to 1 to make outputs deterministic.\n\n\n\n\nerror_behavior\n\n\t\n\n\"error\"\n\n\t\n\n\"truncate\" or \"error\"\n\n\t\n\nFor timeouts and context-length-exceeded errors. One of: \"truncate\" (return as many tokens as possible) and \"error\" (return an error). This parameter is only accepted by pay per token endpoints.\n\n\n\n\nn\n\n\t\n\n1\n\n\t\n\nInteger greater than zero\n\n\t\n\nThe API returns n independent chat completions when n is specified. Recommended for workloads that generate multiple completions on the same input for additional inference efficiency and cost savings. Only available for provisioned throughput endpoints.\n\n\n\n\nstop\n\n\t\n\n[]\n\n\t\n\nString or List[String]\n\n\t\n\nModel stops generating further tokens when any one of the sequences in stop is encountered.\n\n\n\n\nsuffix\n\n\t\n\n\"\"\n\n\t\n\nString\n\n\t\n\nA string that is appended to the end of every completion.\n\n\n\n\necho\n\n\t\n\nfalse\n\n\t\n\nBoolean\n\n\t\n\nReturns the prompt along with the completion.\n\n\n\n\nuse_raw_prompt\n\n\t\n\nfalse\n\n\t\n\nBoolean\n\n\t\n\nIf true, pass the prompt directly into the model without any transformation.\n\nCompletion response\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nid\n\n\t\n\nString\n\n\t\n\nUnique identifier for the text completion.\n\n\n\n\nchoices\n\n\t\n\nCompletionChoice\n\n\t\n\nA list of text completions. For every prompt passed in, n choices are generated if n is specified. Default n is 1.\n\n\n\n\nobject\n\n\t\n\nString\n\n\t\n\nThe object type. Equal to \"text_completion\"\n\n\n\n\ncreated\n\n\t\n\nInteger\n\n\t\n\nThe time the completion was generated in seconds.\n\n\n\n\nusage\n\n\t\n\nUsage\n\n\t\n\nToken usage metadata.\n\nCompletionChoice\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nindex\n\n\t\n\nInteger\n\n\t\n\nThe index of the prompt in request.\n\n\n\n\ntext\n\n\t\n\nString\n\n\t\n\nThe generated completion.\n\n\n\n\nfinish_reason\n\n\t\n\nString\n\n\t\n\nThe reason the model stopped generating tokens.\n\nEmbedding task\n\nEmbedding tasks map input strings into embedding vectors. Many inputs can be batched together in each request.\n\nEmbedding request\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\ninput\n\n\t\n\nString or List[String]\n\n\t\n\nRequired. The input text to embed. Can be a string or a list of strings.\n\n\n\n\ninstruction\n\n\t\n\nString\n\n\t\n\nAn optional instruction to pass to the embedding model.\n\nInstructions are optional and highly model specific. For instance the The BGE authors recommend no instruction when indexing chunks and recommend using the instruction \"Represent this sentence for searching relevant passages:\" for retrieval queries. Other models like Instructor-XL support a wide range of instruction strings.\n\nEmbeddings response\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nid\n\n\t\n\nString\n\n\t\n\nUnique identifier for the embedding.\n\n\n\n\nobject\n\n\t\n\nString\n\n\t\n\nThe object type. Equal to \"list\".\n\n\n\n\nmodel\n\n\t\n\nString\n\n\t\n\nThe name of the embedding model used to create the embedding.\n\n\n\n\ndata\n\n\t\n\nEmbeddingObject\n\n\t\n\nThe embedding object.\n\n\n\n\nusage\n\n\t\n\nUsage\n\n\t\n\nToken usage metadata.\n\nEmbeddingObject\n\nField\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nobject\n\n\t\n\nString\n\n\t\n\nThe object type. Equal to \"embedding\".\n\n\n\n\nindex\n\n\t\n\nInteger\n\n\t\n\nThe index of the embedding in the list of embeddings generated by the model.\n\n\n\n\nembedding\n\n\t\n\nList[Float]\n\n\t\n\nThe embedding vector. Each model will return a fixed size vector (1024 for BGE-Large)\n\nAdditional resources\n\nDatabricks Foundation Model APIs\n\nQuery generative AI models\n\nSupported models for pay-per-token\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nEndpoints\nUsage\nChat task\nCompletion task\nEmbedding task\nAdditional resources\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "What do tokens per second ranges in provisioned throughput mean? | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/foundation-models/prov-throughput-tokens.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nPre-trained models\nAI Gateway\nModel serving\nCustom Python models\nFoundation Model APIs\nProvisioned throughput Foundation Model APIs\nWhat do tokens per second ranges in provisioned throughput mean?\nConduct your own LLM endpoint benchmarking\nSupported models for pay-per-token\nFunction calling on Databricks\nStructured outputs on Databricks\nFoundation model REST API reference\nQuery generative AI models\nBatch inference\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Nov 27, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Serve models with Databricks  Databricks Foundation Model APIs  Provisioned throughput Foundation Model APIs  What do tokens per second ranges in provisioned throughput mean?\nWhat do tokens per second ranges in provisioned throughput mean?\n\nFebruary 13, 2024\n\nThis article describes how and why Databricks measures tokens per second for provisioned throughput workloads for Foundation Model APIs.\n\nPerformance for large language models (LLMs) is often measured in terms of tokens per second. When configuring production model serving endpoints, it’s important to consider the number of requests your application sends to the endpoint. Doing so helps you understand if your endpoint needs to be configured to scale so as to not impact latency.\n\nWhen configuring the scale-out ranges for endpoints deployed with provisioned throughput, Databricks found it easier to reason about the inputs going into your system using tokens.\n\nWhat are tokens?\n\nLLMs read and generate text in terms of what is called a token. Tokens can be words or sub-words, and the exact rules for splitting text into tokens vary from model to model. For instance, you can use online tools to see how Llama’s tokenizer converts words to tokens.\n\nWhy measure LLM performance in terms of tokens per second?\n\nTraditionally, serving endpoints are configured based on the number of concurrent requests per second (RPS). However, an LLM inference request takes a different amount of time based on how many tokens are passed in and how many it generates, which can be imbalanced across requests. Therefore, deciding how much scale out your endpoint needs really requires measuring endpoint scale in terms of the content of your request - tokens.\n\nDifferent use cases feature different input and output token ratios:\n\nVarying lengths of input contexts: While some requests might involve only a few input tokens, for example a short question, others may involve hundreds or even thousands of tokens, like a long document for summarization. This variability makes configuring a serving endpoint based only on RPS challenging since it does not account for the varying processing demands of the different requests.\n\nVarying lengths of output depending on use case: Different use cases for LLMs can lead to vastly different output token lengths. Generating output tokens is the most time intensive part of LLM inference, so this can dramatically impact throughput. For example, summarization involves shorter, pithier responses, but text generation, like writing articles or product descriptions, can generate much longer answers.\n\nHow do I select the tokens per second range for my endpoint?\n\nProvisioned throughput serving endpoints are configured in terms of a range of tokens per second that you can send to the endpoint. The endpoint scales up and down to handle the load of your production application. You are charged per hour based on the range of tokens per second your endpoint is scaled to.\n\nThe best way to know what tokens per second range on your provisioned throughput serving endpoint works for your use case is to perform a load test with a representative dataset. See Conduct your own LLM endpoint benchmarking.\n\nThere are two important factors to consider:\n\nHow Databricks measures tokens per second performance of the LLM\n\nDatabricks benchmarks endpoints against a workload representing summarization tasks that are common for retrieval-augmented generation use cases. Specifically, the workload consists of:\n\n2048 input tokens\n\n256 output tokens\n\nThe token ranges displayed combine input and output token throughput and, by default, optimize for balancing throughput and latency.\n\nDatabricks benchmarks that users can send that many tokens per second concurrently to the endpoint at a batch size of 1 per request. This simulates multiple requests hitting the endpoint at the same time, which more accurately represents how you would actually use the endpoint in production.\n\nHow autoscaling works\n\nModel Serving features a rapid autoscaling system that scales the underlying compute to meet the tokens per second demand of your application. Databricks scales up provisioned throughput in chunks of tokens per second, so you are charged for additional units of provisioned throughput only when you’re using them.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat are tokens?\nWhy measure LLM performance in terms of tokens per second?\nHow do I select the tokens per second range for my endpoint?\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Provisioned throughput Foundation Model APIs | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nPre-trained models\nAI Gateway\nModel serving\nCustom Python models\nFoundation Model APIs\nProvisioned throughput Foundation Model APIs\nWhat do tokens per second ranges in provisioned throughput mean?\nConduct your own LLM endpoint benchmarking\nSupported models for pay-per-token\nFunction calling on Databricks\nStructured outputs on Databricks\nFoundation model REST API reference\nQuery generative AI models\nBatch inference\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Nov 27, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Serve models with Databricks  Databricks Foundation Model APIs  Provisioned throughput Foundation Model APIs\nProvisioned throughput Foundation Model APIs\n\nNovember 08, 2024\n\nThis article demonstrates how to deploy models using Foundation Model APIs provisioned throughput. Databricks recommends provisioned throughput for production workloads, and it provides optimized inference for foundation models with performance guarantees.\n\nWhat is provisioned throughput?\n\nProvisioned throughput refers to how many tokens worth of requests you can submit to an endpoint at the same time. Provisioned throughput serving endpoints are dedicated endpoints that are configured in terms of a range of tokens per second that you can send to the endpoint.\n\nSee the following resources for more information:\n\nWhat do tokens per second ranges in provisioned throughput mean?\nConduct your own LLM endpoint benchmarking\n\nSee Provisioned throughput Foundation Model APIs for a list of supported model architectures for provisioned throughput endpoints.\n\nRequirements\n\nSee requirements. For deploying fine-tuned foundation models, see Deploy fine-tuned foundation models.\n\n[Recommended] Deploy foundation models from Unity Catalog\n\nPreview\n\nThis feature is in Public Preview.\n\nDatabricks recommends using the foundation models that are pre-installed in Unity Catalog. You can find these models under the catalog system in the schema ai (system.ai).\n\nTo deploy a foundation model:\n\nNavigate to system.ai in Catalog Explorer.\n\nClick on the name of the model to deploy.\n\nOn the model page, click the Serve this model button.\n\nThe Create serving endpoint page appears. See Create your provisioned throughput endpoint using the UI.\n\nDeploy foundation models from Databricks Marketplace\n\nAlternatively, you can install foundation models to Unity Catalog from Databricks Marketplace.\n\nYou can search for a model family and from the model page, you can select Get access and provide login credentials to install the model to Unity Catalog.\n\nAfter the model is installed to Unity Catalog, you can create a model serving endpoint using the Serving UI.\n\nDeploy DBRX models\n\nDatabricks recommends serving the DBRX Instruct model for your workloads. To serve the DBRX Instruct model using provisioned throughput, follow the guidance in [Recommended] Deploy foundation models from Unity Catalog.\n\nWhen serving these DBRX models, provisioned throughput supports a context length of up to 16k.\n\nDBRX models use the following default system prompt to ensure relevance and accuracy in model responses:\n\nCopy\nYou are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\nYou assist with various tasks, from writing to coding (using markdown for code blocks — remember to use ``` with code, JSON, and tables).\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER'S QUERY.\n\nDeploy fine-tuned foundation models\n\nIf you cannot use the models in the system.ai schema or install models from the Databricks Marketplace, you can deploy a fine-tuned foundation model by logging it to Unity Catalog. This section and the following sections show how to set up your code to log an MLflow model to Unity Catalog and create your provisioned throughput endpoint using either the UI or the REST API.\n\nSee Provisioned throughput limits for the supported Meta Llama 3.1 and 3.2 fine-tuned models and their region availability.\n\nRequirements\n\nDeploying fine-tuned foundation models is only supported by MLflow 2.11 or above. Databricks Runtime 15.0 ML and above pre-installs the compatible MLflow version.\n\nDatabricks recommends using models in Unity Catalog for faster upload and download of large models.\n\nDefine catalog, schema and model name\n\nTo deploy a fine-tuned foundation model, define the target Unity Catalog catalog, schema, and the model name of your choice.\n\nCopy\nPython\nmlflow.set_registry_uri('databricks-uc')\nCATALOG = \"catalog\"\nSCHEMA = \"schema\"\nMODEL_NAME = \"model_name\"\nregistered_model_name = f\"{CATALOG}.{SCHEMA}.{MODEL_NAME}\"\n\nLog your model\n\nTo enable provisioned throughput for your model endpoint, you must log your model using the MLflow transformers flavor and specify the task argument with the appropriate model type interface from the following options:\n\n\"llm/v1/completions\"\n\n\"llm/v1/chat\"\n\n\"llm/v1/embeddings\"\n\nThese arguments specify the API signature used for the model serving endpoint. Please refer to the MLflow documentation for more details about these tasks and corresponding input/output schemas.\n\nThe following is an example of how to log a text-completion language model logged using MLflow:\n\nCopy\nPython\nmodel = AutoModelForCausalLM.from_pretrained(\"mosaicml/mixtral-8x7b-instruct\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"mosaicml/mixtral-8x7b-instruct\")\n\nwith mlflow.start_run():\n    components = {\n      \"model\": model,\n      \"tokenizer\": tokenizer,\n    }\n    mlflow.transformers.log_model(\n        transformers_model=components,\n        artifact_path=\"model\",\n        # Specify the llm/v1/xxx task that is compatible with the model being logged\n        task=\"llm/v1/completions\",\n        # Specify an input example that conforms to the input schema for the task.\n        input_example={\"prompt\": np.array([\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is Apache Spark?\\n\\n### Response:\\n\"])},\n        # By passing the model name, MLflow automatically registers the Transformers model to Unity Catalog with the given catalog/schema/model_name.\n        registered_model_name=registered_model_name\n        # Optionally, you can set save_pretrained to False to avoid unnecessary copy of model weight and gain more efficiency\n        save_pretrained=False\n    )\n\n\nNote\n\nIf you are using MLflow earlier than 2.12, you have to specify the task within metadata parameter of the same mlflow.transformer.log_model() function instead.\n\nmetadata = {\"task\": \"llm/v1/completions\"}\n\nmetadata = {\"task\": \"llm/v1/chat\"}\n\nmetadata = {\"task\": \"llm/v1/embeddings\"}\n\nProvisioned throughput also supports both the base and large GTE embedding models. The following is an example of how to log the model Alibaba-NLP/gte-large-en-v1.5 so that it can be served with provisioned throughput:\n\nCopy\nPython\nmodel = AutoModel.from_pretrained(\"Alibaba-NLP/gte-large-en-v1.5\")\ntokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-large-en-v1.5\")\nwith mlflow.start_run():\n    components = {\n      \"model\": model,\n      \"tokenizer\": tokenizer,\n    }\n    mlflow.transformers.log_model(\n        transformers_model=components,\n        artifact_path=\"model\",\n        task=\"llm/v1/embeddings\",\n        registered_model_name=registered_model_name,\n        # model_type is required for logging a fine-tuned BGE models.\n        metadata={\n            \"model_type\": \"gte-large\"\n        }\n    )\n\n\nAfter your model is logged in Unity Catalog, continue on Create your provisioned throughput endpoint using the UI to create a model serving endpoint with provisioned throughput.\n\nCreate your provisioned throughput endpoint using the UI\n\nAfter the logged model is in Unity Catalog, create a provisioned throughput serving endpoint with the following steps:\n\nNavigate to the Serving UI in your workspace.\n\nSelect Create serving endpoint.\n\nIn the Entity field, select your model from Unity Catalog. For eligible models, the UI for the served entity shows the Provisioned Throughput screen.\n\nIn the Up to dropdown you can configure the maximum tokens per second throughput for your endpoint.\n\nProvisioned throughput endpoints automatically scale, so you can select Modify to view the minimum tokens per second your endpoint can scale down to.\n\nCreate your provisioned throughput endpoint using the REST API\n\nTo deploy your model in provisioned throughput mode using the REST API, you must specify min_provisioned_throughput and max_provisioned_throughput fields in your request. If you prefer Python, you can also create an endpoint using the MLflow Deployment SDK.\n\nTo identify the suitable range of provisioned throughput for your model, see Get provisioned throughput in increments.\n\nCopy\nPython\nimport requests\nimport json\n\n# Set the name of the MLflow endpoint\nendpoint_name = \"llama2-13b-chat\"\n\n# Name of the registered MLflow model\nmodel_name = \"ml.llm-catalog.llama-13b\"\n\n# Get the latest version of the MLflow model\nmodel_version = 3\n\n# Get the API endpoint and token for the current notebook context\nAPI_ROOT = \"<YOUR-API-URL>\"\nAPI_TOKEN = \"<YOUR-API-TOKEN>\"\n\nheaders = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\noptimizable_info = requests.get(\n  url=f\"{API_ROOT}/api/2.0/serving-endpoints/get-model-optimization-info/{model_name}/{model_version}\",\n  headers=headers)\n  .json()\n\nif 'optimizable' not in optimizable_info or not optimizable_info['optimizable']:\n  raise ValueError(\"Model is not eligible for provisioned throughput\")\n\nchunk_size = optimizable_info['throughput_chunk_size']\n\n# Minimum desired provisioned throughput\nmin_provisioned_throughput = 2 * chunk_size\n\n# Maximum desired provisioned throughput\nmax_provisioned_throughput = 3 * chunk_size\n\n# Send the POST request to create the serving endpoint\ndata = {\n  \"name\": endpoint_name,\n  \"config\": {\n    \"served_entities\": [\n      {\n        \"entity_name\": model_name,\n        \"entity_version\": model_version,\n        \"min_provisioned_throughput\": min_provisioned_throughput,\n        \"max_provisioned_throughput\": max_provisioned_throughput,\n      }\n    ]\n  },\n}\n\nresponse = requests.post(\n  url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n)\n\nprint(json.dumps(response.json(), indent=4))\n\nLog probability for chat completion tasks\n\nFor chat completion tasks, you can use the logprobs parameter to provide the log probability of a token being sampled as part of the large language model generation process. You can use logprobs for a variety of scenarios including classification, assessing model uncertainty, and running evaluation metrics. See Chat task for parameter details.\n\nGet provisioned throughput in increments\n\nProvisioned throughput is available in increments of tokens per second with specific increments varying by model. To identify the suitable range for your needs, Databricks recommends using the model optimization information API within the platform.\n\nCopy\nBash\nGET api/2.0/serving-endpoints/get-model-optimization-info/{registered_model_name}/{version}\n\n\nThe following is an example response from the API:\n\nCopy\nJSON\n{\n  \"optimizable\": true,\n  \"model_type\": \"llama\",\n  \"throughput_chunk_size\": 980\n}\n\nNotebook examples\n\nThe following notebooks show examples of how to create a provisioned throughput Foundation Model API:\n\nProvisioned throughput serving for GTE model notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nProvisioned throughput serving for Mistral model notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nProvisioned throughput serving for BGE model notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLimitations\n\nModel deployment might fail due to GPU capacity issues, which results in a timeout during endpoint creation or update. Reach out to your Databricks account team to help resolve.\n\nAuto-scaling for Foundation Models APIs is slower than CPU model serving. Databricks recommends over-provisioning to avoid request timeouts.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat is provisioned throughput?\nRequirements\n[Recommended] Deploy foundation models from Unity Catalog\nDeploy foundation models from Databricks Marketplace\nDeploy DBRX models\nDeploy fine-tuned foundation models\nCreate your provisioned throughput endpoint using the UI\nCreate your provisioned throughput endpoint using the REST API\nNotebook examples\nLimitations\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Supported models for pay-per-token | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/foundation-models/supported-models.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nPre-trained models\nAI Gateway\nModel serving\nCustom Python models\nFoundation Model APIs\nProvisioned throughput Foundation Model APIs\nSupported models for pay-per-token\nFunction calling on Databricks\nStructured outputs on Databricks\nFoundation model REST API reference\nQuery generative AI models\nBatch inference\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Nov 27, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Serve models with Databricks  Databricks Foundation Model APIs  Supported models for pay-per-token\nSupported models for pay-per-token\n\nOctober 31, 2024\n\nImportant\n\nOnly the GTE Large (En) and Meta Llama 3.1 70B Instruct models are available in pay-per-token EU and US supported regions.\n\nSee Foundation Model APIs limits for the pay-per-token models only supported in US regions.\n\nThis article describes the state-of-the-art open models that are supported by the Databricks Foundation Model APIs in pay-per-token mode.\n\nYou can send query requests to these models using the pay-per-token endpoints available in your Databricks workspace. See Query generative AI models and pay-per-token supported models table for the names of the model endpoints to use.\n\nIn addition to supporting models in pay-per-token mode, Foundation Model APIs also offers provisioned throughput mode. Databricks recommends provisioned throughput for production workloads. This mode supports all models of a model architecture family (for example, DBRX models), including the fine-tuned and custom pre-trained models supported in pay-per-token mode. See Provisioned throughput Foundation Model APIs for the list of supported architectures.\n\nYou can interact with these supported models using the AI Playground.\n\nMeta Llama 3.1 405B Instruct\n\nPreview\n\nThe use of this model with Foundation Model APIs is in Public Preview. Reach out to your Databricks account team if you encounter endpoint failures or stabilization errors when using this model.\n\nImportant\n\nMeta Llama 3.1 is licensed under the LLAMA 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved. Customers are responsible for ensuring compliance with applicable model licenses.\n\nMeta-Llama-3.1-405B-Instruct is the largest openly available state-of-the-art large language model, built and trained by Meta. The use of this model enables customers to unlock new capabilities, such as advanced, multi-step reasoning and high-quality synthetic data generation. This model is competitive with GPT-4-Turbo in terms of quality.\n\nLike Meta-Llama-3.1-70B-Instruct, this model has a context of 128,000 tokens and support across ten languages. It aligns with human preferences for helpfulness and safety, and is optimized for dialogue use cases. Learn more about the Meta Llama 3.1 models.\n\nSimilar to other large language models, Llama-3.1’s output may omit some facts and occasionally produce false information. Databricks recommends using retrieval augmented generation (RAG) in scenarios where accuracy is especially important.\n\nDBRX Instruct\n\nImportant\n\nDBRX is provided under and subject to the Databricks Open Model License, Copyright © Databricks, Inc. All rights reserved. Customers are responsible for ensuring compliance with applicable model licenses, including the Databricks Acceptable Use policy.\n\nDBRX Instruct is a state-of-the-art mixture of experts (MoE) language model trained by Databricks.\n\nThe model outperforms established open source models on standard benchmarks, and excels at a broad set of natural language tasks such as: text summarization, question-answering, extraction and coding.\n\nDBRX Instruct can handle up to 32k tokens of input length, and generates outputs of up to 4k tokens. Thanks to its MoE architecture, DBRX Instruct is highly efficient for inference, activating only 36B parameters out of a total of 132B trained parameters. The pay-per-token endpoint that serves this model has a rate limit of one query per second. See Model Serving limits and regions.\n\nSimilar to other large language models, DBRX Instruct output may omit some facts and occasionally produce false information. Databricks recommends using retrieval augmented generation (RAG) in scenarios where accuracy is especially important.\n\nDBRX models use the following default system prompt to ensure relevance and accuracy in model responses:\n\nCopy\nYou are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\nYou assist with various tasks, from writing to coding (using markdown for code blocks — remember to use ``` with code, JSON, and tables).\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER'S QUERY.\n\nMeta Llama 3.1 70B Instruct\n\nImportant\n\nStarting July 23, 2024, Meta-Llama-3.1-70B-Instruct replaces support for Meta-Llama-3-70B-Instruct in Foundation Model APIs pay-per-token endpoints.\n\nImportant\n\nMeta Llama 3.1 is licensed under the LLAMA 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved. Customers are responsible for ensuring compliance with applicable model licenses.\n\nMeta-Llama-3.1-70B-Instruct is a state-of-the-art large language model with a context of 128,000 tokens that was built and trained by Meta. The model has support across ten languages, aligns with human preferences for helpfulness and safety, and is optimized for dialogue use cases. Learn more about the Meta Llama 3.1 models.\n\nSimilar to other large language models, Llama-3’s output may omit some facts and occasionally produce false information. Databricks recommends using retrieval augmented generation (RAG) in scenarios where accuracy is especially important.\n\nMixtral-8x7B Instruct\n\nMixtral-8x7B Instruct is a high-quality sparse mixture of experts model (SMoE) trained by Mistral AI. Mixtral-8x7B Instruct can be used for a variety of tasks such as question-answering, summarization, and extraction.\n\nMixtral can handle context lengths up to 32k tokens. Mixtral can process English, French, Italian, German, and Spanish. Mixtral matches or outperforms Llama 2 70B and GPT3.5 on most benchmarks (Mixtral performance), while being four times faster than Llama 70B during inference.\n\nSimilar to other large language models, Mixtral-8x7B Instruct model should not be relied on to produce factually accurate information. While great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs. To reduce risk, Databricks defaults to using a variant of Mistral’s safe mode system prompt.\n\nGTE Large (En)\n\nImportant\n\nGTE Large (En) is provided under and subject to the Apache 2.0 License, Copyright © The Apache Software Foundation, All rights reserved. Customers are responsible for ensuring compliance with applicable model licenses.\n\nGeneral Text Embedding (GTE) is a text embedding model that can map any text to a 1024-dimension embedding vector and an embedding window of 8192 tokens. These vectors can be used in vector databases for LLMs, and for tasks like retrieval, classification, question-answering, clustering, or semantic search. This endpoint serves the English version of the model and does not generate normalized embeddings.\n\nEmbedding models are especially effective when used in tandem with LLMs for retrieval augmented generation (RAG) use cases. GTE can be used to find relevant text snippets in large chunks of documents that can be used in the context of an LLM.\n\nBGE Large (En)\n\nBAAI General Embedding (BGE) is a text embedding model that can map any text to a 1024-dimension embedding vector and an embedding window of 512 tokens. These vectors can be used in vector databases for LLMs, and for tasks like retrieval, classification, question-answering, clustering, or semantic search. This endpoint serves the English version of the model and generates normalized embeddings.\n\nEmbedding models are especially effective when used in tandem with LLMs for retrieval augmented generation (RAG) use cases. BGE can be used to find relevant text snippets in large chunks of documents that can be used in the context of an LLM.\n\nIn RAG applications, you may be able to improve the performance of your retrieval system by including an instruction parameter. The BGE authors recommend trying the instruction \"Represent this sentence for searching relevant passages:\" for query embeddings, though its performance impact is domain dependent.\n\nAdditional resources\n\nQuery generative AI models\n\nFoundation model REST API reference\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nMeta Llama 3.1 405B Instruct\nDBRX Instruct\nMeta Llama 3.1 70B Instruct\nMixtral-8x7B Instruct\nGTE Large (En)\nBGE Large (En)\nAdditional resources\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Databricks Foundation Model APIs | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/foundation-models/index.html#token-foundation-apis",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Nov 27, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Serve models with Databricks  Databricks Foundation Model APIs\nDatabricks Foundation Model APIs\n\nNovember 25, 2024\n\nThis article provides an overview of the Foundation Model APIs in Databricks. It includes requirements for use, supported models, and limitations.\n\nWhat are Databricks Foundation Model APIs?\n\nMosaic AI Model Serving now supports Foundation Model APIs which allow you to access and query state-of-the-art open models from a serving endpoint. With Foundation Model APIs, you can quickly and easily build applications that leverage a high-quality generative AI model without maintaining your own model deployment. Foundation Model APIs is a Databricks Designated Service, which means that it uses Databricks Geos to manage data residency when processing customer content.\n\nThe Foundation Model APIs are provided in two pricing modes:\n\nPay-per-token: This is the easiest way to start accessing foundation models on Databricks and is recommended for beginning your journey with Foundation Model APIs. This mode is not designed for high-throughput applications or performant production workloads.\n\nProvisioned throughput: This mode is recommended for all production workloads, especially those that require high throughput, performance guarantees, fine-tuned models, or have additional security requirements. Provisioned throughput endpoints are available with compliance certifications like HIPAA.\n\nSee Use Foundation Model APIs for guidance on how to use these two modes and the supported models.\n\nUsing the Foundation Model APIs you can:\n\nQuery a generalized LLM to verify a project’s validity before investing more resources.\n\nQuery a generalized LLM in order to create a quick proof-of-concept for an LLM-based application before investing in training and deploying a custom model.\n\nUse a foundation model, along with a vector database, to build a chatbot using retrieval augmented generation (RAG).\n\nReplace proprietary models with open alternatives to optimize for cost and performance.\n\nEfficiently compare LLMs to see which is the best candidate for your use case, or swap a production model with a better performing one.\n\nBuild an LLM application for development or production on top of a scalable, SLA-backed LLM serving solution that can support your production traffic spikes.\n\nRequirements\n\nDatabricks API token to authenticate endpoint requests.\n\nServerless compute (for provisioned throughput models).\n\nA workspace in a supported region:\n\nPay-per-token regions.\n\nProvisioned throughput regions.\n\nUse Foundation Model APIs\n\nYou have multiple options for using the Foundation Model APIs.\n\nThe APIs are compatible with OpenAI, so you can use the OpenAI client for querying. You can also use the UI, the Foundation Models APIs Python SDK, the MLflow Deployments SDK, or the REST API for querying supported models. Databricks recommends using the OpenAI client SDK or API for extended interactions and the UI for trying out the feature.\n\nSee Query generative AI models for scoring examples.\n\nPay-per-token Foundation Model APIs\n\nPay-per-tokens models are accessible in your Databricks workspace, and are recommended for getting started. To access them in your workspace, navigate to the Serving tab in the left sidebar. The Foundation Model APIs are located at the top of the Endpoints list view.\n\nThe following table summarizes the supported models for pay-per-token. See Supported models for pay-per-token for additional model information.\n\nIf you want to test out and chat with these models you can do so using the AI Playground. See Chat with LLMs and prototype GenAI apps using AI Playground.\n\nImportant\n\nStarting July 23, 2024, Meta-Llama-3.1-70B-Instruct replaces support for Meta-Llama-3-70B-Instruct in Foundation Model APIs pay-per-token endpoints.\n\nThe following models are now retired. See Retired models for recommended replacement models.\n\nLlama 2 70B Chat\n\nMPT 7B Instruct\n\nMPT 30B Instruct\n\nModel\n\n\t\n\nTask type\n\n\t\n\nEndpoint\n\n\t\n\nNotes\n\n\n\n\nGTE Large (English)\n\n\t\n\nEmbedding\n\n\t\n\ndatabricks-gte-large-en\n\n\t\n\nDoes not generate normalized embeddings.\n\n\n\n\nMeta-Llama-3.1-70B-Instruct\n\n\t\n\nChat\n\n\t\n\ndatabricks-meta-llama-3-1-70b-instruct\n\n\t\n\n\nMeta-Llama-3.1-405B-Instruct*\n\n\t\n\nChat\n\n\t\n\ndatabricks-meta-llama-3-1-405b-instruct\n\n\t\n\nSee Foundation Model APIs limits for region availability.\n\n\n\n\nDBRX Instruct\n\n\t\n\nChat\n\n\t\n\ndatabricks-dbrx-instruct\n\n\t\n\nSee Foundation Model APIs limits for region availability.\n\n\n\n\nMixtral-8x7B Instruct\n\n\t\n\nChat\n\n\t\n\ndatabricks-mixtral-8x7b-instruct\n\n\t\n\nSee Foundation Model APIs limits for region availability.\n\n\n\n\nBGE Large (English)\n\n\t\n\nEmbedding\n\n\t\n\ndatabricks-bge-large-en\n\n\t\n\nSee Foundation Model APIs limits for region availability.\n\n* Reach out to your Databricks account team if you encounter endpoint failures or stabilization errors when using this model.\n\nSee Query generative AI models for guidance on how to query Foundation Model APIs.\n\nSee Foundation model REST API reference for required parameters and syntax.\n\nProvisioned throughput Foundation Model APIs\n\nProvisioned throughput provides endpoints with optimized inference for foundation model workloads that require performance guarantees. Databricks recommends provisioned throughput for production workloads. See Provisioned throughput Foundation Model APIs for a step-by-step guide on how to deploy Foundation Model APIs in provisioned throughout mode.\n\nProvisioned throughput support includes:\n\nBase models of all sizes, such as DBRX Base. Base models can be accessed using the Databricks Marketplace, or you can alternatively download them from Hugging Face or another external source and register them in the Unity Catalog. The latter approach works with any fine-tuned variant of the supported models, irrespective of the fine-tuning method employed.\n\nFine-tuned variants of base models, such as LlamaGuard-7B or meta-llama/Llama-3.1-8B. This includes models that are fine-tuned on proprietary data.\n\nFully custom weights and tokenizers, such as those trained from scratch or continued pre-trained or other variations using the base model architecture (such as CodeLlama).\n\nThe following table summarizes the supported model architectures for provisioned throughput.\n\nImportant\n\nMeta Llama 3.2 is licensed under the LLAMA 3.2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved. Customers are responsible for ensuring their compliance with the terms of this license and the Llama 3.2 Acceptable Use Policy.\n\nMeta Llama 3.1 are licensed under the LLAMA 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved. Customers are responsible for ensuring compliance with applicable model licenses.\n\nModel architecture\n\n\t\n\nTask types\n\n\t\n\nNotes\n\n\n\n\nMeta Llama 3.2 3B\n\n\t\n\nChat or Completion\n\n\t\n\nSee Provisioned throughput limits for supported model variants and region availability.\n\n\n\n\nMeta Llama 3.2 1B\n\n\t\n\nChat or Completion\n\n\t\n\nSee Provisioned throughput limits for supported model variants and region availability.\n\n\n\n\nMeta Llama 3.1\n\n\t\n\nChat or Completion\n\n\t\n\nSee Provisioned throughput limits for supported model variants and region availability.\n\n\n\n\nMeta Llama 3\n\n\t\n\nChat or Completion\n\n\t\n\n\nMeta Llama 2\n\n\t\n\nChat or Completion\n\n\t\n\n\nDBRX\n\n\t\n\nChat or Completion\n\n\t\n\nSee Provisioned throughput limits for region availability.\n\n\n\n\nMistral\n\n\t\n\nChat or Completion\n\n\t\n\n\nMixtral\n\n\t\n\nChat or Completion\n\n\t\n\n\nMPT\n\n\t\n\nChat or Completion\n\n\t\n\n\nGTE v1.5 (English)\n\n\t\n\nEmbedding\n\n\t\n\nDoes not generate normalized embeddings.\n\n\n\n\nBGE v1.5 (English)\n\n\t\n\nEmbedding\n\n\t\nLimitations\n\nSee Foundation Model APIs limits.\n\nAdditional resources\n\nQuery generative AI models\n\nProvisioned throughput Foundation Model APIs\n\nPerform batch inference using ai_query\n\nSupported models for pay-per-token\n\nFoundation model REST API reference\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat are Databricks Foundation Model APIs?\nRequirements\nUse Foundation Model APIs\nLimitations\nAdditional resources\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Generative AI models maintenance policy | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/foundation-models/retired-models-policy.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Nov 27, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Generative AI models maintenance policy\nGenerative AI models maintenance policy\n\nNovember 21, 2024\n\nThis article describes the model maintenance policy for the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nIn order to continue supporting the most state-of-the-art models, Databricks might update supported models or retire older models for the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nModel retirement policy\n\nThe following retirement policy applies only to supported chat and completion models in the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nWhen a model is retired, it is no longer available for use and is removed from the indicated feature offerings. Databricks takes the following steps to notify customers about a model that is set for retirement:\n\nA warning message displays in the model card from the Serving page of your Databricks workspace that indicates that the model is planned for retirement.\n\nA warning message displays in the dropdown menu for Foundation Model Fine-tuning in the Experiments tab that indicates that the model is planned for retirement.\n\nThe applicable documentation contains a notice that indicates the model is planned for retirement and the start date it will no longer be supported.\n\nAfter users are notified about the upcoming model retirement, Databricks will retire the model in three months. During this three-month period, customers can either:\n\nChoose to migrate to a provisioned throughput endpoint to continue using the model past its end-of-life date\n\nMigrate existing workflows to use recommended replacement models.\n\nOn the retirement date, the model is removed from the product, and applicable documentation is updated to recommend using a replacement model.\n\nSee Retired models for a list of currently retired models and planned retirement dates.\n\nModel updates\n\nDatabricks might ship incremental updates to pay-per-token models to deliver optimizations. When a model is updated, the endpoint URL remains the same, but the model ID in the response object changes to reflect the date of the update. For example, if an update is shipped to meta-llama/Meta-Llama-3.1-405B on 3/4/2024, the model name in the response object updates to meta-llama/Meta-Llama-3.1-405B-030424. Databricks maintains a version history of the updates that you can refer to.\n\nRetired models\n\nThe following sections summarize current and upcoming model retirements for the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nFoundation Model Fine-tuning retirements\n\nThe following table shows retired model families, their retirement dates, and recommended replacement model families to use for Foundation Model Fine-tuning workloads. Databricks recommends that you migrate your applications to use replacement models before the indicated retirement date.\n\nModel family\n\n\t\n\nRetirement date\n\n\t\n\nRecommended replacement model family\n\n\n\n\nMeta-Llama-3\n\n\t\n\nDecember 13, 2024\n\n\t\n\nMeta-Llama-3.1\n\n\n\n\nMeta-Llama-2\n\n\t\n\nDecember 13, 2024\n\n\t\n\nMeta-Llama-3.1\n\n\n\n\nCode Llama\n\n\t\n\nDecember 13, 2024\n\n\t\n\nMeta-Llama-3.1\n\nFoundation Model APIs pay-per-token retirements\n\nThe following table shows model retirements, their retirement dates, and recommended replacement models to use for Foundation Model APIs pay-per-token serving workloads. Databricks recommends that you migrate your applications to use replacement models before the indicated retirement date.\n\nImportant\n\nOn July 23, 2024, Meta-Llama-3.1-70B-Instruct replaced support for Meta-Llama-3-70B-Instruct in Foundation Model APIs pay-per-token endpoints.\n\nModel\n\n\t\n\nRetirement date\n\n\t\n\nRecommended replacement model\n\n\n\n\nMeta-Llama-3-70B-Instruct\n\n\t\n\nJuly 23, 2024\n\n\t\n\nMeta-Llama-3.1-70B-Instruct\n\n\n\n\nMeta-Llama-2-70B-Chat\n\n\t\n\nOctober 30, 2024\n\n\t\n\nMeta-Llama-3.1-70B-Instruct\n\n\n\n\nMPT 7B Instruct\n\n\t\n\nAugust 30, 2024\n\n\t\n\nMixtral-8x7B\n\n\n\n\nMPT 30B Instruct\n\n\t\n\nAugust 30, 2024\n\n\t\n\nMixtral-8x7B\n\nIf you require long-term support for a specific model version, Databricks recommends using Foundation Model APIs provisioned throughput for your serving workloads.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nModel retirement policy\nModel updates\nRetired models\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  }
]