[
  {
    "title": "Generative AI models maintenance policy | Databricks Documentation",
    "url": "https://docs.databricks.com/aws/en/machine-learning/retired-models-policy",
    "html": "Skip to main content\nGet started\nDevelopers\nReference\nRelease notes\nResources\nEnglish\nAWS\nTRY DATABRICKS\nDatabricks on AWS\nGet started\nWhat is Databricks?\nRelease notes\nData guides\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nAI Gateway\nDeploy models\nTrain models\nServe data for AI\nGuide: Build gen AI apps\nBuild gen AI apps\nTrace gen AI apps\nEvaluate gen AI apps\nMLOps\nMLflow for AI agent and ML model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nData warehousing\nBusiness intelligence\nCompute\nNotebooks\nDevelopers\nTechnology partners\nAdministration\nSecurity and compliance\nData governance (Unity Catalog)\nReference\nResources\nAI and machine learningGen AI model maintenance policy\nGenerative AI models maintenance policy\n\nThis article describes the model maintenance policy for the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nIn order to continue supporting the most state-of-the-art models, Databricks might update supported models or retire older models for the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nModel retirement policy​\n\nThe following retirement policy applies only to supported chat and completion models in the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nWhen a model is retired, it is no longer available for use and is removed from the indicated feature offerings. Databricks takes the following steps to notify customers about a model that is set for retirement:\n\nA warning message displays in the model card from the Serving page of your Databricks workspace that indicates that the model is planned for retirement.\nA warning message displays in the dropdown menu for Foundation Model Fine-tuning in the Experiments tab that indicates that the model is planned for retirement.\nThe applicable documentation contains a notice that indicates the model is planned for retirement and the start date it will no longer be supported.\n\nAfter users are notified about the upcoming model retirement, Databricks will retire the model in three months. During this three-month period, customers can either:\n\nChoose to migrate to a provisioned throughput endpoint to continue using the model past its end-of-life date.\nMigrate existing workflows to use recommended replacement models.\n\nOn the retirement date, the model is removed from the product, and applicable documentation is updated to recommend using a replacement model.\n\nSee Retired models for a list of currently retired models and planned retirement dates.\n\nModel updates​\n\nDatabricks might ship incremental updates to pay-per-token models to deliver optimizations. When a model is updated, the endpoint URL remains the same, but the model ID in the response object changes to reflect the date of the update. For example, if an update is shipped to meta-llama/Meta-Llama-3.3-70B on 3/4/2024, the model name in the response object updates to meta-llama/Meta-Llama-3.3-70B-030424. Databricks maintains a version history of the updates that you can refer to.\n\nRetired models​\n\nThe following sections summarize current and upcoming model retirements for the Foundation Model APIs pay-per-token and Foundation Model Fine-tuning offerings.\n\nFoundation Model Fine-tuning retirements​\n\nThe following table shows retired model families, their retirement dates, and recommended replacement model families to use for Foundation Model Fine-tuning workloads. Databricks recommends that you migrate your applications to use replacement models before the indicated retirement date.\n\nModel family\tRetirement date\tRecommended replacement model family\nDBRX\tApril 30, 2025\tLlama-3.1-70B\nMixtral\tApril 30, 2025\tLlama-3.1-70B\nMistral\tApril 30, 2025\tLlama-3.1-8B\nMeta-Llama-3.1-405B\tJanuary 30, 2025\tLlama-3.1-70B\nMeta-Llama-3\tJanuary 7, 2025\tMeta-Llama-3.1\nMeta-Llama-2\tJanuary 7, 2025\tMeta-Llama-3.1\nCode Llama\tJanuary 7, 2025\tMeta-Llama-3.1\nFoundation Model APIs pay-per-token retirements​\n\nThe following table shows model retirements, their retirement dates, and recommended replacement models to use for Foundation Model APIs pay-per-token serving workloads. Databricks recommends that you migrate your applications to use replacement models before the indicated retirement date.\n\nIMPORTANT\n\nOn December 11, 2024, Meta-Llama-3.3-70B-Instruct replaced support for Meta-Llama-3.1-70B-Instruct in Foundation Model APIs pay-per-token endpoints.\n\nModel\tRetirement date\tRecommended replacement model\nDBRX Instruct\tApril 30, 2025\tMeta-Llama-3.3-70B-Instruct\nMixtral-8x7B Instruct\tApril 30, 2025\tMeta-Llama-3.3-70B-Instruct\nMeta-Llama-3.1-70B-Instruct\tDecember 11, 2024\tMeta-Llama-3.3-70B-Instruct\nMeta-Llama-3-70B-Instruct\tJuly 23, 2024\tMeta-Llama-3.3-70B-Instruct\nMeta-Llama-2-70B-Chat\tOctober 30, 2024\tMeta-Llama-3.3-70B-Instruct\nMPT 7B Instruct\tAugust 30, 2024\tMeta-Llama-3.3-70B-Instruct\nMPT 30B Instruct\tAugust 30, 2024\tMeta-Llama-3.3-70B-Instruct\n\nIf you require long-term support for a specific model version, Databricks recommends using Foundation Model APIs provisioned throughput for your serving workloads.\n\nLast updated on Jan 29, 2025\nModel retirement policy\nModel updates\nRetired models\nFoundation Model Fine-tuning retirements\nFoundation Model APIs pay-per-token retirements\nWas this article helpful?\nSend us feedback·Privacy Notice·Terms of Use·Modern Slavery Statement·California Privacy·Your Privacy Choices \n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nManage Preferences\nReject All Accept All"
  }
]