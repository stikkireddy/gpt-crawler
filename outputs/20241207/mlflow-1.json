[
  {
    "title": "Upgrade models to Unity Catalog | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/upgrade-models.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nUpgrade ML workflows to target models in Unity Catalog\nUpgrade models to Unity Catalog\nManage model lifecycle using the Workspace Model Registry (legacy)\nWorkspace Model Registry example\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Manage model lifecycle in Unity Catalog  Upgrade models to Unity Catalog\nUpgrade models to Unity Catalog\n\nOctober 10, 2023\n\nDatabricks recommends deploying ML pipelines as code, rather than deploying individual ML models. The recommended approach for migration is to upgrade ML pipelines to use models in Unity Catalog.\n\nIn some cases, you might want to migrate individual models to Unity Catalog for initial testing, or for particularly critical models. The notebook demonstrates how to upgrade existing models to Unity Catalog.\n\nUpgrade Models to Unity Catalog\n\nOpen notebook in new tab\n Copy link for import\n\nExpand notebook ▼\nWas this article helpful?\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Share models across workspaces | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nUpgrade ML workflows to target models in Unity Catalog\nUpgrade models to Unity Catalog\nManage model lifecycle using the Workspace Model Registry (legacy)\nMLflow Model Registry Webhooks on Databricks\nShare models across workspaces\nWorkspace Model Registry example\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Manage model lifecycle in Unity Catalog  Manage model lifecycle using the Workspace Model Registry (legacy)  Share models across workspaces\nShare models across workspaces\n\nNovember 21, 2024\n\nImportant\n\nDatabricks recommends using Models in Unity Catalog to share models across workspaces. The approach in this article is deprecated.\n\nDatabricks supports sharing models across multiple workspaces. For example, you can develop and log a model in a development workspace, and then access and compare it against models in a separate production workspace. This is useful when multiple teams share access to models or when your organization has multiple workspaces to handle the different stages of development. For cross-workspace model development and deployment, Databricks recommends the deploy code approach, where the model training code is deployed to multiple environments.\n\nIn multi-workspace situations, you can access models across Databricks workspaces by using a remote model registry. For example, data scientists could access the production model registry with read-only access to compare their in-development models against the current production models. An example multi-workspace set-up is shown below.\n\nAccess to a remote registry is controlled by tokens. Each user or script that needs access creates a personal access token in the remote registry and copies that token into the secret manager of their local workspace. Each API request sent to the remote registry workspace must include the access token; MLflow provides a simple mechanism to specify the secrets to be used when performing model registry operations.\n\nNote\n\nAs a security best practice when you authenticate with automated tools, systems, scripts, and apps, Databricks recommends that you use OAuth tokens.\n\nIf you use personal access token authentication, Databricks recommends using personal access tokens belonging to service principals instead of workspace users. To create tokens for service principals, see Manage tokens for a service principal.\n\nAll client and fluent API methods for model registry are supported for remote workspaces.\n\nRequirements\n\nUsing a model registry across workspaces requires the MLflow Python client, release 1.11.0 or above.\n\nNote\n\nThis workflow is implemented from logic in the MLflow client. Ensure that the environment running the client has access to make network requests against the Databricks workspace containing the remote model registry. A common restriction put on the registry workspace is an IP allow list, which can disallow connections from MLflow clients running in a cluster in another workspace.\n\nSet up the API token for a remote registry\n\nIn the model registry workspace, create an access token.\n\nIn the local workspace, create secrets to store the access token and the remote workspace information:\n\nCreate a secret scope: databricks secrets create-scope <scope>.\n\nPick a unique name for the target workspace, shown here as <prefix>. Then create three secrets:\n\ndatabricks secrets put-secret <scope> <prefix>-host : Enter the hostname of the model registry workspace. For example, https://cust-success.cloud.databricks.com/.\n\ndatabricks secrets put-secret <scope> <prefix>-token : Enter the access token from the model registry workspace.\n\ndatabricks secrets put-secret <scope> <prefix>-workspace-id : Enter the workspace ID for the model registry workspace which can be found in the URL of any page.\n\nNote\n\nYou might want to share the secret scope with other users, since there is a limit on the number of secret scopes per workspace.\n\nSpecify a remote registry\n\nBased on the secret scope and name prefix you created for the remote registry workspace, you can construct a registry URI of the form:\n\nCopy\nPython\nregistry_uri = f'databricks://<scope>:<prefix>'\n\n\nYou can use the URI to specify a remote registry for fluent API methods by first calling:\n\nCopy\nPython\nmlflow.set_registry_uri(registry_uri)\n\n\nOr, you can specify it explicitly when you instantiate an MlflowClient:\n\nCopy\nPython\nclient = MlflowClient(registry_uri=registry_uri)\n\n\nThe following workflows show examples of both approaches.\n\nRegister a model in the remote registry\n\nOne way to register a model is to use the mlflow.register_model API:\n\nCopy\nPython\nmlflow.set_registry_uri(registry_uri)\nmlflow.register_model(model_uri=f'runs:/<run-id>/<artifact-path>', name=model_name)\n\n\nExamples for other model registration methods can be found in the notebook at the end of this page.\n\nNote\n\nRegistering a model in a remote workspace creates a temporary copy of the model artifacts in DBFS in the remote workspace. You may want to delete this copy once the model version is in READY status. The temporary files can be found under the /dbfs/databricks/mlflow/tmp-external-source/<run-id> folder.\n\nYou can also specify a tracking_uri to point to a MLflow Tracking service in another workspace in a similar manner to registry_uri. This means you can take a run on a remote workspace and register its model in the current or another remote workspace.\n\nUse a model from the remote registry\n\nYou can load and use a model version in a remote registry with mlflow.<flavor>.load_model methods by first setting the registry URI:\n\nCopy\nPython\nmlflow.set_registry_uri(registry_uri)\nmodel = mlflow.pyfunc.load_model(f'models:/<model-name>/Staging')\nmodel.predict(...)\n\n\nOr, you can explicitly specify the remote registry in the models:/ URI:\n\nCopy\nPython\nmodel = mlflow.pyfunc.load_model(f'models://<scope>:<prefix>@databricks/<model-name>/Staging')\nmodel.predict(...)\n\n\nOther helper methods for accessing the model files are also supported, such as:\n\nCopy\nPython\nclient.get_latest_versions(model_name)\nclient.get_model_version_download_uri(model_name, version)\n\nManage a model in the remote registry\n\nYou can perform any action on models in the remote registry as long as you have the required permissions. For example, if you have CAN MANAGE permissions on a model, you can transition a model version stage or delete the model using MlflowClient methods:\n\nCopy\nPython\nclient = MlflowClient(tracking_uri=None, registry_uri=registry_uri)\nclient.transition_model_version_stage(model_name, version, 'Archived')\nclient.delete_registered_model(model_name)\n\nNotebook example: Remote model registry\n\nThe following notebook is applicable for workspaces that are not enabled for Unity Catalog. It shows how to log models to the MLflow tracking server from the current workspace, and register the models into Model Registry in a different workspace. Databricks recommends using Models in Unity Catalog to share models across workspaces.\n\nRemote Model Registry example notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nSet up the API token for a remote registry\nSpecify a remote registry\nRegister a model in the remote registry\nUse a model from the remote registry\nManage a model in the remote registry\nNotebook example: Remote model registry\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "MLflow Model Registry Webhooks on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/model-registry-webhooks.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nUpgrade ML workflows to target models in Unity Catalog\nUpgrade models to Unity Catalog\nManage model lifecycle using the Workspace Model Registry (legacy)\nMLflow Model Registry Webhooks on Databricks\nShare models across workspaces\nWorkspace Model Registry example\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Manage model lifecycle in Unity Catalog  Manage model lifecycle using the Workspace Model Registry (legacy)  MLflow Model Registry Webhooks on Databricks\nMLflow Model Registry Webhooks on Databricks\n\nOctober 04, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nWebhooks enable you to listen for Model Registry events so your integrations can automatically trigger actions. You can use webhooks to automate and integrate your machine learning pipeline with existing CI/CD tools and workflows. For example, you can trigger CI builds when a new model version is created or notify your team members through Slack each time a model transition to production is requested.\n\nWebhooks are available through the Databricks REST API or the Python client databricks-registry-webhooks on PyPI.\n\nNote\n\nWebhooks are not available when you use Models in Unity Catalog. For an alternative, see Can I use stage transition requests or trigger webhooks on events?. Sending webhooks to private endpoints (endpoints that are not accessible from the public internet) is not supported.\n\nWebhook events\n\nYou can specify a webhook to trigger upon one or more of these events:\n\nMODEL_VERSION_CREATED: A new model version was created for the associated model.\n\nMODEL_VERSION_TRANSITIONED_STAGE: A model version’s stage was changed.\n\nTRANSITION_REQUEST_CREATED: A user requested a model version’s stage be transitioned.\n\nCOMMENT_CREATED: A user wrote a comment on a registered model.\n\nREGISTERED_MODEL_CREATED: A new registered model was created. This event type can only be specified for a registry-wide webhook, which can be created by not specifying a model name in the create request.\n\nMODEL_VERSION_TAG_SET: A user set a tag on the model version.\n\nMODEL_VERSION_TRANSITIONED_TO_STAGING: A model version was transitioned to staging.\n\nMODEL_VERSION_TRANSITIONED_TO_PRODUCTION: A model version was transitioned to production.\n\nMODEL_VERSION_TRANSITIONED_TO_ARCHIVED: A model version was archived.\n\nTRANSITION_REQUEST_TO_STAGING_CREATED: A user requested a model version be transitioned to staging.\n\nTRANSITION_REQUEST_TO_PRODUCTION_CREATED: A user requested a model version be transitioned to production.\n\nTRANSITION_REQUEST_TO_ARCHIVED_CREATED: A user requested a model version be archived.\n\nTypes of webhooks\n\nThere are two types of webhooks based on their trigger targets:\n\nWebhooks with HTTP endpoints (HTTP registry webhooks): Send triggers to an HTTP endpoint.\n\nWebhooks with job triggers (job registry webhooks): Trigger a job in a Databricks workspace. If IP allowlisting is enabled in the job’s workspace, you must allowlist the workspace IPs of the model registry. See IP allowlisting for job registry webhooks for more information.\n\nThere are also two types of webhooks based on their scope, with different access control requirements:\n\nModel-specific webhooks: The webhook applies to a specific registered model. You must have CAN MANAGE permissions on the registered model to create, modify, delete, or test model-specific webhooks.\n\nRegistry-wide webhooks: The webhook is triggered by events on any registered model in the workspace, including the creation of a new registered model. To create a registry-wide webhook, omit the model_name field on creation. You must have workspace admin permissions to create, modify, delete, or test registry-wide webhooks.\n\nWebhook payload\n\nEach event trigger has minimal fields included in the payload for the outgoing request to the webhook endpoint.\n\nSensitive information like artifact path location is excluded. Users and principals with appropriate ACLs can use client or REST APIs to query the Model Registry for this information.\n\nPayloads are not encrypted. See Security for information on how to validate that Databricks is the source of the webhook.\n\nThe text field facilitates Slack integration. To send a Slack message, provide a Slack webhook endpoint as the webhook URL.\n\nJob registry webhook payload\n\nThe payload for a job registry webhook depends on the type of job and is sent to the jobs/run-now endpoint in the target workspace.\n\nSingle-task jobs\n\nSingle-task jobs have one of three payloads based on the task type.\n\nNotebook and Python wheel jobs\n\nNotebook and Python wheel jobs have a JSON payload with a parameter dictionary that contains a field event_message.\n\nCopy\nJSON\n{\n  \"job_id\": 1234567890,\n  \"notebook_params\": {\n    \"event_message\": \"<Webhook Payload>\"\n  }\n}\n\nPython, JAR, and Spark Submit jobs\n\nPython, JAR, and Spark submit jobs have a JSON payload with a parameter list.\n\nCopy\nJSON\n{\n  \"job_id\": 1234567890,\n  \"python_params\": [\"<Webhook Payload>\"]\n}\n\nAll other jobs\n\nAll other types of jobs have a JSON payload with no parameters.\n\nCopy\nJSON\n{\n  \"job_id\": 1234567890\n}\n\nMulti-task jobs\n\nMulti-task jobs have a JSON payload with all parameters populated to account for different task types.\n\nCopy\nJSON\n{\n  \"job_id\": 1234567890,\n  \"notebook_params\": {\n    \"event_message\": \"<Webhook Payload>\"\n  },\n  \"python_named_params\": {\n    \"event_message\": \"<Webhook Payload>\"\n  },\n  \"jar_params\": [\"<Webhook Payload>\"],\n  \"python_params\": [\"<Webhook Payload>\"],\n  \"spark_submit_params\": [\"<Webhook Payload>\"]\n}\n\nExample payloads\nevent: MODEL_VERSION_TRANSITIONED_STAGE\n\nResponse\n\nCopy\nPOST\n/your/endpoint/for/event/model-versions/stage-transition\n--data {\n  \"event\": \"MODEL_VERSION_TRANSITIONED_STAGE\",\n  \"webhook_id\": \"c5596721253c4b429368cf6f4341b88a\",\n  \"event_timestamp\": 1589859029343,\n  \"model_name\": \"Airline_Delay_SparkML\",\n  \"version\": \"8\",\n  \"to_stage\": \"Production\",\n  \"from_stage\": \"None\",\n  \"text\": \"Registered model 'someModel' version 8 transitioned from None to Production.\"\n}\n\nevent: MODEL_VERSION_TAG_SET\n\nResponse\n\nCopy\nPOST\n/your/endpoint/for/event/model-versions/tag-set\n--data {\n  \"event\": \"MODEL_VERSION_TAG_SET\",\n  \"webhook_id\": \"8d7fc634e624474f9bbfde960fdf354c\",\n  \"event_timestamp\": 1589859029343,\n  \"model_name\": \"Airline_Delay_SparkML\",\n  \"version\": \"8\",\n  \"tags\": [{\"key\":\"key1\",\"value\":\"value1\"},{\"key\":\"key2\",\"value\":\"value2\"}],\n  \"text\": \"example@yourdomain.com set version tag(s) 'key1' => 'value1', 'key2' => 'value2' for registered model 'someModel' version 8.\"\n}\n\nevent: COMMENT_CREATED\n\nResponse\n\nCopy\nPOST\n/your/endpoint/for/event/comments/create\n--data {\n  \"event\": \"COMMENT_CREATED\",\n  \"webhook_id\": \"8d7fc634e624474f9bbfde960fdf354c\",\n  \"event_timestamp\": 1589859029343,\n  \"model_name\": \"Airline_Delay_SparkML\",\n  \"version\": \"8\",\n  \"comment\": \"Raw text content of the comment\",\n  \"text\": \"A user commented on registered model 'someModel' version 8.\"\n}\n\nSecurity\n\nFor security, Databricks includes the X-Databricks-Signature in the header computed from the payload and the shared secret key associated with the webhook using the HMAC with SHA-256 algorithm.\n\nIn addition, you can include a standard Authorization header in the outgoing request by specifying one in the HttpUrlSpec of the webhook.\n\nClient verification\n\nIf a shared secret is set, the payload recipient should verify the source of the HTTP request by using the shared secret to HMAC-encode the payload, and then comparing the encoded value with the X-Databricks-Signature from the header. This is particularly important if SSL certificate validation is disabled (that is, if the enable_ssl_verification field is set to false).\n\nNote\n\nenable_ssl_verification is true by default. For self-signed certificates, this field must be false, and the destination server must disable certificate validation.\n\nFor security purposes, Databricks recommends that you perform secret validation with the HMAC-encoded portion of the payload. If you disable host name validation, you increase the risk that a request could be maliciously routed to an unintended host.\n\nCopy\nPython\nimport hmac\nimport hashlib\nimport json\n\nsecret = shared_secret.encode('utf-8')\nsignature_key = 'X-Databricks-Signature'\n\ndef validate_signature(request):\n  if not request.headers.has_key(signature_key):\n    raise Exception('No X-Signature. Webhook not be trusted.')\n\n  x_sig = request.headers.get(signature_key)\n  body = request.body.encode('utf-8')\n  h = hmac.new(secret, body, hashlib.sha256)\n  computed_sig = h.hexdigest()\n\n  if not hmac.compare_digest(computed_sig, x_sig.encode()):\n    raise Exception('X-Signature mismatch. Webhook not be trusted.')\n\nAuthorization header for HTTP registry webhooks\n\nIf an Authorization header is set, clients should verify the source of the HTTP request by verifying the bearer token or authorization credentials in the Authorization header.\n\nIP allowlisting for job registry webhooks\n\nTo use a webhook that triggers job runs in a different workspace that has IP allowlisting enabled, you must allowlist the region NAT IP where the webhook is located to accept incoming requests.\n\nIf the webhook and the job are in the same workspace, you do not need to add any IPs to your allowlist.\n\nContact your accounts team to identify the IPs you need to allowlist.\n\nAudit logging\n\nIf audit logging is enabled for your workspace, the following events are included in the audit logs:\n\nCreate webhook\n\nUpdate webhook\n\nList webhook\n\nDelete webhook\n\nTest webhook\n\nWebhook trigger\n\nWebhook trigger audit logging\n\nFor webhooks with HTTP endpoints, the HTTP request sent to the URL specified for the webhook along with the URL and enable_ssl_verification values are logged.\n\nFor webhooks with job triggers, the job_id and workspace_url values are logged.\n\nExamples\n\nThis section includes:\n\nHTTP registry webhook workflow example.\n\njob registry webhook workflow example.\n\nlist webhooks example.\n\ntwo example notebooks: one illustrating the REST API, and one illustrating the Python client.\n\nHTTP registry webhook example workflow\n1. Create a webhook\n\nWhen an HTTPS endpoint is ready to receive the webhook event request, you can create a webhook using the webhooks Databricks REST API. For example, the webhook’s URL can point to Slack to post messages to a channel.\n\nCopy\nBash\n$ curl -X POST -H \"Authorization: Bearer <access-token>\" -d \\\n'{\"model_name\": \"<model-name>\",\n  \"events\": [\"MODEL_VERSION_CREATED\"],\n  \"description\": \"Slack notifications\",\n  \"status\": \"TEST_MODE\",\n  \"http_url_spec\": {\n    \"url\": \"https://hooks.slack.com/services/...\",\n    \"secret\": \"anyRandomString\"\n    \"authorization\": \"Bearer AbcdEfg1294\"}}' https://<databricks-instance>/api/2.0/mlflow/registry-webhooks/create\n\nCopy\nPython\nfrom databricks_registry_webhooks import RegistryWebhooksClient, HttpUrlSpec\n\nhttp_url_spec = HttpUrlSpec(\n  url=\"https://hooks.slack.com/services/...\",\n  secret=\"secret_string\",\n  authorization=\"Bearer AbcdEfg1294\"\n)\nhttp_webhook = RegistryWebhooksClient().create_webhook(\n  model_name=\"<model-name>\",\n  events=[\"MODEL_VERSION_CREATED\"],\n  http_url_spec=http_url_spec,\n  description=\"Slack notifications\",\n  status=\"TEST_MODE\"\n)\n\n\nResponse\n\nCopy\n{\"webhook\": {\n   \"id\":\"1234567890\",\n   \"creation_timestamp\":1571440826026,\n   \"last_updated_timestamp\":1582768296651,\n   \"status\":\"TEST_MODE\",\n   \"events\":[\"MODEL_VERSION_CREATED\"],\n   \"http_url_spec\": {\n     \"url\": \"https://hooks.slack.com/services/...\",\n     \"enable_ssl_verification\": True\n}}}\n\n\nYou can also create an HTTP registry webhook with the Databricks Terraform provider and databricks_mlflow_webhook.\n\n2. Test the webhook\n\nThe previous webhook was created in TEST_MODE, so a mock event can be triggered to send a request to the specified URL. However, the webhook does not trigger on a real event. The test endpoint returns the received status code and body from the specified URL.\n\nCopy\nBash\n$ curl -X POST -H \"Authorization: Bearer <access-token>\" -d \\\n'{\"id\": \"1234567890\"}' \\\nhttps://<databricks-instance>/api/2.0/mlflow/registry-webhooks/test\n\nCopy\nPython\nfrom databricks_registry_webhooks import RegistryWebhooksClient\n\nhttp_webhook = RegistryWebhooksClient().test_webhook(\n  id=\"1234567890\"\n)\n\n\nResponse\n\nCopy\n{\n \"status\":200,\n \"body\":\"OK\"\n}\n\n3. Update the webhook to active status\n\nTo enable the webhook for real events, set its status to ACTIVE through an update call, which can also be used to change any of its other properties.\n\nCopy\nBash\n$ curl -X PATCH -H \"Authorization: Bearer <access-token>\" -d \\\n'{\"id\": \"1234567890\", \"status\": \"ACTIVE\"}' \\\nhttps://<databricks-instance>/api/2.0/mlflow/registry-webhooks/update\n\nCopy\nPython\nfrom databricks_registry_webhooks import RegistryWebhooksClient\n\nhttp_webhook = RegistryWebhooksClient().update_webhook(\n  id=\"1234567890\",\n  status=\"ACTIVE\"\n)\n\n\nResponse\n\nCopy\n{\"webhook\": {\n   \"id\":\"1234567890\",\n   \"creation_timestamp\":1571440826026,\n   \"last_updated_timestamp\":1582768296651,\n   \"status\": \"ACTIVE\",\n   \"events\":[\"MODEL_VERSION_CREATED\"],\n   \"http_url_spec\": {\n     \"url\": \"https://hooks.slack.com/services/...\",\n     \"enable_ssl_verification\": True\n}}}\n\n4. Delete the webhook\n\nTo disable the webhook, set its status to DISABLED (using a similar update command as above), or delete it.\n\nCopy\nBash\n$ curl -X DELETE -H \"Authorization: Bearer <access-token>\" -d \\\n'{\"id\": \"1234567890\"}' \\\nhttps://<databricks-instance>/api/2.0/mlflow/registry-webhooks/delete\n\nCopy\nPython\nfrom databricks_registry_webhooks import RegistryWebhooksClient\n\nhttp_webhook = RegistryWebhooksClient().delete_webhook(\n  id=\"1234567890\"\n)\n\n\nResponse\n\nCopy\n{}\n\nJob registry webhook example workflow\n\nThe workflow for managing job registry webhooks is similar to HTTP registry webhooks, with the only difference being the job_spec field that replaces the http_url_spec field.\n\nWith webhooks, you can trigger jobs in the same workspace or in a different workspace. The workspace is specified using the optional parameter workspace_url. If no workspace_url is present, the default behavior is to trigger a job in the same workspace as the webhook.\n\nRequirements\n\nAn existing job.\n\nA personal access token. Note that access tokens can only be read by the MLflow service and cannot be returned by Databricks users in the Model Registry API.\n\nNote\n\nAs a security best practice when you authenticate with automated tools, systems, scripts, and apps, Databricks recommends that you use OAuth tokens.\n\nIf you use personal access token authentication, Databricks recommends using personal access tokens belonging to service principals instead of workspace users. To create tokens for service principals, see Manage tokens for a service principal.\n\nCreate a job registry webhook\nCopy\nBash\n$ curl -X POST -H \"Authorization: Bearer <access-token>\" -d \\ '{\"model_name\": \"<model-name>\",\n  \"events\": [\"TRANSITION_REQUEST_CREATED\"],\n  \"description\": \"Job webhook trigger\",\n  \"status\": \"TEST_MODE\",\n  \"job_spec\": {\n    \"job_id\": \"1\",\n    \"workspace_url\": \"https://my-databricks-workspace.com\",\n    \"access_token\": \"dapi12345...\"}}'\nhttps://<databricks-instance>/api/2.0/mlflow/registry-webhooks/create\n\nCopy\nPython\nfrom databricks_registry_webhooks import RegistryWebhooksClient, JobSpec\n\njob_spec = JobSpec(\n  job_id=\"1\",\n  workspace_url=\"https://my-databricks-workspace.com\",\n  access_token=\"dapi12345...\"\n)\njob_webhook = RegistryWebhooksClient().create_webhook(\n  model_name=\"<model-name>\",\n  events=[\"TRANSITION_REQUEST_CREATED\"],\n  job_spec=job_spec,\n  description=\"Job webhook trigger\",\n  status=\"TEST_MODE\"\n)\n\n\nResponse\n\nCopy\n{\"webhook\": {\n   \"id\":\"1234567891\",\n   \"creation_timestamp\":1591440826026,\n   \"last_updated_timestamp\":1591440826026,\n   \"status\":\"TEST_MODE\",\n   \"events\":[\"TRANSITION_REQUEST_CREATED\"],\n   \"job_spec\": {\n     \"job_id\": \"1\",\n     \"workspace_url\": \"https://my-databricks-workspace.com\"\n}}}\n\n\nYou can also create a job registry webhook with the Databricks Terraform provider and databricks_mlflow_webhook.\n\nList registry webhooks example\nCopy\nBash\n$ curl -X GET -H \"Authorization: Bearer <access-token>\" -d \\ '{\"model_name\": \"<model-name>\"}'\nhttps://<databricks-instance>/api/2.0/mlflow/registry-webhooks/list\n\nCopy\nPython\nfrom databricks_registry_webhooks import RegistryWebhooksClient\n\nwebhooks_list = RegistryWebhooksClient().list_webhooks(model_name=\"<model-name>\")\n\n\nResponse\n\nCopy\n{\"webhooks\": [{\n   \"id\":\"1234567890\",\n   \"creation_timestamp\":1571440826026,\n   \"last_updated_timestamp\":1582768296651,\n   \"status\": \"ACTIVE\",\n   \"events\":[\"MODEL_VERSION_CREATED\"],\n   \"http_url_spec\": {\n     \"url\": \"https://hooks.slack.com/services/...\",\n     \"enable_ssl_verification\": True\n}},\n{\n   \"id\":\"1234567891\",\n   \"creation_timestamp\":1591440826026,\n   \"last_updated_timestamp\":1591440826026,\n   \"status\":\"TEST_MODE\",\n   \"events\":[\"TRANSITION_REQUEST_CREATED\"],\n   \"job_spec\": {\n     \"job_id\": \"1\",\n     \"workspace_url\": \"https://my-databricks-workspace.com\"\n}}]}\n\nNotebooks\nMLflow Model Registry webhooks REST API example notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nMLflow Model Registry webhooks Python client example notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWebhook events\nTypes of webhooks\nWebhook payload\nSecurity\nAudit logging\nExamples\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Log model dependencies | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/log-model-dependencies.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nLog model dependencies\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Log, load, register, and deploy MLflow models  Log model dependencies\nLog model dependencies\n\nOctober 30, 2024\n\nIn this article, you learn how to log a model and its dependencies as model artifacts, so they are available in your environment for production tasks like model serving.\n\nLog Python package model dependencies\n\nMLflow has native support for some Python ML libraries, where MLflow can reliably log dependencies for models that use these libraries. See built-in model flavors.\n\nFor example, MLflow supports scikit-learn in the mlflow.sklearn module, and the command mlflow.sklearn.log_model logs the sklearn version. The same applies for autologging with those ML libraries. See the MLflow github repository for additional examples.\n\nNote\n\nTo enable trace logging for generative AI workloads, MLflow supports OpenAI autologging.\n\nFor ML libraries that can be installed with pip install PACKAGE_NAME==VERSION, but do not have built-in MLflow model flavors, you can log those packages using the mlflow.pyfunc.log_model method. Be sure to log the requirements with the exact library version, for example, f\"nltk=={nltk.__version__}\" instead of just nltk.\n\nmlflow.pyfunc.log_model supports logging for:\n\nPublic and custom libraries packaged as Python egg or Python wheel files.\n\nPublic packages on PyPI and privately hosted packages on your own PyPI server.\n\nWith mlflow.pyfunc.log_model, MLflow tries to infer the dependencies automatically. MLflow infers the dependencies using mlflow.models.infer_pip_requirements, and logs them to a requirements.txt file as a model artifact.\n\nIn older versions, MLflow sometimes doesn’t identify all Python requirements automatically, especially if the library isn’t a built-in model flavor. In these cases, you can specify additional dependencies with the extra_pip_requirements parameter in the log_model command. See an example of using the extra_pip_requirements parameter.\n\nImportant\n\nYou can also overwrite the entire set of requirements with the conda_env and pip_requirements parameters, but doing so is generally discouraged because this overrides the dependencies which MLflow picks up automatically. See an example of how to use the `pip_requirements` parameter to overwrite requirements.\n\nCustomized model logging\n\nFor scenarios where more customized model logging is necessary, you can either:\n\nWrite a custom Python model. Doing so allows you to subclass mlflow.pyfunc.PythonModel to customize initialization and prediction. This approach works well for customization of Python-only models.\n\nFor a simple example, see the add N model example.\n\nFor a more complex example, see the custom XGBoost model example.\n\nWrite a custom flavor. In this scenario, you can customize logging more than the generic pyfunc flavor, but doing so requires more work to implement.\n\nCustom Python code\n\nYou may have Python code dependencies that can’t be installed using the %pip install command, such as one or more .py files.\n\nWhen logging a model, you can tell MLflow that the model can find those dependencies at a specified path by using the code_path parameter in mlflow.pyfunc.log_model. MLflow stores any files or directories passed using code_path as artifacts along with the model in a code directory. When loading the model, MLflow adds these files or directories to the Python path. This route also works with custom Python wheel files, which can be included in the model using code_path, just like .py files.\n\nCopy\nPython\nmlflow.pyfunc.log_model( artifact_path=artifact_path,\n                         code_path=[filename.py],\n                         data_path=data_path,\n                         conda_env=conda_env,\n                       )\n\nLog non-Python package model dependencies\n\nMLflow does not automatically pick up non-Python dependencies, such as Java packages, R packages, and native packages (such as Linux packages). For these packages, you need to log additional data.\n\nDependency list: Databricks recommends logging an artifact with the model specifying these non-Python dependencies. This could be a simple .txt or .json file. mlflow.pyfunc.log_model allows you to specify this additional artifact using the artifacts argument.\n\nCustom packages: Just as for custom Python dependencies above, you need to ensure that the packages are available in your deployment environment. For packages in a central location such as Maven Central or your own repository, make sure that the location is available at scoring or serving time. For private packages not hosted elsewhere, you can log packages along with the model as artifacts.\n\nDeploy models with dependencies\n\nWhen deploying a model from the MLflow Tracking Server or Model Registry, you need to ensure that the deployment environment has the right dependencies installed. The simplest path may depend on your deployment mode: batch/streaming or online serving, and on the types of dependencies.\n\nFor all deployment modes, Databricks recommends running inference on the same runtime version that you used during training, since the Databricks Runtime in which you created your model has various libraries already installed. MLflow in Databricks automatically saves that runtime version in the MLmodel metadata file in a databricks_runtime field, such as databricks_runtime: 10.2.x-cpu-ml-scala2.12.\n\nOnline serving: Mosaic AI Model Serving\n\nDatabricks offers Model Serving, where your MLflow machine learning models are exposed as scalable REST API endpoints.\n\nFor Python dependencies in the requirements.txt file, Databricks and MLflow handle everything for public PyPI dependencies. Similarly, if you specified .py files or Python wheel files when logging the model by using the code_path argument, MLflow loads those dependencies for you automatically.\n\nFor these model serving scenarios, see the following:\n\nUse custom Python libraries with Model Serving\n\nPackage custom artifacts and files for Model Serving\n\nFor Python dependencies in the requirements.txt file, Databricks and MLflow handle everything for public PyPI dependencies. Similarly, if you specified .py files or Python wheel files when logging the model by using the code_path argument, MLflow loads those dependencies for you automatically.\n\nOnline serving: third-party systems or Docker containers\n\nIf your scenario requires serving to third-party serving solutions or your own Docker-based solution, you can export your model as a Docker container.\n\nDatabricks recommends the following for third-party serving that automatically handles Python dependencies. However, for non-Python dependencies, the container needs to be modified to include them.\n\nMLflow’s Docker integration for Docker-based serving solution: MLflow models build-docker\n\nMLflow’s SageMaker integration: mlflow.sagemaker API\n\nBatch and streaming jobs\n\nBatch and streaming scoring should be run as Databricks Jobs. A notebook job often suffices, and the simplest way to prepare code is to use the Databricks Model Registry to generate a scoring notebook.\n\nThe following describes the process and the steps to follow to ensure dependencies are installed and applied accordingly:\n\nStart your scoring cluster with the same Databricks Runtime version used during training. Read the databricks_runtime field from the MLmodel metadata file, and start a cluster with that runtime version.\n\nThis can be done manually in the cluster configuration or automated with custom logic. For automation, the runtime version format that you read from the metadata file in the Jobs API and Clusters API.\n\nNext, install any non-Python dependencies. To ensure your non-Python dependencies are accessible to your deployment environment, you can either:\n\nManually install the non-Python dependencies of your model on the Databricks cluster as part of the cluster configuration before running inference.\n\nAlternatively, you can write custom logic in your scoring job deployment to automate the installation of the dependencies onto your cluster. Assuming you saved your non-Python dependencies as artifacts as described in Log non-Python package model dependencies, this automation can install libraries using the Libraries API. Or, you can write specific code to generate a cluster-scoped initialization script to install the dependencies.\n\nYour scoring job installs the Python dependencies in the job execution environment. In Databricks, the Model Registry allows you to generate a notebook for inference which does this for you.\n\nWhen you use the Databricks Model Registry to generate a scoring notebook, the notebook contains code to install the Python dependencies in the model’s requirements.txt file. For your notebook job for batch or streaming scoring, this code initializes your notebook environment, so that the model dependencies are installed and ready for your model.\n\nMLflow handles any custom Python code included in the code_path parameter in log_model. This code is added to the Python path when the model’s predict() method is called. You can also do this manually by either:\n\nCalling mlflow.pyfunc.spark_udf with the env_manager=['virtualenv'/'conda'] argument.\n\nExtracting the requirements using mlflow.pyfunc.get_model_dependencies and installing them using %pip install.\n\nNote\n\nIf you specified .py files or Python wheel files when logging the model using the code_path argument, MLflow loads those dependencies for you automatically.\n\nWas this article helpful?\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nLog Python package model dependencies\nLog non-Python package model dependencies\nDeploy models with dependencies\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Upgrade ML workflows to target models in Unity Catalog | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/upgrade-workflows.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nUpgrade ML workflows to target models in Unity Catalog\nUpgrade models to Unity Catalog\nManage model lifecycle using the Workspace Model Registry (legacy)\nWorkspace Model Registry example\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Manage model lifecycle in Unity Catalog  Upgrade ML workflows to target models in Unity Catalog\nUpgrade ML workflows to target models in Unity Catalog\n\nOctober 30, 2024\n\nThis article explains how to migrate and upgrade existing Databricks workflows to use models in Unity Catalog.\n\nRequirements\nRequired privileges\n\nTo execute a model training, deployment, or inference workflow in Unity Catalog, the principal running the workflow must have USE CATALOG and USE SCHEMA privileges on the catalog and schema that hold the model.\n\nThe following privileges are also required:\n\nTo create a model, the principal must have the CREATE MODEL privilege.\n\nTo load or deploy a model, the principal must have the EXECUTE privilege on the registered model.\n\nOnly the owner of a registered model can do the following:\n\nCreate a new model version.\n\nSet an alias on a registered model.\n\nCompute requirements\n\nThe compute resource specified for the workflow must have access to Unity Catalog. See Access modes.\n\nCreate parallel training, deployment, and inference workflows\n\nTo upgrade model training and inference workflows to Unity Catalog, Databricks recommends an incremental approach in which you create a parallel training, deployment, and inference pipeline that leverage models in Unity Catalog. When you’re comfortable with the results using Unity Catalog, you can switch downstream consumers to read the batch inference output, or increase the traffic routed to models in Unity Catalog in serving endpoints.\n\nModel training workflow\n\nClone your model training workflow. Confirm that the principal running the workflow and the compute specified for the workflow meet the Requirements.\n\nNext, modify the model training code in the cloned workflow. You might need to clone the notebook run by the workflow, or create and target a new git branch in the cloned workflow. Follow these steps to install the necessary version of MLflow and configure the client to target Unity Catalog in your training code. Then, update the model training code to register models to Unity Catalog. See Train and register Unity Catalog-compatible models.\n\nModel deployment workflow\n\nClone your model deployment workflow. Confirm that the principal running the workflow and the compute specified for the workflow meet the Requirements.\n\nIf you have model validation logic in your deployment workflow, update it to load model versions from UC. Use aliases to manage production model rollouts.\n\nModel inference workflow\nBatch inference workflow\n\nClone the batch inference workflow. Confirm that the principal running the workflow and the compute specified for the workflow meet the Requirements.\n\nModel serving workflow\n\nIf you are using Mosaic AI Model Serving, you don’t need to clone your existing endpoint. Instead, use the traffic split feature to start routing a small fraction of traffic to models in Unity Catalog. As you review the results using Unity Catalog, increase the amount of traffic until all of the traffic is rerouted.\n\nPromote a model across environments\n\nPromoting a model across environments works differently with models in Unity Catalog. For details, see Promote a model across environments.\n\nUse job webhooks for manual approval for model deployment\n\nDatabricks recommends that you automate model deployment if possible, using appropriate checks and tests during the model deployment process. However, if you do need to perform manual approvals to deploy production models, you can use job notifications to call out to external CI/CD systems to request manual approval for deploying a model, after your model training job completes successfully. After manual approval is provided, your CI/CD system can then deploy the model version to serve traffic, for example by setting the “Champion” alias on it.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nCreate parallel training, deployment, and inference workflows\nModel training workflow\nModel deployment workflow\nModel inference workflow\nPromote a model across environments\nUse job webhooks for manual approval for model deployment\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "LLMOps workflows on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/mlops/llmops.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nHow does Databricks support CI/CD for machine learning?\nModel deployment patterns\nMLOps Stacks: model development process as code\nLLMOps\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  MLOps workflows on Databricks  LLMOps workflows on Databricks\nLLMOps workflows on Databricks\n\nNovember 15, 2024\n\nThis article complements MLOps workflows on Databricks by adding information specific to LLMOps workflows. For more details, see The Big Book of MLOps.\n\nHow does the MLOps workflow change for LLMs?\n\nLLMs are a class of natural language processing (NLP) models that have significantly surpassed their predecessors in size and performance across a variety of tasks, such as open-ended question answering, summarization, and execution of instructions.\n\nDevelopment and evaluation of LLMs differs in some important ways from traditional ML models. This section briefly summarizes some of the key properties of LLMs and the implications for MLOps.\n\nKey properties of LLMs\n\n\t\n\nImplications for MLOps\n\n\n\n\nLLMs are available in many forms.\n\nGeneral proprietary and OSS models that are accessed using paid APIs.\n\nOff-the-shelf open source models that vary from general to specific applications.\n\nCustom models that have been fine-tuned for specific applications.\n\nCustom pre-trained applications.\n\n\t\n\nDevelopment process: Projects often develop incrementally, starting from existing, third-party or open source models and ending with custom fine-tuned models.\n\n\n\n\nMany LLMs take general natural language queries and instructions as input. Those queries can contain carefully engineered prompts to elicit the desired responses.\n\n\t\n\nDevelopment process: Designing text templates for querying LLMs is often an important part of developing new LLM pipelines.\n\nPackaging ML artifacts: Many LLM pipelines use existing LLMs or LLM serving endpoints. The ML logic developed for those pipelines might focus on prompt templates, agents, or chains instead of the model itself. The ML artifacts packaged and promoted to production might be these pipelines, rather than models.\n\n\n\n\nMany LLMs can be given prompts with examples, context, or other information to help answer the query.\n\n\t\n\nServing infrastructure: When augmenting LLM queries with context, you might use additional tools such as vector databases to search for relevant context.\n\n\n\n\nThird-party APIs provide proprietary and open-source models.\n\n\t\n\nAPI governance: Using centralized API governance provides the ability to easily switch between API providers.\n\n\n\n\nLLMs are very large deep learning models, often ranging from gigabytes to hundreds of gigabytes.\n\n\t\n\nServing infrastructure: LLMs might require GPUs for real-time model serving, and fast storage for models that need to be loaded dynamically.\n\nCost/performance tradeoffs: Because larger models require more computation and are more expensive to serve, techniques for reducing model size and computation might be required.\n\n\n\n\nLLMs are hard to evaluate using traditional ML metrics since there is often no single “right” answer.\n\n\t\n\nHuman feedback: Human feedback is essential for evaluating and testing LLMs. You should incorporate user feedback directly into the MLOps process, including for testing, monitoring, and future fine-tuning.\n\nCommonalities between MLOps and LLMOps\n\nMany aspects of MLOps processes do not change for LLMs. For example, the following guidelines also apply to LLMs:\n\nUse separate environments for development, staging, and production.\n\nUse Git for version control.\n\nManage model development with MLflow, and use Models in Unity Catalog to manage the model lifecycle.\n\nStore data in a lakehouse architecture using Delta tables.\n\nYour existing CI/CD infrastructure should not require any changes.\n\nThe modular structure of MLOps remains the same, with pipelines for featurization, model training, model inference, and so on.\n\nReference architecture diagrams\n\nThis section uses two LLM-based applications to illustrate some of the adjustments to the reference architecture of traditional MLOps. The diagrams show the production architecture for 1) a retrieval-augmented generation (RAG) application using a third-party API, and 2) a RAG application using a self-hosted fine-tuned model. Both diagrams show an optional vector database — this item can be replaced by directly querying the LLM through the Model Serving endpoint.\n\nRAG with a third-party LLM API\n\nThe diagram shows a production architecture for a RAG application that connects to a third-party LLM API using Databricks External Models.\n\nRAG with a fine-tuned open source model\n\nThe diagram shows a production architecture for a RAG application that fine-tunes an open source model.\n\nLLMOps changes to MLOps production architecture\n\nThis section highlights the major changes to the MLOps reference architecture for LLMOps applications.\n\nModel hub\n\nLLM applications often use existing, pretrained models selected from an internal or external model hub. The model can be used as-is or fine-tuned.\n\nDatabricks includes a selection of high-quality, pre-trained foundation models in Unity Catalog and in Databricks Marketplace. You can use these pre-trained models to access state-of-the-art AI capabilities, saving you the time and expense of building your own custom models. For details, see Pre-trained models in Unity Catalog and Marketplace.\n\nVector database\n\nSome LLM applications use vector databases for fast similarity searches, for example to provide context or domain knowledge in LLM queries. Databricks provides an integrated vector search functionality that lets you use any Delta table in Unity Catalog as a vector database. The vector search index automatically syncs with the Delta table. For details, see Vector Search.\n\nYou can create a model artifact that encapsulates the logic to retrieve information from a vector database and provides the returned data as context to the LLM. You can then log the model using the MLflow LangChain or PyFunc model flavor.\n\nFine-tune LLM\n\nBecause LLM models are expensive and time-consuming to create from scratch, LLM applications often fine-tune an existing model to improve its performance in a particular scenario. In the reference architecture, fine-tuning and model deployment are represented as distinct Databricks Jobs. Validating a fine-tuned model before deploying is often a manual process.\n\nDatabricks provides Foundation Model Fine-tuning, which lets you use your own data to customize an existing LLM to optimize its performance for your specific application. For details, see Foundation Model Fine-tuning.\n\nModel serving\n\nIn the RAG using a third-party API scenario, an important architectural change is that the LLM pipeline makes external API calls, from the Model Serving endpoint to internal or third-party LLM APIs. This adds complexity, potential latency, and additional credential management.\n\nDatabricks provides Mosaic AI Model Serving, which provides a unified interface to deploy, govern, and query AI models. For details, see Mosaic AI Model Serving.\n\nHuman feedback in monitoring and evaluation\n\nHuman feedback loops are essential in most LLM applications. Human feedback should be managed like other data, ideally incorporated into monitoring based on near real-time streaming.\n\nThe Mosaic AI Agent Framework review app helps you gather feedback from human reviewers. For details, see Get feedback about the quality of an agentic application.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nHow does the MLOps workflow change for LLMs?\nCommonalities between MLOps and LLMOps\nReference architecture diagrams\nLLMOps changes to MLOps production architecture\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Model deployment patterns | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/mlops/deployment-patterns.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nHow does Databricks support CI/CD for machine learning?\nModel deployment patterns\nMLOps Stacks: model development process as code\nLLMOps\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  MLOps workflows on Databricks  Model deployment patterns\nModel deployment patterns\n\nOctober 01, 2024\n\nThis article describes two common patterns for moving ML artifacts through staging and into production. The asynchronous nature of changes to models and code means that there are multiple possible patterns that an ML development process might follow.\n\nModels are created by code, but the resulting model artifacts and the code that created them can operate asynchronously. That is, new model versions and code changes might not happen at the same time. For example, consider the following scenarios:\n\nTo detect fraudulent transactions, you develop an ML pipeline that retrains a model weekly. The code may not change very often, but the model might be retrained every week to incorporate new data.\n\nYou might create a large, deep neural network to classify documents. In this case, training the model is computationally expensive and time-consuming, and retraining the model is likely to happen infrequently. However, the code that deploys, serves, and monitors this model can be updated without retraining the model.\n\nThe two patterns differ in whether the model artifact or the training code that produces the model artifact is promoted towards production.\n\nDeploy code (recommended)\n\nIn most situations, Databricks recommends the “deploy code” approach. This approach is incorporated into the recommended MLOps workflow.\n\nIn this pattern, the code to train models is developed in the development environment. The same code moves to staging and then production. The model is trained in each environment: initially in the development environment as part of model development, in staging (on a limited subset of data) as part of integration tests, and in the production environment (on the full production data) to produce the final model.\n\nAdvantages:\n\nIn organizations where access to production data is restricted, this pattern allows the model to be trained on production data in the production environment.\n\nAutomated model retraining is safer, since the training code is reviewed, tested, and approved for production.\n\nSupporting code follows the same pattern as model training code. Both go through integration tests in staging.\n\nDisadvantages:\n\nThe learning curve for data scientists to hand off code to collaborators can be steep. Predefined project templates and workflows are helpful.\n\nAlso in this pattern, data scientists must be able to review training results from the production environment, as they have the knowledge to identify and fix ML-specific issues.\n\nIf your situation requires that the model be trained in staging over the full production dataset, you can use a hybrid approach by deploying code to staging, training the model, and then deploying the model to production. This approach saves training costs in production but adds an extra operation cost in staging.\n\nDeploy models\n\nIn this pattern, the model artifact is generated by training code in the development environment. The artifact is then tested in the staging environment before being deployed into production.\n\nConsider this option when one or more of the following apply:\n\nModel training is very expensive or hard to reproduce.\n\nAll work is done in a single Databricks workspace.\n\nYou are not working with external repos or a CI/CD process.\n\nAdvantages:\n\nA simpler handoff for data scientists\n\nIn cases where model training is expensive, only requires training the model once.\n\nDisadvantages:\n\nIf production data is not accessible from the development environment (which may be true for security reasons), this architecture may not be viable.\n\nAutomated model retraining is tricky in this pattern. You could automate retraining in the development environment, but the team responsible for deploying the model in production might not accept the resulting model as production-ready.\n\nSupporting code, such as pipelines used for feature engineering, inference, and monitoring, needs to be deployed to production separately.\n\nTypically an environment (development, staging, or production) corresponds to a catalog in Unity Catalog. For details on how to implement this pattern, see the upgrade guide.\n\nThe diagram below contrasts the code lifecycle for the above deployment patterns across the different execution environments.\n\nThe environment shown in the diagram is the final environment in which a step is run. For example, in the deploy models pattern, final unit and integration testing is performed in the development environment. In the deploy code pattern, unit tests and integration tests are run in the development environments, and final unit and integration testing is performed in the staging environment.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nDeploy code (recommended)\nDeploy models\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "How does Databricks support CI/CD for machine learning? | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/mlops/ci-cd-for-ml.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nHow does Databricks support CI/CD for machine learning?\nModel deployment patterns\nMLOps Stacks: model development process as code\nLLMOps\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  MLOps workflows on Databricks  How does Databricks support CI/CD for machine learning?\nHow does Databricks support CI/CD for machine learning?\n\nNovember 15, 2024\n\nCI/CD (continuous integration and continuous delivery) refers to an automated process for developing, deploying, monitoring, and maintaining your applications. By automating the building, testing, and deployment of code, development teams can deliver releases more frequently and reliably than manual processes still prevalent across many data engineering and data science teams. CI/CD for machine learning brings together techniques of MLOps, DataOps, ModelOps, and DevOps.\n\nThis article describes how Databricks supports CI/CD for machine learning solutions. In machine learning applications, CI/CD is important not only for code assets, but is also applied to data pipelines, including both input data and the results generated by the model.\n\nMachine learning elements that need CI/CD\n\nOne of the challenges of ML development is that different teams own different parts of the process. Teams may rely on different tools and have different release schedules. Databricks provides a single, unified data and ML platform with integrated tools to improve teams’ efficiency and ensure consistency and repeatability of data and ML pipelines.\n\nIn general for machine learning tasks, the following should be tracked in an automated CI/CD workflow:\n\nTraining data, including data quality, schema changes, and distribution changes.\n\nInput data pipelines.\n\nCode for training, validating, and serving the model.\n\nModel predictions and performance.\n\nIntegrate Databricks into your CI/CD processes\n\nMLOps, DataOps, ModelOps, and DevOps refer to the integration of development processes with “operations” - making the processes and infrastructure predictable and reliable. This set of articles describes how to integrate operations (“ops”) principles into your ML workflows on the Databricks platform.\n\nDatabricks incorporates all of the components required for the ML lifecycle including tools to build “configuration as code” to ensure reproducibility and “infrastructure as code” to automate the provisioning of cloud services. It also includes logging and alerting services to help you detect and troubleshoot problems when they occur.\n\nDataOps: Reliable and secure data\n\nGood ML models depend on reliable data pipelines and infrastructure. With the Databricks Data Intelligence Platform, the entire data pipeline from ingesting data to the outputs from the served model is on a single platform and uses the same toolset, which facilitates productivity, reproducibility, sharing, and troubleshooting.\n\nDataOps tasks and tools in Databricks\n\nThe table lists common DataOps tasks and tools in Databricks:\n\nDataOps task\n\n\t\n\nTool in Databricks\n\n\n\n\nIngest and transform data\n\n\t\n\nAutoloader and Apache Spark\n\n\n\n\nTrack changes to data including versioning and lineage\n\n\t\n\nDelta tables\n\n\n\n\nBuild, manage, and monitor data processing pipelines\n\n\t\n\nDelta Live Tables\n\n\n\n\nEnsure data security and governance\n\n\t\n\nUnity Catalog\n\n\n\n\nExploratory data analysis and dashboards\n\n\t\n\nDatabricks SQL, Dashboards, and Databricks notebooks\n\n\n\n\nGeneral coding\n\n\t\n\nDatabricks SQL and Databricks notebooks\n\n\n\n\nSchedule data pipelines\n\n\t\n\nDatabricks Jobs\n\n\n\n\nAutomate general workflows\n\n\t\n\nDatabricks Jobs\n\n\n\n\nCreate, store, manage, and discover features for model training\n\n\t\n\nDatabricks Feature Store\n\n\n\n\nData monitoring\n\n\t\n\nLakehouse Monitoring\n\nModelOps: Model development and lifecycle\n\nDeveloping a model requires a series of experiments and a way to track and compare the conditions and results of those experiments. The Databricks Data Intelligence Platform includes MLflow for model development tracking and the MLflow Model Registry to manage the model lifecycle including staging, serving, and storing model artifacts.\n\nAfter a model is released to production, many things can change that might affect its performance. In addition to monitoring the model’s prediction performance, you should also monitor input data for changes in quality or statistical characteristics that might require retraining the model.\n\nModelOps tasks and tools in Databricks\n\nThe table lists common ModelOps tasks and tools provided by Databricks:\n\nModelOps task\n\n\t\n\nTool in Databricks\n\n\n\n\nTrack model development\n\n\t\n\nMLflow model tracking\n\n\n\n\nManage model lifecycle\n\n\t\n\nModels in Unity Catalog\n\n\n\n\nModel code version control and sharing\n\n\t\n\nDatabricks Git folders\n\n\n\n\nNo-code model development\n\n\t\n\nAutoML\n\n\n\n\nModel monitoring\n\n\t\n\nLakehouse Monitoring\n\nDevOps: Production and automation\n\nThe Databricks platform supports ML models in production with the following:\n\nEnd-to-end data and model lineage: From models in production back to the raw data source, on the same platform.\n\nProduction-level Model Serving: Automatically scales up or down based on your business needs.\n\nJobs: Automates jobs and create scheduled machine learning workflows.\n\nGit folders: Code versioning and sharing from the workspace, also helps teams follow software engineering best practices.\n\nDatabricks Terraform provider: Automates deployment infrastructure across clouds for ML inference jobs, serving endpoints, and featurization jobs.\n\nModel serving\n\nFor deploying models to production, MLflow significantly simplifies the process, providing single-click deployment as a batch job for large amounts of data or as a REST endpoint on an autoscaling cluster. The integration of Databricks Feature Store with MLflow also ensures consistency of features for training and serving; also, MLflow models can automatically look up features from the Feature Store, even for low latency online serving.\n\nThe Databricks platform supports many model deployment options:\n\nCode and containers.\n\nBatch serving.\n\nLow-latency online serving.\n\nOn-device or edge serving.\n\nMulti-cloud, for example, training the model on one cloud and deploying it with another.\n\nFor more information, see Mosaic AI Model Serving.\n\nJobs\n\nDatabricks Jobs allow you to automate and schedule any type of workload, from ETL to ML. Databricks also supports integrations with popular third party orchestrators like Airflow.\n\nGit folders\n\nThe Databricks platform includes Git support in the workspace to help teams follow software engineering best practices by performing Git operations through the UI. Administrators and DevOps engineers can use APIs to set up automation with their favorite CI/CD tools. Databricks supports any type of Git deployment including private networks.\n\nFor more information about best practices for code development using Databricks Git folders, see CI/CD workflows with Git integration and Databricks Git folders and Use CI/CD. These techniques, together with the Databricks REST API, let you build automated deployment processes with GitHub Actions, Azure DevOps pipelines, or Jenkins jobs.\n\nUnity Catalog for governance and security\n\nThe Databricks platform includes Unity Catalog, which lets admins set up fine-grained access control, security policies, and governance for all data and AI assets across Databricks.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nMachine learning elements that need CI/CD\nIntegrate Databricks into your CI/CD processes\nDataOps: Reliable and secure data\nModelOps: Model development and lifecycle\nDevOps: Production and automation\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Access the MLflow tracking server from outside Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/access-hosted-tracking-server.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nTrack ML and deep learning training runs\nDatabricks Autologging\nAccess the MLflow tracking server from outside Databricks\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Track model development using MLflow  Access the MLflow tracking server from outside Databricks\nAccess the MLflow tracking server from outside Databricks\n\nSeptember 25, 2024\n\nYou may wish to log to the MLflow tracking server from your own applications or from the MLflow CLI.\n\nThis article describes the required configuration steps. Start by installing MLflow and configuring your credentials (Step 1). You can then either configure an application (Step 2) or configure the MLflow CLI (Step 3).\n\nFor information on how to launch and log to an open-source tracking server, see the MLflow open source documentation.\n\nStep 1: Configure your environment\n\nIf you don’t have a Databricks account, you can try Databricks for free. See Get started with Databricks.\n\nTo configure your environment to access your Databricks hosted MLflow tracking server:\n\nInstall MLflow using pip install mlflow.\n\nConfigure authentication according to your Databricks subscription.\n\nCommunity Edition. Do one of the following:\n\n(Recommended) Use mlflow.login() to be prompted for your credentials.\n\nCopy\nPython\nimport mlflow\n\nmlflow.login()\n\n\nThe following is a response example. If the authentication succeeds, you see the message, “Successfully signed into Databricks!”.\n\nCopy\n2023/10/25 22:59:27 ERROR mlflow.utils.credentials: Failed to sign in Databricks: default auth: cannot configure default credentials\nDatabricks Host (should begin with https://): https://community.cloud.databricks.com/\nUsername: weirdmouse@gmail.com\nPassword: ··········\n2023/10/25 22:59:38 INFO mlflow.utils.credentials: Successfully signed in Databricks!\n\n\nSpecify credentials using environment variables:\n\nCopy\nBash\n# Configure MLflow to communicate with a Databricks-hosted tracking server\nexport MLFLOW_TRACKING_URI=databricks\n# Specify your Databricks username & password\nexport DATABRICKS_USERNAME=\"...\"\nexport DATABRICKS_PASSWORD=\"...\"\n\n\nDatabricks Platform. Do one of:\n\nGenerate a REST API token and create your credentials file using databricks configure --token.\n\nSpecify credentials using environment variables:\n\nCopy\nBash\n# Configure MLflow to communicate with a Databricks-hosted tracking server\nexport MLFLOW_TRACKING_URI=databricks\n# Specify the workspace hostname and token\nexport DATABRICKS_HOST=\"...\"\nexport DATABRICKS_TOKEN=\"...\"\n\n\nStep 2: Configure MLflow applications\n\nConfigure MLflow applications to log to Databricks by setting the tracking URI to databricks, or databricks://<profileName>, if you specified a profile name via --profile while creating your credentials file. For example, you can achieve this by setting the MLFLOW_TRACKING_URI environment variable to “databricks”.\n\nStep 3: Configure the MLflow CLI\n\nConfigure the MLflow CLI to communicate with a Databricks tracking server with the MLFLOW_TRACKING_URI environment variable. For example, to create an experiment using the CLI with the tracking URI databricks, run:\n\nCopy\nBash\n# Replace <your-username> with your Databricks username\nexport MLFLOW_TRACKING_URI=databricks\nmlflow experiments create -n /Users/<your-username>/my-experiment\n\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nStep 1: Configure your environment\nStep 2: Configure MLflow applications\nStep 3: Configure the MLflow CLI\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Build dashboards with the MLflow Search API | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/build-dashboards.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nTrack ML and deep learning training runs\nOrganize training runs with MLflow experiments\nManage training code with MLflow runs\nBuild dashboards with the MLflow Search API\nDatabricks Autologging\nAccess the MLflow tracking server from outside Databricks\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Track model development using MLflow  Track ML and deep learning training runs  Build dashboards with the MLflow Search API\nBuild dashboards with the MLflow Search API\n\nOctober 10, 2023\n\nYou can pull aggregate metrics on your MLflow runs using the mlflow.search_runs API and display them in a dashboard. Regularly such reviewing metrics can provide insight into your progress and productivity. For example, you can track improvement of a goal metric like revenue or accuracy over time, across many runs and/or experiments.\n\nThis notebook demonstrates how to build the following custom dashboard using the mlflow.search_runs API:\n\nYou can either run the notebook on your own experiments or against autogenerated mock experiment data.\n\nDashboard comparing MLflow runs notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Manage training code with MLflow runs | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/runs.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nTrack ML and deep learning training runs\nOrganize training runs with MLflow experiments\nManage training code with MLflow runs\nBuild dashboards with the MLflow Search API\nDatabricks Autologging\nAccess the MLflow tracking server from outside Databricks\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Track model development using MLflow  Track ML and deep learning training runs  Manage training code with MLflow runs\nManage training code with MLflow runs\n\nNovember 22, 2024\n\nThis article describes MLflow runs for managing machine learning training. It also includes guidance on how to manage and compare runs across experiments.\n\nAn MLflow run corresponds to a single execution of model code. Each run records the following information:\n\nSource: Name of the notebook that launched the run or the project name and entry point for the run.\n\nVersion: Git commit hash if notebook is stored in a Databricks Git folder. Otherwise, notebook revision.\n\nStart & end time: Start and end time of the run.\n\nParameters: Model parameters saved as key-value pairs. Both keys and values are strings.\n\nMetrics: Model evaluation metrics saved as key-value pairs. The value is numeric. Each metric can be updated throughout the course of the run (for example, to track how your model’s loss function is converging), and MLflow records and lets you visualize the metric’s history.\n\nTags: Run metadata saved as key-value pairs. You can update tags during and after a run completes. Both keys and values are strings.\n\nArtifacts: Output files in any format. For example, you can record images, models (for example, a pickled scikit-learn model), and data files (for example, a Parquet file) as an artifact.\n\nAll MLflow runs are logged to the active experiment. If you have not explicitly set an experiment as the active experiment, runs are logged to the notebook experiment.\n\nView runs\n\nYou can access a run either from its parent experiment page or directly from the notebook that created the run.\n\nFrom the experiment page, in the runs table, click the start time of a run.\n\nFrom the notebook, click  next to the date and time of the run in the Experiment Runs sidebar.\n\nThe run screen shows the parameters used for the run, the metrics resulting from the run, and any tags or notes. To display Notes, Parameters, Metrics, or Tags for this run, click  to the left of the label.\n\nYou also access artifacts saved from a run in this screen.\n\nCode snippets for prediction\n\nIf you log a model from a run, the model appears in the Artifacts section of this page. To display code snippets illustrating how to load and use the model to make predictions on Spark and pandas DataFrames, click the model name.\n\nView the notebook used for a run\n\nTo view the version of the notebook that created a run:\n\nOn the experiment page, click the link in the Source column.\n\nOn the run page, click the link next to Source.\n\nFrom the notebook, in the Experiment Runs sidebar, click the Notebook icon  in the box for that Experiment Run.\n\nThe version of the notebook associated with the run appears in the main window with a highlight bar showing the date and time of the run.\n\nAdd a tag to a run\n\nTags are key-value pairs that you can create and use later to search for runs.\n\nFrom the run page, click  if it is not already open. The tags table appears.\n\nClick in the Name and Value fields and type the key and value for your tag.\n\nClick Add.\n\nEdit or delete a tag for a run\n\nTo edit or delete an existing tag, use the icons in the Actions column.\n\nReproduce the software environment of a run\n\nYou can reproduce the exact software environment for the run by clicking Reproduce Run. The following dialog appears:\n\nWith the default settings, when you click Confirm:\n\nThe notebook is cloned to the location shown in the dialog.\n\nIf the original cluster still exists, the cloned notebook is attached to the original cluster and the cluster is started.\n\nIf the original cluster no longer exists, a new cluster with the same configuration, including any installed libraries, is created and started. The notebook is attached to the new cluster.\n\nYou can select a different location for the cloned notebook and inspect the cluster configuration and installed libraries:\n\nTo select a different folder to save the cloned notebook, click Edit Folder.\n\nTo see the cluster spec, click View Spec. To clone only the notebook and not the cluster, uncheck this option.\n\nTo see the libraries installed on the original cluster, click View Libraries. If you don’t care about installing the same libraries as on the original cluster, uncheck this option.\n\nManage runs\nRename run\n\nTo rename a run, click  at the upper right corner of the run page and select Rename.\n\nFilter runs\n\nYou can search for runs based on parameter or metric values. You can also search for runs by tag.\n\nTo search for runs that match an expression containing parameter and metric values, enter a query in the search field and click Search. Some query syntax examples are:\n\nmetrics.r2 > 0.3\n\nparams.elasticNetParam = 0.5\n\nparams.elasticNetParam = 0.5 AND metrics.avg_areaUnderROC > 0.3\n\nMIN(metrics.rmse) <= 1\n\nMAX(metrics.memUsage) > 0.9\n\nLATEST(metrics.memUsage) = 0 AND MIN(metrics.rmse) <= 1\n\nBy default, metric values are filtered based on the last logged value. Using MIN or MAX lets you search for runs based on the minimum or maximum metric values, respectively. Only runs logged after August 2024 have minimum and maximum metric values.\n\nTo search for runs by tag, enter tags in the format: tags.<key>=\"<value>\". String values must be enclosed in quotes as shown.\n\ntags.estimator_name=\"RandomForestRegressor\"\n\ntags.color=\"blue\" AND tags.size=5\n\nBoth keys and values can contain spaces. If the key includes spaces, you must enclose it in backticks as shown.\n\nCopy\ntags.`my custom tag` = \"my value\"\n\n\nYou can also filter runs based on their state (Active or Deleted) and based on whether a model version is associated with the run. To do this, make your selections from the State and Time Created drop-down menus respectively.\n\nDownload runs\n\nSelect one or more runs.\n\nClick Download CSV. A CSV file containing the following fields downloads:\n\nCopy\nRun ID,Name,Source Type,Source Name,User,Status,<parameter1>,<parameter2>,...,<metric1>,<metric2>,...\n\nDelete runs\n\nYou can delete runs using the Databricks Mosaic AI UI with the following steps:\n\nIn the experiment, select one or more runs by clicking in the checkbox to the left of the run.\n\nClick Delete.\n\nIf the run is a parent run, decide whether you also want to delete descendant runs. This option is selected by default.\n\nClick Delete to confirm. Deleted runs are saved for 30 days. To display deleted runs, select Deleted in the State field.\n\nBulk delete runs based on the creation time\n\nYou can use Python to bulk delete runs of an experiment that were created prior to or at a UNIX timestamp. Using Databricks Runtime 14.1 or later, you can call the mlflow.delete_runs API to delete runs and return the number of runs deleted.\n\nThe following are the mlflow.delete_runs parameters:\n\nexperiment_id: The ID of the experiment containing the runs to delete.\n\nmax_timestamp_millis: The maximum creation timestamp in milliseconds since the UNIX epoch for deleting runs. Only runs created prior to or at this timestamp are deleted.\n\nmax_runs: Optional. A positive integer that indicates the maximum number of runs to delete. The maximum allowed value for max_runs is 10000. If not specified, max_runs defaults to 10000.\n\nCopy\nPython\nimport mlflow\n\n# Replace <experiment_id>, <max_timestamp_ms>, and <max_runs> with your values.\nruns_deleted = mlflow.delete_runs(\n  experiment_id=<experiment_id>,\n  max_timestamp_millis=<max_timestamp_ms>,\n  max_runs=<max_runs>\n)\n# Example:\nruns_deleted = mlflow.delete_runs(\n  experiment_id=\"4183847697906956\",\n  max_timestamp_millis=1711990504000,\n  max_runs=10\n)\n\n\nUsing Databricks Runtime 13.3 LTS or earlier, you can run the following client code in a Databricks Notebook.\n\nCopy\nPython\nfrom typing import Optional\n\ndef delete_runs(experiment_id: str,\n                max_timestamp_millis: int,\n                max_runs: Optional[int] = None) -> int:\n    \"\"\"\n    Bulk delete runs in an experiment that were created prior to or at the specified timestamp.\n    Deletes at most max_runs per request.\n\n    :param experiment_id: The ID of the experiment containing the runs to delete.\n    :param max_timestamp_millis: The maximum creation timestamp in milliseconds\n                                 since the UNIX epoch for deleting runs. Only runs\n                                 created prior to or at this timestamp are deleted.\n    :param max_runs: Optional. A positive integer indicating the maximum number\n                     of runs to delete. The maximum allowed value for max_runs\n                     is 10000. If not specified, max_runs defaults to 10000.\n    :return: The number of runs deleted.\n    \"\"\"\n    from mlflow.utils.databricks_utils import get_databricks_host_creds\n    from mlflow.utils.request_utils import augmented_raise_for_status\n    from mlflow.utils.rest_utils import http_request\n\n    json_body = {\"experiment_id\": experiment_id, \"max_timestamp_millis\": max_timestamp_millis}\n    if max_runs is not None:\n        json_body[\"max_runs\"] = max_runs\n    response = http_request(\n        host_creds=get_databricks_host_creds(),\n        endpoint=\"/api/2.0/mlflow/databricks/runs/delete-runs\",\n        method=\"POST\",\n        json=json_body,\n    )\n    augmented_raise_for_status(response)\n    return response.json()[\"runs_deleted\"]\n\n\nSee the Databricks Experiments API documentation for parameters and return value specifications for deleting runs based on creation time.\n\nRestore runs\n\nYou can restore previously deleted runs using the Databricks Mosaic AI UI.\n\nOn the Experiment page, select Deleted in the State field to display deleted runs.\n\nSelect one or more runs by clicking in the checkbox to the left of the run.\n\nClick Restore.\n\nClick Restore to confirm. To display the restored runs, select Active in the State field.\n\nBulk restore runs based on the deletion time\n\nYou can also use Python to bulk restore runs of an experiment that were deleted at or after a UNIX timestamp. Using Databricks Runtime 14.1 or later, you can call the mlflow.restore_runs API to restore runs and return the number of restored runs.\n\nThe following are the mlflow.restore_runs parameters:\n\nexperiment_id: The ID of the experiment containing the runs to restore.\n\nmin_timestamp_millis: The minimum deletion timestamp in milliseconds since the UNIX epoch for restoring runs. Only runs deleted at or after this timestamp are restored.\n\nmax_runs: Optional. A positive integer that indicates the maximum number of runs to restore. The maximum allowed value for max_runs is 10000. If not specified, max_runs defaults to 10000.\n\nCopy\nPython\nimport mlflow\n\n# Replace <experiment_id>, <min_timestamp_ms>, and <max_runs> with your values.\nruns_restored = mlflow.restore_runs(\n  experiment_id=<experiment_id>,\n  min_timestamp_millis=<min_timestamp_ms>,\n  max_runs=<max_runs>\n)\n# Example:\nruns_restored = mlflow.restore_runs(\n  experiment_id=\"4183847697906956\",\n  min_timestamp_millis=1711990504000,\n  max_runs=10\n)\n\n\nUsing Databricks Runtime 13.3 LTS or earlier, you can run the following client code in a Databricks Notebook.\n\nCopy\nPython\nfrom typing import Optional\n\ndef restore_runs(experiment_id: str,\n                 min_timestamp_millis: int,\n                 max_runs: Optional[int] = None) -> int:\n    \"\"\"\n    Bulk restore runs in an experiment that were deleted at or after the specified timestamp.\n    Restores at most max_runs per request.\n\n    :param experiment_id: The ID of the experiment containing the runs to restore.\n    :param min_timestamp_millis: The minimum deletion timestamp in milliseconds\n                                 since the UNIX epoch for restoring runs. Only runs\n                                 deleted at or after this timestamp are restored.\n    :param max_runs: Optional. A positive integer indicating the maximum number\n                     of runs to restore. The maximum allowed value for max_runs\n                     is 10000. If not specified, max_runs defaults to 10000.\n    :return: The number of runs restored.\n    \"\"\"\n    from mlflow.utils.databricks_utils import get_databricks_host_creds\n    from mlflow.utils.request_utils import augmented_raise_for_status\n    from mlflow.utils.rest_utils import http_request\n    json_body = {\"experiment_id\": experiment_id, \"min_timestamp_millis\": min_timestamp_millis}\n    if max_runs is not None:\n        json_body[\"max_runs\"] = max_runs\n    response = http_request(\n        host_creds=get_databricks_host_creds(),\n        endpoint=\"/api/2.0/mlflow/databricks/runs/restore-runs\",\n        method=\"POST\",\n        json=json_body,\n    )\n    augmented_raise_for_status(response)\n    return response.json()[\"runs_restored\"]\n\n\nSee the Databricks Experiments API documentation for parameters and return value specifications for restoring runs based on deletion time.\n\nCompare runs\n\nYou can compare runs from a single experiment or from multiple experiments. The Comparing Runs page presents information about the selected runs in graphic and tabular formats. You can also create visualizations of run results and tables of run information, run parameters, and metrics.\n\nTo create a visualization:\n\nSelect the plot type (Parallel Coordinates Plot, Scatter Plot, or Contour Plot).\n\nFor a Parallel Coordinates Plot, select the parameters and metrics to plot. From here, you can identify relationships between the selected parameters and metrics, which helps you better define the hyperparameter tuning space for your models.\n\nFor a Scatter Plot or Contour Plot, select the parameter or metric to display on each axis.\n\nThe Parameters and Metrics tables display the run parameters and metrics from all selected runs. The columns in these tables are identified by the Run details table immediately above. For simplicity, you can hide parameters and metrics that are identical in all selected runs by toggling .\n\nCompare runs from a single experiment\n\nOn the experiment page, select two or more runs by clicking in the checkbox to the left of the run, or select all runs by checking the box at the top of the column.\n\nClick Compare. The Comparing <N> Runs screen appears.\n\nCompare runs from multiple experiments\n\nOn the experiments page, select the experiments you want to compare by clicking in the box at the left of the experiment name.\n\nClick Compare (n) (n is the number of experiments you selected). A screen appears showing all of the runs from the experiments you selected.\n\nSelect two or more runs by clicking in the checkbox to the left of the run, or select all runs by checking the box at the top of the column.\n\nClick Compare. The Comparing <N> Runs screen appears.\n\nCopy runs between workspaces\n\nTo import or export MLflow runs to or from your Databricks workspace, you can use the community-driven open source project MLflow Export-Import.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nView runs\nAdd a tag to a run\nReproduce the software environment of a run\nManage runs\nCompare runs\nCopy runs between workspaces\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Organize training runs with MLflow experiments | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/experiments.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nTrack ML and deep learning training runs\nOrganize training runs with MLflow experiments\nManage training code with MLflow runs\nBuild dashboards with the MLflow Search API\nDatabricks Autologging\nAccess the MLflow tracking server from outside Databricks\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Track model development using MLflow  Track ML and deep learning training runs  Organize training runs with MLflow experiments\nOrganize training runs with MLflow experiments\n\nNovember 26, 2024\n\nExperiments are units of organization for your model training runs. There are two types of experiments: workspace and notebook.\n\nYou can create a workspace experiment from the Databricks Mosaic AI UI or the MLflow API. Workspace experiments are not associated with any notebook, and any notebook can log a run to these experiments by using the experiment ID or the experiment name.\n\nA notebook experiment is associated with a specific notebook. Databricks automatically creates a notebook experiment if there is no active experiment when you start a run using mlflow.start_run().\n\nTo see all of the experiments in a workspace that you have access to, select Machine Learning > Experiments in the sidebar.\n\nCreate workspace experiment\n\nThis section describes how to create a workspace experiment using the Databricks UI. You can create a workspace experiment directly from the workspace or from the Experiments page.\n\nYou can also use the MLflow API, or the Databricks Terraform provider with databricks_mlflow_experiment.\n\nFor instructions on logging runs to workspace experiments, see Logging example notebook.\n\nNote\n\nFor GDPR compliance, stale experiments are purged from workspaces. For workspaces where customer-managed keys are used, the stale experiments are only purged when the customer-managed key is accessible by Databricks. See Configure customer-managed keys for encryption to give access.\n\nCreate experiment from the workspace\n\nClick  Workspace in the sidebar.\n\nNavigate to the folder in which you want to create the experiment.\n\nRight-click on the folder and select Create > MLflow experiment.\n\nIn the Create MLflow Experiment dialog, enter a name for the experiment and an optional artifact location. If you do not specify an artifact location, artifacts are stored in MLflow-managed artifact storage: dbfs:/databricks/mlflow-tracking/<experiment-id>.\n\nDatabricks supports Unity Catalog volumes and S3 artifact locations.\n\nIn MLflow 2.15.0 and above, you can store artifacts in a Unity Catalog volume. When you create an MLflow experiment, specify a volumes path of the form dbfs:/Volumes/catalog_name/schema_name/volume_name/user/specified/path as your MLflow experiment artifact location, as shown in the following code:\n\nCopy\nPython\nEXP_NAME = \"/Users/first.last@databricks.com/my_experiment_name\"\nCATALOG = \"my_catalog\"\nSCHEMA = \"my_schema\"\nVOLUME = \"my_volume\"\nARTIFACT_PATH = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n\nmlflow.set_tracking_uri(\"databricks\")\nmlflow.set_registry_uri(\"databricks-uc\")\n\nif mlflow.get_experiment_by_name(EXP_NAME) is None:\n    mlflow.create_experiment(name=EXP_NAME, artifact_location=ARTIFACT_PATH)\nmlflow.set_experiment(EXP_NAME)\n\n\nTo store artifacts in S3, specify a URI of the form s3://<bucket>/<path>. MLflow obtains credentials to access S3 from your clusters’s instance profile. Artifacts stored in S3 do not appear in the MLflow UI; you must download them using an object storage client.\n\nThe upload and download file size limits are both 5GB.\n\nNote\n\nWhen you store an artifact in a location other than DBFS, the artifact does not appear in the MLflow UI. Models stored in locations other than DBFS cannot be registered in Model Registry.\n\nClick Create. An empty experiment appears.\n\nTo log runs to this experiment, call mlflow.set_experiment() with the experiment path. The experiment path appears at the top of the experiment page. See Logging example notebook for details and an example notebook.\n\nCreate experiment from the Experiments page\n\nTo create a foundation model fine-tuning, AutoML, or custom experiment, click Experiments or select New > Experiment in the left sidebar.\n\nAt the top of the page, select one of the following options to configure an experiment:\n\nFoundation Model Fine-tuning. The Foundation Model Fine-tuning dialog appears. For details, see Create a training run using the Foundation Model Fine-tuning UI.\n\nForecasting. The Configure Forecasting experiment dialog appears. For details, see Configure the AutoML experiment.\n\nClassification. The Configure Classification experiment dialog appears. For details, see Set up classification experiment with the UI.\n\nRegression. The Configure Classification experiment dialog appears. For details, see Set up regression experiment with the UI.\n\nCustom. The Create MLflow Experiment dialog appears. For details, see Step 4 in Create experiment from the workspace.\n\nCreate notebook experiment\n\nWhen you use the mlflow.start_run() command in a notebook, the run logs metrics and parameters to the active experiment. If no experiment is active, Databricks creates a notebook experiment. A notebook experiment shares the same name and ID as its corresponding notebook. The notebook ID is the numerical identifier at the end of a Notebook URL and ID.\n\nAlternatively, you can pass a Databricks workspace path to an existing notebook in mlflow.set_experiment() to create a notebook experiment for it.\n\nFor instructions on logging runs to notebook experiments, see Logging example notebook.\n\nNote\n\nIf you delete a notebook experiment using the API (for example, MlflowClient.tracking.delete_experiment() in Python), the notebook itself is moved into the Trash folder.\n\nView experiments\n\nEach experiment that you have access to appears on the experiments page. From this page, you can view any experiment. Click on an experiment name to display the experiment page.\n\nAdditional ways to access the experiment page:\n\nYou can access the experiment page for a workspace experiment from the workspace menu.\n\nYou can access the experiment page for a notebook experiment from the notebook.\n\nTo search for experiments, type text in the Filter experiments field and press Enter or click the magnifying glass icon. The experiment list changes to show only those experiments that contain the search text in the Name, Created by, Location, or Description column.\n\nClick the name of any experiment in the table to display its experiment page:\n\nThe experiment page lists all runs associated with the experiment. From the table, you can open the run page for any run associated with the experiment by clicking its Run Name. The Source column gives you access to the notebook version that created the run. You can also search and filter runs by metrics or parameter settings.\n\nView workspace experiment\n\nClick  Workspace in the sidebar.\n\nGo to the folder containing the experiment.\n\nClick the experiment name.\n\nView notebook experiment\n\nIn the notebook’s right sidebar, click the Experiment icon .\n\nThe Experiment Runs sidebar appears and shows a summary of each run associated with the notebook experiment, including run parameters and metrics. At the top of the sidebar is the name of the experiment that the notebook most recently logged runs to (either a notebook experiment or a workspace experiment).\n\nFrom the sidebar, you can navigate to the experiment page or directly to a run.\n\nTo view the experiment, click  at the far right, next to Experiment Runs.\n\nTo display a run, click the name of the run.\n\nManage experiments\n\nYou can rename, delete, or manage permissions for an experiment you own from the experiments page, the experiment page, or the workspace menu.\n\nNote\n\nYou cannot directly rename, delete, or manage permissions on an MLflow experiment that was created by a notebook in a Databricks Git folder. You must perform these actions at the Git folder level.\n\nRename experiment from the experiments page or the experiment page\n\nTo rename an experiment from the experiments page or the experiment page, click  and select Rename.\n\nRename experiment from the workspace menu\n\nClick  Workspace in the sidebar.\n\nGo to the folder containing the experiment.\n\nRight-click on the experiment name and select Rename.\n\nCopy experiment name\n\nTo copy the experiment name, click  at the top of the experiment page. You can use this name in the MLflow command set_experiment to set the active MLflow experiment.\n\nYou can also copy the experiment name from the experiment sidebar in a notebook.\n\nDelete notebook experiment\n\nNotebook experiments are part of the notebook and cannot be deleted separately. When you delete a notebook, the associated notebook experiment is deleted. When you delete a notebook experiment using the UI, the notebook is also deleted.\n\nTo delete notebook experiments using the API, use the Workspace API to ensure both the notebook and experiment are deleted from the workspace.\n\nDelete workspace experiment from the workspace menu\n\nClick  Workspace in the sidebar.\n\nGo to the folder containing the experiment.\n\nRight-click on the experiment name and select Move to Trash.\n\nDelete workspace or notebook experiment from the experiments page or the experiment page\n\nTo delete an experiment from the experiments page or the experiment page, click  and select Delete.\n\nWhen you delete a notebook experiment, the notebook is also deleted.\n\nChange permissions for experiment\n\nTo change permissions for an experiment from the experiment page, click Share.\n\nYou can change permissions for an experiment that you own from the experiments page. Click  in the Actions column and select Permission.\n\nFor information on experiment permission levels, see MLflow experiment ACLs.\n\nCopy experiments between workspaces\n\nTo migrate MLflow experiments between workspaces, you can use the community-driven open source project MLflow Export-Import.\n\nWith these tools, you can:\n\nShare and collaborate with other data scientists in the same or another tracking server. For example, you can clone an experiment from another user into your workspace.\n\nCopy MLflow experiments and runs from your local tracking server to your Databricks workspace.\n\nBack up mission critical experiments and models to another Databricks workspace.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nCreate workspace experiment\nCreate notebook experiment\nView experiments\nManage experiments\nCopy experiments between workspaces\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Workspace Model Registry example | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/workspace-model-registry-example.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nUpgrade ML workflows to target models in Unity Catalog\nUpgrade models to Unity Catalog\nManage model lifecycle using the Workspace Model Registry (legacy)\nWorkspace Model Registry example\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Manage model lifecycle in Unity Catalog  Workspace Model Registry example\nWorkspace Model Registry example\n\nOctober 10, 2023\n\nNote\n\nThis documentation covers the Workspace Model Registry. Databricks recommends using Models in Unity Catalog. Models in Unity Catalog provides centralized model governance, cross-workspace access, lineage, and deployment. Workspace Model Registry will be deprecated in the future.\n\nThis example illustrates how to use the Workspace Model Registry to build a machine learning application that forecasts the daily power output of a wind farm. The example shows how to:\n\nTrack and log models with MLflow\n\nRegister models with the Model Registry\n\nDescribe models and make model version stage transitions\n\nIntegrate registered models with production applications\n\nSearch and discover models in the Model Registry\n\nArchive and delete models\n\nThe article describes how to perform these steps using the MLflow Tracking and MLflow Model Registry UIs and APIs.\n\nFor a notebook that performs all these steps using the MLflow Tracking and Registry APIs, see the Model Registry example notebook.\n\nLoad dataset, train model, and track with MLflow Tracking\n\nBefore you can register a model in the Model Registry, you must first train and log the model during an experiment run. This section shows how to load the wind farm dataset, train a model, and log the training run to MLflow.\n\nLoad dataset\n\nThe following code loads a dataset containing weather data and power output information for a wind farm in the United States. The dataset contains wind direction, wind speed, and air temperature features sampled every six hours (once at 00:00, once at 08:00, and once at 16:00), as well as daily aggregate power output (power), over several years.\n\nCopy\nPython\nimport pandas as pd\nwind_farm_data = pd.read_csv(\"https://github.com/dbczumar/model-registry-demo-notebook/raw/master/dataset/windfarm_data.csv\", index_col=0)\n\ndef get_training_data():\n  training_data = pd.DataFrame(wind_farm_data[\"2014-01-01\":\"2018-01-01\"])\n  X = training_data.drop(columns=\"power\")\n  y = training_data[\"power\"]\n  return X, y\n\ndef get_validation_data():\n  validation_data = pd.DataFrame(wind_farm_data[\"2018-01-01\":\"2019-01-01\"])\n  X = validation_data.drop(columns=\"power\")\n  y = validation_data[\"power\"]\n  return X, y\n\ndef get_weather_and_forecast():\n  format_date = lambda pd_date : pd_date.date().strftime(\"%Y-%m-%d\")\n  today = pd.Timestamp('today').normalize()\n  week_ago = today - pd.Timedelta(days=5)\n  week_later = today + pd.Timedelta(days=5)\n\n  past_power_output = pd.DataFrame(wind_farm_data)[format_date(week_ago):format_date(today)]\n  weather_and_forecast = pd.DataFrame(wind_farm_data)[format_date(week_ago):format_date(week_later)]\n  if len(weather_and_forecast) < 10:\n    past_power_output = pd.DataFrame(wind_farm_data).iloc[-10:-5]\n    weather_and_forecast = pd.DataFrame(wind_farm_data).iloc[-10:]\n\n  return weather_and_forecast.drop(columns=\"power\"), past_power_output[\"power\"]\n\nTrain model\n\nThe following code trains a neural network using TensorFlow Keras to predict power output based on the weather features in the dataset. MLflow is used to track the model’s hyperparameters, performance metrics, source code, and artifacts.\n\nCopy\nPython\ndef train_keras_model(X, y):\n  import tensorflow.keras\n  from tensorflow.keras.models import Sequential\n  from tensorflow.keras.layers import Dense\n\n  model = Sequential()\n  model.add(Dense(100, input_shape=(X_train.shape[-1],), activation=\"relu\", name=\"hidden_layer\"))\n  model.add(Dense(1))\n  model.compile(loss=\"mse\", optimizer=\"adam\")\n\n  model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=.2)\n  return model\n\nimport mlflow\n\nX_train, y_train = get_training_data()\n\nwith mlflow.start_run():\n  # Automatically capture the model's parameters, metrics, artifacts,\n  # and source code with the `autolog()` function\n  mlflow.tensorflow.autolog()\n\n  train_keras_model(X_train, y_train)\n  run_id = mlflow.active_run().info.run_id\n\nRegister and manage the model using the MLflow UI\n\nIn this section:\n\nCreate a new registered model\n\nExplore the Model Registry UI\n\nAdd model descriptions\n\nTransition a model version\n\nCreate a new registered model\n\nNavigate to the MLflow Experiment Runs sidebar by clicking the Experiment icon  in the Databricks notebook’s right sidebar.\n\nLocate the MLflow Run corresponding to the TensorFlow Keras model training session, and open it in the MLflow Run UI by clicking the View Run Detail icon.\n\nIn the MLflow UI, scroll down to the Artifacts section and click the directory named model. Click the Register Model button that appears.\n\nSelect Create New Model from the drop-down menu, and input the following model name: power-forecasting-model.\n\nClick Register. This registers a new model called power-forecasting-model and creates a new model version: Version 1.\n\nAfter a few moments, the MLflow UI displays a link to the new registered model. Follow this link to open the new model version in the MLflow Model Registry UI.\n\nExplore the Model Registry UI\n\nThe model version page in the MLflow Model Registry UI provides information about Version 1 of the registered forecasting model, including its author, creation time, and its current stage.\n\nThe model version page also provides a Source Run link, which opens the MLflow Run that was used to create the model in the MLflow Run UI. From the MLflow Run UI, you can access the Source notebook link to view a snapshot of the Databricks notebook that was used to train the model.\n\nTo navigate back to the MLflow Model Registry, click  Models in sidebar.\n\nThe resulting MLflow Model Registry home page displays a list of all the registered models in your Databricks workspace, including their versions and stages.\n\nClick the power-forecasting-model link to open the registered model page, which displays all of the versions of the forecasting model.\n\nAdd model descriptions\n\nYou can add descriptions to registered models and model versions. Registered model descriptions are useful for recording information that applies to multiple model versions (e.g., a general overview of the modeling problem and dataset). Model version descriptions are useful for detailing the unique attributes of a particular model version (e.g., the methodology and algorithm used to develop the model).\n\nAdd a high-level description to the registered power forecasting model. Click the  icon and enter the following description:\n\nCopy\nThis model forecasts the power output of a wind farm based on weather data. The weather data consists of three features: wind speed, wind direction, and air temperature.\n\n\nClick Save.\n\nClick the Version 1 link from the registered model page to navigate back to the model version page.\n\nClick the  icon and enter the following description:\n\nCopy\nThis model version was built using TensorFlow Keras. It is a feed-forward neural network with one hidden layer.\n\n\nClick Save.\n\nTransition a model version\n\nThe MLflow Model Registry defines several model stages: None, Staging, Production, and Archived. Each stage has a unique meaning. For example, Staging is meant for model testing, while Production is for models that have completed the testing or review processes and have been deployed to applications.\n\nClick the Stage button to display the list of available model stages and your available stage transition options.\n\nSelect Transition to -> Production and press OK in the stage transition confirmation window to transition the model to Production.\n\nAfter the model version is transitioned to Production, the current stage is displayed in the UI, and an entry is added to the activity log to reflect the transition.\n\nThe MLflow Model Registry allows multiple model versions to share the same stage. When referencing a model by stage, the Model Registry uses the latest model version (the model version with the largest version ID). The registered model page displays all of the versions of a particular model.\n\nRegister and manage the model using the MLflow API\n\nIn this section:\n\nDefine the model’s name programmatically\n\nRegister the model\n\nAdd model and model version descriptions using the API\n\nTransition a model version and retrieve details using the API\n\nDefine the model’s name programmatically\n\nNow that the model has been registered and transitioned to Production, you can reference it using MLflow programmatic APIs. Define the registered model’s name as follows:\n\nCopy\nPython\nmodel_name = \"power-forecasting-model\"\n\nRegister the model\nCopy\nPython\nmodel_name = get_model_name()\n\nimport mlflow\n\n# The default path where the MLflow autologging function stores the TensorFlow Keras model\nartifact_path = \"model\"\nmodel_uri = \"runs:/{run_id}/{artifact_path}\".format(run_id=run_id, artifact_path=artifact_path)\n\nmodel_details = mlflow.register_model(model_uri=model_uri, name=model_name)\n\nimport time\nfrom mlflow.tracking.client import MlflowClient\nfrom mlflow.entities.model_registry.model_version_status import ModelVersionStatus\n\n# Wait until the model is ready\ndef wait_until_ready(model_name, model_version):\n  client = MlflowClient()\n  for _ in range(10):\n    model_version_details = client.get_model_version(\n      name=model_name,\n      version=model_version,\n    )\n    status = ModelVersionStatus.from_string(model_version_details.status)\n    print(\"Model status: %s\" % ModelVersionStatus.to_string(status))\n    if status == ModelVersionStatus.READY:\n      break\n    time.sleep(1)\n\nwait_until_ready(model_details.name, model_details.version)\n\nAdd model and model version descriptions using the API\nCopy\nPython\nfrom mlflow.tracking.client import MlflowClient\n\nclient = MlflowClient()\nclient.update_registered_model(\n  name=model_details.name,\n  description=\"This model forecasts the power output of a wind farm based on weather data. The weather data consists of three features: wind speed, wind direction, and air temperature.\"\n)\n\nclient.update_model_version(\n  name=model_details.name,\n  version=model_details.version,\n  description=\"This model version was built using TensorFlow Keras. It is a feed-forward neural network with one hidden layer.\"\n)\n\nTransition a model version and retrieve details using the API\nCopy\nPython\nclient.transition_model_version_stage(\n  name=model_details.name,\n  version=model_details.version,\n  stage='production',\n)\nmodel_version_details = client.get_model_version(\n  name=model_details.name,\n  version=model_details.version,\n)\nprint(\"The current model stage is: '{stage}'\".format(stage=model_version_details.current_stage))\n\nlatest_version_info = client.get_latest_versions(model_name, stages=[\"production\"])\nlatest_production_version = latest_version_info[0].version\nprint(\"The latest production version of the model '%s' is '%s'.\" % (model_name, latest_production_version))\n\nLoad versions of the registered model using the API\n\nThe MLflow Models component defines functions for loading models from several machine learning frameworks. For example, mlflow.tensorflow.load_model() is used to load TensorFlow models that were saved in MLflow format, and mlflow.sklearn.load_model() is used to load scikit-learn models that were saved in MLflow format.\n\nThese functions can load models from the MLflow Model Registry.\n\nCopy\nPython\nimport mlflow.pyfunc\n\nmodel_version_uri = \"models:/{model_name}/1\".format(model_name=model_name)\n\nprint(\"Loading registered model version from URI: '{model_uri}'\".format(model_uri=model_version_uri))\nmodel_version_1 = mlflow.pyfunc.load_model(model_version_uri)\n\nmodel_production_uri = \"models:/{model_name}/production\".format(model_name=model_name)\n\nprint(\"Loading registered model version from URI: '{model_uri}'\".format(model_uri=model_production_uri))\nmodel_production = mlflow.pyfunc.load_model(model_production_uri)\n\nForecast power output with the production model\n\nIn this section, the production model is used to evaluate weather forecast data for the wind farm. The forecast_power() application loads the latest version of the forecasting model from the specified stage and uses it to forecast power production over the next five days.\n\nCopy\nPython\ndef plot(model_name, model_stage, model_version, power_predictions, past_power_output):\n  import pandas as pd\n  import matplotlib.dates as mdates\n  from matplotlib import pyplot as plt\n  index = power_predictions.index\n  fig = plt.figure(figsize=(11, 7))\n  ax = fig.add_subplot(111)\n  ax.set_xlabel(\"Date\", size=20, labelpad=20)\n  ax.set_ylabel(\"Power\\noutput\\n(MW)\", size=20, labelpad=60, rotation=0)\n  ax.tick_params(axis='both', which='major', labelsize=17)\n  ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n  ax.plot(index[:len(past_power_output)], past_power_output, label=\"True\", color=\"red\", alpha=0.5, linewidth=4)\n  ax.plot(index, power_predictions.squeeze(), \"--\", label=\"Predicted by '%s'\\nin stage '%s' (Version %d)\" % (model_name, model_stage, model_version), color=\"blue\", linewidth=3)\n  ax.set_ylim(ymin=0, ymax=max(3500, int(max(power_predictions.values) * 1.3)))\n  ax.legend(fontsize=14)\n  plt.title(\"Wind farm power output and projections\", size=24, pad=20)\n  plt.tight_layout()\n  display(plt.show())\n\ndef forecast_power(model_name, model_stage):\n  from mlflow.tracking.client import MlflowClient\n  client = MlflowClient()\n  model_version = client.get_latest_versions(model_name, stages=[model_stage])[0].version\n  model_uri = \"models:/{model_name}/{model_stage}\".format(model_name=model_name, model_stage=model_stage)\n  model = mlflow.pyfunc.load_model(model_uri)\n  weather_data, past_power_output = get_weather_and_forecast()\n  power_predictions = pd.DataFrame(model.predict(weather_data))\n  power_predictions.index = pd.to_datetime(weather_data.index)\n  print(power_predictions)\n  plot(model_name, model_stage, int(model_version), power_predictions, past_power_output)\n\nCreate a new model version\n\nClassical machine learning techniques are also effective for power forecasting. The following code trains a random forest model using scikit-learn and registers it with the MLflow Model Registry via the mlflow.sklearn.log_model() function.\n\nCopy\nPython\nimport mlflow.sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nwith mlflow.start_run():\n  n_estimators = 300\n  mlflow.log_param(\"n_estimators\", n_estimators)\n\n  rand_forest = RandomForestRegressor(n_estimators=n_estimators)\n  rand_forest.fit(X_train, y_train)\n\n  val_x, val_y = get_validation_data()\n  mse = mean_squared_error(rand_forest.predict(val_x), val_y)\n  print(\"Validation MSE: %d\" % mse)\n  mlflow.log_metric(\"mse\", mse)\n\n  # Specify the `registered_model_name` parameter of the `mlflow.sklearn.log_model()`\n  # function to register the model with the MLflow Model Registry. This automatically\n  # creates a new model version\n  mlflow.sklearn.log_model(\n    sk_model=rand_forest,\n    artifact_path=\"sklearn-model\",\n    registered_model_name=model_name,\n  )\n\nFetch the new model version ID using MLflow Model Registry search\nCopy\nPython\nfrom mlflow.tracking.client import MlflowClient\nclient = MlflowClient()\n\nmodel_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\nnew_model_version = max([model_version_info.version for model_version_info in model_version_infos])\n\nwait_until_ready(model_name, new_model_version)\n\nAdd a description to the new model version\nCopy\nPython\nclient.update_model_version(\n  name=model_name,\n  version=new_model_version,\n  description=\"This model version is a random forest containing 100 decision trees that was trained in scikit-learn.\"\n)\n\nTransition the new model version to Staging and test the model\n\nBefore deploying a model to a production application, it is often best practice to test it in a staging environment. The following code transitions the new model version to Staging and evaluates its performance.\n\nCopy\nPython\nclient.transition_model_version_stage(\n  name=model_name,\n  version=new_model_version,\n  stage=\"Staging\",\n)\n\nforecast_power(model_name, \"Staging\")\n\nDeploy the new model version to Production\n\nAfter verifying that the new model version performs well in staging, the following code transitions the model to Production and uses the exact same application code from the Forecast power output with the production model section to produce a power forecast.\n\nCopy\nPython\nclient.transition_model_version_stage(\n  name=model_name,\n  version=new_model_version,\n  stage=\"production\",\n)\n\nforecast_power(model_name, \"production\")\n\n\nThere are now two model versions of the forecasting model in the Production stage: the model version trained in Keras model and the version trained in scikit-learn.\n\nNote\n\nWhen referencing a model by stage, the MLflow Model Model Registry automatically uses the latest production version. This enables you to update your production models without changing any application code.\n\nArchive and delete models\n\nWhen a model version is no longer being used, you can archive it or delete it. You can also delete an entire registered model; this removes all of its associated model versions.\n\nArchive Version 1 of the power forecasting model\n\nArchive Version 1 of the power forecasting model because it is no longer being used. You can archive models in the MLflow Model Registry UI or via the MLflow API.\n\nArchive Version 1 in the MLflow UI\n\nTo archive Version 1 of the power forecasting model:\n\nOpen its corresponding model version page in the MLflow Model Registry UI:\n\nClick the Stage button, select Transition To -> Archived:\n\nPress OK in the stage transition confirmation window.\n\nArchive Version 1 using the MLflow API\n\nThe following code uses the MlflowClient.update_model_version() function to archive Version 1 of the power forecasting model.\n\nCopy\nPython\nfrom mlflow.tracking.client import MlflowClient\n\nclient = MlflowClient()\nclient.transition_model_version_stage(\n  name=model_name,\n  version=1,\n  stage=\"Archived\",\n)\n\nDelete Version 1 of the power forecasting model\n\nYou can also use the MLflow UI or MLflow API to delete model versions.\n\nWarning\n\nModel version deletion is permanent and cannot be undone.\n\nDelete Version 1 in the MLflow UI\n\nTo delete Version 1 of the power forecasting model:\n\nOpen its corresponding model version page in the MLflow Model Registry UI.\n\nSelect the drop-down arrow next to the version identifier and click Delete.\n\nDelete Version 1 using the MLflow API\nCopy\nPython\nclient.delete_model_version(\n   name=model_name,\n   version=1,\n)\n\nDelete the model using the MLflow API\n\nYou must first transition all remaining model version stages to None or Archived.\n\nCopy\nPython\nfrom mlflow.tracking.client import MlflowClient\n\nclient = MlflowClient()\nclient.transition_model_version_stage(\n  name=model_name,\n  version=2,\n  stage=\"Archived\",\n)\n\nCopy\nPython\nclient.delete_registered_model(name=model_name)\n\nNotebook\nMLflow Model Registry example notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nLoad dataset, train model, and track with MLflow Tracking\nRegister and manage the model using the MLflow UI\nRegister and manage the model using the MLflow API\nLoad versions of the registered model using the API\nForecast power output with the production model\nCreate a new model version\nArchive and delete models\nNotebook\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Manage model lifecycle using the Workspace Model Registry (legacy) | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/workspace-model-registry.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nUpgrade ML workflows to target models in Unity Catalog\nUpgrade models to Unity Catalog\nManage model lifecycle using the Workspace Model Registry (legacy)\nMLflow Model Registry Webhooks on Databricks\nShare models across workspaces\nWorkspace Model Registry example\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Manage model lifecycle in Unity Catalog  Manage model lifecycle using the Workspace Model Registry (legacy)\nManage model lifecycle using the Workspace Model Registry (legacy)\n\nJuly 11, 2024\n\nImportant\n\nThis documentation covers the Workspace Model Registry. If your workspace is enabled for Unity Catalog, do not use the procedures on this page. Instead, see Models in Unity Catalog.\n\nFor guidance on how to upgrade from the Workspace Model Registry to Unity Catalog, see Migrate workflows and models to Unity Catalog.\n\nIf your workspace’s default catalog is in Unity Catalog (rather than hive_metastore) and you are running a cluster using Databricks Runtime 13.3 LTS or above, models are automatically created in and loaded from the workspace default catalog, with no configuration required. To use the Workspace Model Registry in this case, you must explicitly target it by running import mlflow; mlflow.set_registry_uri(\"databricks\") at the start of your workload. A small number of workspaces where both the default catalog was configured to a catalog in Unity Catalog prior to January 2024 and the workspace model registry was used prior to January 2024 are exempt from this behavior and continue to use the Workspace Model Registry by default.\n\nStarting in April 2024, Databricks disabled Workspace Model Registry for workspaces in new accounts where the workspace’s default catalog is in Unity Catalog.\n\nThis article describes how to use the Workspace Model Registry as part of your machine learning workflow to manage the full lifecycle of ML models. The Workspace Model Registry is a Databricks-provided, hosted version of the MLflow Model Registry.\n\nThe Workspace Model Registry provides:\n\nChronological model lineage (which MLflow experiment and run produced the model at a given time).\n\nModel Serving.\n\nModel versioning.\n\nStage transitions (for example, from staging to production or archived).\n\nWebhooks so you can automatically trigger actions based on registry events.\n\nEmail notifications of model events.\n\nYou can also create and view model descriptions and leave comments.\n\nThis article includes instructions for both the Workspace Model Registry UI and the Workspace Model Registry API.\n\nFor an overview of Workspace Model Registry concepts, see ML lifecycle management using MLflow.\n\nCreate or register a model\n\nYou can create or register a model using the UI, or register a model using the API.\n\nCreate or register a model using the UI\n\nThere are two ways to register a model in the Workspace Model Registry. You can register an existing model that has been logged to MLflow, or you can create and register a new, empty model and then assign a previously logged model to it.\n\nRegister an existing logged model from a notebook\n\nIn the Workspace, identify the MLflow run containing the model you want to register.\n\nClick the Experiment icon  in the notebook’s right sidebar.\n\nIn the Experiment Runs sidebar, click the  icon next to the date of the run. The MLflow Run page displays. This page shows details of the run including parameters, metrics, tags, and list of artifacts.\n\nIn the Artifacts section, click the directory named xxx-model.\n\nClick the Register Model button at the far right.\n\nIn the dialog, click in the Model box and do one of the following:\n\nSelect Create New Model from the drop-down menu. The Model Name field appears. Enter a model name, for example scikit-learn-power-forecasting.\n\nSelect an existing model from the drop-down menu.\n\nClick Register.\n\nIf you selected Create New Model, this registers a model named scikit-learn-power-forecasting, copies the model into a secure location managed by the Workspace Model Registry, and creates a new version of the model.\n\nIf you selected an existing model, this registers a new version of the selected model.\n\nAfter a few moments, the Register Model button changes to a link to the new registered model version.\n\nClick the link to open the new model version in the Workspace Model Registry UI. You can also find the model in the Workspace Model Registry by clicking  Models in the sidebar.\n\nCreate a new registered model and assign a logged model to it\n\nYou can use the Create Model button on the registered models page to create a new, empty model and then assign a logged model to it. Follow these steps:\n\nOn the registered models page, click Create Model. Enter a name for the model and click Create.\n\nFollow Steps 1 through 3 in Register an existing logged model from a notebook.\n\nIn the Register Model dialog, select the name of the model you created in Step 1 and click Register. This registers a model with the name you created, copies the model into a secure location managed by the Workspace Model Registry, and creates a model version: Version 1.\n\nAfter a few moments, the MLflow Run UI replaces the Register Model button with a link to the new registered model version. You can now select the model from the Model drop-down list in the Register Model dialog on the Experiment Runs page. You can also register new versions of the model by specifying its name in API commands like Create ModelVersion.\n\nRegister a model using the API\n\nThere are three programmatic ways to register a model in the Workspace Model Registry. All methods copy the model into a secure location managed by the Workspace Model Registry.\n\nTo log a model and register it with the specified name during an MLflow experiment, use the mlflow.<model-flavor>.log_model(...) method. If a registered model with the name doesn’t exist, the method registers a new model, creates Version 1, and returns a ModelVersion MLflow object. If a registered model with the name exists already, the method creates a new model version and returns the version object.\n\nCopy\nPython\nwith mlflow.start_run(run_name=<run-name>) as run:\n  ...\n  mlflow.<model-flavor>.log_model(<model-flavor>=<model>,\n    artifact_path=\"<model-path>\",\n    registered_model_name=\"<model-name>\"\n  )\n\n\nTo register a model with the specified name after all your experiment runs complete and you have decided which model is most suitable to add to the registry, use the mlflow.register_model() method. For this method, you need the run ID for the mlruns:URI argument. If a registered model with the name doesn’t exist, the method registers a new model, creates Version 1, and returns a ModelVersion MLflow object. If a registered model with the name exists already, the method creates a new model version and returns the version object.\n\nCopy\nPython\nresult=mlflow.register_model(\"runs:<model-path>\", \"<model-name>\")\n\n\nTo create a new registered model with the specified name, use the MLflow Client API create_registered_model() method. If the model name exists, this method throws an MLflowException.\n\nCopy\nPython\nclient = MlflowClient()\nresult = client.create_registered_model(\"<model-name>\")\n\n\nYou can also register a model with the Databricks Terraform provider and databricks_mlflow_model.\n\nQuota limits\n\nStarting May 2024 for all Databricks workspaces, the Workspace Model Registry imposes quota limits on the total number of registered models and model versions per workspace. See Resource limits. If you exceed the registry quotas, Databricks recommends that you delete registered models and model versions that you no longer need. Databricks also recommends that you adjust your model registration and retention strategy to stay under the limit. If you require an increase to your workspace limits, reach out to your Databricks account team.\n\nThe following notebook illustrates how to inventory and delete your model registry entities.\n\nInventory workspace model registry entities notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nView models in the UI\nRegistered models page\n\nThe registered models page displays when you click  Models in the sidebar. This page shows all of the models in the registry.\n\nYou can create a new model from this page.\n\nAlso from this page, workspace administrators can set permissions for all models in the Workspace Model Registry.\n\nRegistered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n\nSet up Model Serving.\n\nAutomatically generate a notebook to use the model for inference.\n\nConfigure email notifications.\n\nCompare model versions.\n\nSet permissions for the model.\n\nDelete a model.\n\nModel version page\n\nTo view the model version page, do one of the following:\n\nClick a version name in the Latest Version column on the registered models page.\n\nClick a version name in the Version column in the registered model page.\n\nThis page displays information about a specific version of a registered model and also provides a link to the source run (the version of the notebook that was run to create the model). From this page, you can also:\n\nAutomatically generate a notebook to use the model for inference.\n\nDelete a model.\n\nControl access to models\n\nYou must have at least CAN MANAGE permission to configure permissions on a model. For information on model permission levels, see MLflow model ACLs. A model version inherits permissions from its parent model. You cannot set permissions for model versions.\n\nIn the sidebar, click  Models.\n\nSelect a model name.\n\nClick Permissions. The Permission Settings dialog opens\n\nIn the dialog, select the Select User, Group or Service Principal… drop-down and select a user, group, or service principal.\n\nSelect a permission from the permission drop-down.\n\nClick Add and click Save.\n\nWorkspace admins and users with CAN MANAGE permission at the registry-wide level can set permission levels on all models in the workspace by clicking Permissions on the Models page.\n\nTransition a model stage\n\nA model version has one of the following stages: None, Staging, Production, or Archived. The Staging stage is meant for model testing and validating, while the Production stage is for model versions that have completed the testing or review processes and have been deployed to applications for live scoring. An Archived model version is assumed to be inactive, at which point you can consider deleting it. Different versions of a model can be in different stages.\n\nA user with appropriate permission can transition a model version between stages. If you have permission to transition a model version to a particular stage, you can make the transition directly. If you do not have permission, you can request a stage transition and a user that has permission to transition model versions can approve, reject, or cancel the request.\n\nYou can transition a model stage using the UI or using the API.\n\nTransition a model stage using the UI\n\nFollow these instructions to transition a model’s stage.\n\nTo display the list of available model stages and your available options, in a model version page, click the drop down next to Stage: and request or select a transition to another stage.\n\nEnter an optional comment and click OK.\n\nTransition a model version to the Production stage\n\nAfter testing and validation, you can transition or request a transition to the Production stage.\n\nWorkspace Model Registry allows more than one version of the registered model in each stage. If you want to have only one version in Production, you can transition all versions of the model currently in Production to Archived by checking Transition existing Production model versions to Archived.\n\nApprove, reject, or cancel a model version stage transition request\n\nA user without stage transition permission can request a stage transition. The request appears in the Pending Requests section in the model version page:\n\nTo approve, reject, or cancel a stage transition request, click the Approve, Reject, or Cancel link.\n\nThe creator of a transition request can also cancel the request.\n\nView model version activities\n\nTo view all the transitions requested, approved, pending, and applied to a model version, go to the Activities section. This record of activities provides a lineage of the model’s lifecycle for auditing or inspection.\n\nTransition a model stage using the API\n\nUsers with appropriate permissions can transition a model version to a new stage.\n\nTo update a model version stage to a new stage, use the MLflow Client API transition_model_version_stage() method:\n\nCopy\nPython\n  client = MlflowClient()\n  client.transition_model_version_stage(\n    name=\"<model-name>\",\n    version=<model-version>,\n    stage=\"<stage>\",\n    description=\"<description>\"\n  )\n\n\nThe accepted values for <stage> are: \"Staging\"|\"staging\", \"Archived\"|\"archived\", \"Production\"|\"production\", \"None\"|\"none\".\n\nUse model for inference\n\nPreview\n\nThis feature is in Public Preview.\n\nAfter a model is registered in the Workspace Model Registry, you can automatically generate a notebook to use the model for batch or streaming inference. Alternatively, you can create an endpoint to use the model for real-time serving with Model Serving.\n\nIn the upper-right corner of the registered model page or the model version page, click . The Configure model inference dialog appears, which allows you to configure batch, streaming, or real-time inference.\n\nImportant\n\nAnaconda Inc. updated their terms of service for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See Anaconda Commercial Edition FAQ for more information. Your use of any Anaconda channels is governed by their terms of service.\n\nMLflow models logged before v1.18 (Databricks Runtime 8.3 ML or earlier) were by default logged with the conda defaults channel (https://repo.anaconda.com/pkgs/) as a dependency. Because of this license change, Databricks has stopped the use of the defaults channel for models logged using MLflow v1.18 and above. The default channel logged is now conda-forge, which points at the community managed https://conda-forge.org/.\n\nIf you logged a model before MLflow v1.18 without excluding the defaults channel from the conda environment for the model, that model may have a dependency on the defaults channel that you may not have intended. To manually confirm whether a model has this dependency, you can examine channel value in the conda.yaml file that is packaged with the logged model. For example, a model’s conda.yaml with a defaults channel dependency may look like this:\n\nCopy\nYAML\nchannels:\n- defaults\ndependencies:\n- python=3.8.8\n- pip\n- pip:\n    - mlflow\n    - scikit-learn==0.23.2\n    - cloudpickle==1.6.0\n      name: mlflow-env\n\n\nBecause Databricks can not determine whether your use of the Anaconda repository to interact with your models is permitted under your relationship with Anaconda, Databricks is not forcing its customers to make any changes. If your use of the Anaconda.com repo through the use of Databricks is permitted under Anaconda’s terms, you do not need to take any action.\n\nIf you would like to change the channel used in a model’s environment, you can re-register the model to the Workspace model registry with a new conda.yaml. You can do this by specifying the channel in the conda_env parameter of log_model().\n\nFor more information on the log_model() API, see the MLflow documentation for the model flavor you are working with, for example, log_model for scikit-learn.\n\nFor more information on conda.yaml files, see the MLflow documentation.\n\nConfigure batch inference\n\nWhen you follow these steps to create a batch inference notebook, the notebook is saved in your user folder under the Batch-Inference folder in a folder with the model’s name. You can edit the notebook as needed.\n\nClick the Batch inference tab.\n\nFrom the Model version drop-down, select the model version to use. The first two items in the drop-down are the current Production and Staging version of the model (if they exist). When you select one of these options, the notebook automatically uses the Production or Staging version as of the time it is run. You do not need to update the notebook as you continue to develop the model.\n\nClick the Browse button next to Input table. The Select input data dialog appears. If necessary, you can change the cluster in the Compute drop-down.\n\nNote\n\nFor Unity Catalog enabled workspaces, the Select input data dialog allows you to select from three levels, <catalog-name>.<database-name>.<table-name>.\n\nSelect the table containing the input data for the model, and click Select. The generated notebook automatically imports this data and sends it to the model. You can edit the generated notebook if the data requires any transformations before it is input to the model.\n\nPredictions are saved in a folder in the directory dbfs:/FileStore/batch-inference. By default, predictions are saved in a folder with the same name as the model. Each run of the generated notebook writes a new file to this directory with the timestamp appended to the name. You can also choose not to include the timestamp and to overwrite the file with subsequent runs of the notebook; instructions are provided in the generated notebook.\n\nYou can change the folder where the predictions are saved by typing a new folder name into the Output table location field or by clicking the folder icon to browse the directory and select a different folder.\n\nTo save predictions to a location in Unity Catalog, you must edit the notebook. For an example notebook that shows how to train a machine-learning model that uses data in Unity Catalog and write the results back to Unity Catalog, see Machine learning tutorial.\n\nConfigure streaming inference using Delta Live Tables\n\nWhen you follow these steps to create a streaming inference notebook, the notebook is saved in your user folder under the DLT-Inference folder in a folder with the model’s name. You can edit the notebook as needed.\n\nClick the Streaming (Delta Live Tables) tab.\n\nFrom the Model version drop-down, select the model version to use. The first two items in the drop-down are the current Production and Staging version of the model (if they exist). When you select one of these options, the notebook automatically uses the Production or Staging version as of the time it is run. You do not need to update the notebook as you continue to develop the model.\n\nClick the Browse button next to Input table. The Select input data dialog appears. If necessary, you can change the cluster in the Compute drop-down.\n\nNote\n\nFor Unity Catalog enabled workspaces, the Select input data dialog allows you to select from three levels, <catalog-name>.<database-name>.<table-name>.\n\nSelect the table containing the input data for the model, and click Select. The generated notebook creates a data transform that uses the input table as a source and integrates the MLflow PySpark inference UDF to perform model predictions. You can edit the generated notebook if the data requires any additional transformations before or after the model is applied.\n\nProvide the output Delta Live Table name. The notebook creates a live table with the given name and uses it to store the model predictions. You can modify the generated notebook to customize the target dataset as needed - for example: define a streaming live table as output, add schema information or data quality constraints.\n\nYou can then either create a new Delta Live Tables pipeline with this notebook or add it to an existing pipeline as an additional notebook library.\n\nConfigure real-time inference\n\nModel Serving exposes your MLflow machine learning models as scalable REST API endpoints. To create a Model Serving endpoint, see Create custom model serving endpoints.\n\nProvide feedback\n\nThis feature is in preview, and we would love to get your feedback. To provide feedback, click Provide Feedback in the Configure model inference dialog.\n\nCompare model versions\n\nYou can compare model versions in the Workspace Model Registry.\n\nOn the registered model page, select two or more model versions by clicking in the checkbox to the left of the model version.\n\nClick Compare.\n\nThe Comparing <N> Versions screen appears, showing a table that compares the parameters, schema, and metrics of the selected model versions. At the bottom of the screen, you can select the type of plot (scatter, contour, or parallel coordinates) and the parameters or metrics to plot.\n\nControl notification preferences\n\nYou can configure Workspace Model Registry to notify you by email about activity on registered models and model versions that you specify.\n\nOn the registered model page, the Notify me about menu shows three options:\n\nAll new activity: Send email notifications about all activity on all model versions of this model. If you created the registered model, this setting is the default.\n\nActivity on versions I follow: Send email notifications only about model versions you follow. With this selection, you receive notifications for all model versions that you follow; you cannot turn off notifications for a specific model version.\n\nMute notifications: Do not send email notifications about activity on this registered model.\n\nThe following events trigger an email notification:\n\nCreation of a new model version\n\nRequest for a stage transition\n\nStage transition\n\nNew comments\n\nYou are automatically subscribed to model notifications when you do any of the following:\n\nComment on that model version\n\nTransition a model version’s stage\n\nMake a transition request for the model’s stage\n\nTo see if you are following a model version, look at the Follow Status field on the model version page, or at the table of model versions on the registered model page.\n\nTurn off all email notifications\n\nYou can turn off email notifications in the Workspace Model Registry Settings tab of the User Settings menu:\n\nClick your username in the upper-right corner of the Databricks workspace, and select Settings from the drop-down menu.\n\nIn the Settings sidebar, select Notifications.\n\nTurn off Model Registry email notifications.\n\nAn account admin can turn off email notifications for the entire organization in the admin settings page.\n\nMaximum number of emails sent\n\nWorkspace Model Registry limits the number of emails sent to each user per day per activity. For example, if you receive 20 emails in one day about new model versions created for a registered model, Workspace Model Registry sends an email noting that the daily limit has been reached, and no additional emails about that event are sent until the next day.\n\nTo increase the limit of the number of emails allowed, contact your Databricks account team.\n\nWebhooks\n\nPreview\n\nThis feature is in Public Preview.\n\nWebhooks enable you to listen for Workspace Model Registry events so your integrations can automatically trigger actions. You can use webhooks to automate and integrate your machine learning pipeline with existing CI/CD tools and workflows. For example, you can trigger CI builds when a new model version is created or notify your team members through Slack each time a model transition to production is requested.\n\nAnnotate a model or model version\n\nYou can provide information about a model or model version by annotating it. For example, you may want to include an overview of the problem or information about the methodology and algorithm used.\n\nAnnotate a model or model version using the UI\n\nThe Databricks UI provides several ways to annotate models and model versions. You can add text information using a description or comments, and you can add searchable key-value tags. Descriptions and tags are available for models and model versions; comments are only available for model versions.\n\nDescriptions are intended to provide information about the model.\n\nComments provide a way to maintain an ongoing discussion about activities on a model version.\n\nTags let you customize model metadata to make it easier to find specific models.\n\nAdd or update the description for a model or model version\n\nFrom the registered model or model version page, click Edit next to Description. An edit window appears.\n\nEnter or edit the description in the edit window.\n\nClick Save to save your changes or Cancel to close the window.\n\nIf you entered a description of a model version, the description appears in the Description column in the table on the registered model page. The column displays a maximum of 32 characters or one line of text, whichever is shorter.\n\nAdd comments for a model version\n\nScroll down the model version page and click the down arrow next to Activities.\n\nType your comment in the edit window and click Add Comment.\n\nAdd tags for a model or model version\n\nFrom the registered model or model version page, click  if it is not already open. The tags table appears.\n\nClick in the Name and Value fields and type the key and value for your tag.\n\nClick Add.\n\nEdit or delete tags for a model or model version\n\nTo edit or delete an existing tag, use the icons in the Actions column.\n\nAnnotate a model version using the API\n\nTo update a model version description, use the MLflow Client API update_model_version() method:\n\nCopy\nPython\nclient = MlflowClient()\nclient.update_model_version(\n  name=\"<model-name>\",\n  version=<model-version>,\n  description=\"<description>\"\n)\n\n\nTo set or update a tag for a registered model or model version, use the MLflow Client API `set_registered_model_tag()`) or `set_model_version_tag()` method:\n\nCopy\nPython\nclient = MlflowClient()\nclient.set_registered_model_tag()(\n  name=\"<model-name>\",\n  key=\"<key-value>\",\n  tag=\"<tag-value>\"\n)\n\nCopy\nPython\nclient = MlflowClient()\nclient.set_model_version_tag()(\n  name=\"<model-name>\",\n  version=<model-version>,\n  key=\"<key-value>\",\n  tag=\"<tag-value>\"\n)\n\nRename a model (API only)\n\nTo rename a registered model, use the MLflow Client API rename_registered_model() method:\n\nCopy\nPython\nclient=MlflowClient()\nclient.rename_registered_model(\"<model-name>\", \"<new-model-name>\")\n\n\nNote\n\nYou can rename a registered model only if it has no versions, or all versions are in the None or Archived stage.\n\nSearch for a model\n\nYou can search for models in the Workspace Model Registry using the UI or the API.\n\nNote\n\nWhen you search for a model, only models for which you have at least CAN READ permissions are returned.\n\nSearch for a model using the UI\n\nTo display registered models, click  Models in the sidebar.\n\nTo search for a specific model, enter text in the search box. You can enter the name of a model or any part of the name:\n\nYou can also search on tags. Enter tags in this format: tags.<key>=<value>. To search for multiple tags, use the AND operator.\n\nYou can search on both the model name and tags using the MLflow search syntax. For example:\n\nSearch for a model using the API\n\nYou can search for registered models in the Workspace Model Registry with the MLflow Client API method search_registered_models()\n\nIf you have set tags on your models, you can also search by those tags with search_registered_models().\n\nCopy\nPython\nprint(f\"Find registered models with a specific tag value\")\nfor m in client.search_registered_models(f\"tags.`<key-value>`='<tag-value>'\"):\n  pprint(dict(m), indent=4)\n\n\nYou can also search for a specific model name and list its version details using MLflow Client API search_model_versions() method:\n\nCopy\nPython\nfrom pprint import pprint\n\nclient=MlflowClient()\n[pprint(mv) for mv in client.search_model_versions(\"name='<model-name>'\")]\n\n\nThis outputs:\n\nCopy\nConsole\n{   'creation_timestamp': 1582671933246,\n    'current_stage': 'Production',\n    'description': 'A random forest model containing 100 decision trees '\n                   'trained in scikit-learn',\n    'last_updated_timestamp': 1582671960712,\n    'name': 'sk-learn-random-forest-reg-model',\n    'run_id': 'ae2cc01346de45f79a44a320aab1797b',\n    'source': './mlruns/0/ae2cc01346de45f79a44a320aab1797b/artifacts/sklearn-model',\n    'status': 'READY',\n    'status_message': None,\n    'user_id': None,\n    'version': 1 }\n\n{   'creation_timestamp': 1582671960628,\n    'current_stage': 'None',\n    'description': None,\n    'last_updated_timestamp': 1582671960628,\n    'name': 'sk-learn-random-forest-reg-model',\n    'run_id': 'd994f18d09c64c148e62a785052e6723',\n    'source': './mlruns/0/d994f18d09c64c148e62a785052e6723/artifacts/sklearn-model',\n    'status': 'READY',\n    'status_message': None,\n    'user_id': None,\n    'version': 2 }\n\nDelete a model or model version\n\nYou can delete a model using the UI or the API.\n\nDelete a model version or model using the UI\n\nWarning\n\nYou cannot undo this action. You can transition a model version to the Archived stage rather than deleting it from the registry. When you delete a model, all model artifacts stored by the Workspace Model Registry and all the metadata associated with the registered model are deleted.\n\nNote\n\nYou can only delete models and model versions in the None or Archived stage. If a registered model has versions in the Staging or Production stage, you must transition them to either the None or Archived stage before deleting the model.\n\nTo delete a model version:\n\nClick  Models in the sidebar.\n\nClick a model name.\n\nClick a model version.\n\nClick  at the upper right corner of the screen and select Delete from the drop-down menu.\n\nTo delete a model:\n\nClick  Models in the sidebar.\n\nClick a model name.\n\nClick  at the upper right corner of the screen and select Delete from the drop-down menu.\n\nDelete a model version or model using the API\n\nWarning\n\nYou cannot undo this action. You can transition a model version to the Archived stage rather than deleting it from the registry. When you delete a model, all model artifacts stored by the Workspace Model Registry and all the metadata associated with the registered model are deleted.\n\nNote\n\nYou can only delete models and model versions in the None or Archived stage. If a registered model has versions in the Staging or Production stage, you must transition them to either the None or Archived stage before deleting the model.\n\nDelete a model version\n\nTo delete a model version, use the MLflow Client API delete_model_version() method:\n\nCopy\nPython\n# Delete versions 1,2, and 3 of the model\nclient = MlflowClient()\nversions=[1, 2, 3]\nfor version in versions:\n  client.delete_model_version(name=\"<model-name>\", version=version)\n\nDelete a model\n\nTo delete a model, use the MLflow Client API delete_registered_model() method:\n\nCopy\nPython\nclient = MlflowClient()\nclient.delete_registered_model(name=\"<model-name>\")\n\nShare models across workspaces\n\nDatabricks recommends using Models in Unity Catalog to share models across workspaces. Unity Catalog provides out-of-the-box support for cross-workspace model access, governance, and audit logging.\n\nHowever, if using the workspace model registry, you can also share models across multiple workspaces with some setup. For example, you can develop and log a model in your own workspace and then access it from another workspace using a remote Workspace model registry. This is useful when multiple teams share access to models. You can create multiple workspaces and use and manage models across these environments.\n\nCopy MLflow objects between workspaces\n\nTo import or export MLflow objects to or from your Databricks workspace, you can use the community-driven open source project MLflow Export-Import to migrate MLflow experiments, models, and runs between workspaces.\n\nWith these tools, you can:\n\nShare and collaborate with other data scientists in the same or another tracking server. For example, you can clone an experiment from another user into your workspace.\n\nCopy a model from one workspace to another, such as from a development to a production workspace.\n\nCopy MLflow experiments and runs from your local tracking server to your Databricks workspace.\n\nBack up mission critical experiments and models to another Databricks workspace.\n\nExample\n\nThis example illustrates how to use the Workspace Model Registry to build a machine learning application.\n\nWorkspace Model Registry example\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nCreate or register a model\nQuota limits\nView models in the UI\nControl access to models\nTransition a model stage\nUse model for inference\nCompare model versions\nControl notification preferences\nWebhooks\nAnnotate a model or model version\nRename a model (API only)\nSearch for a model\nDelete a model or model version\nShare models across workspaces\nCopy MLflow objects between workspaces\nExample\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Databricks Autologging | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/databricks-autologging.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nTrack ML and deep learning training runs\nDatabricks Autologging\nAccess the MLflow tracking server from outside Databricks\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Track model development using MLflow  Databricks Autologging\nDatabricks Autologging\n\nSeptember 27, 2024\n\nThis page covers how to customize Databricks Autologging, which automatically captures model parameters, metrics, files, and lineage information when you train models from a variety of popular machine learning libraries. Training sessions are recorded as MLflow tracking runs. Model files are also tracked so you can easily log them to the MLflow Model Registry.\n\nNote\n\nTo enable trace logging for generative AI workloads, MLflow supports OpenAI autologging.\n\nThe following video shows Databricks Autologging with a scikit-learn model training session in an interactive Python notebook. Tracking information is automatically captured and displayed in the Experiment Runs sidebar and in the MLflow UI.\n\nRequirements\n\nDatabricks Autologging is generally available in all regions with Databricks Runtime 10.4 LTS ML or above.\n\nDatabricks Autologging is available in select preview regions with Databricks Runtime 9.1 LTS ML or above.\n\nHow it works\n\nWhen you attach an interactive Python notebook to a Databricks cluster, Databricks Autologging calls mlflow.autolog() to set up tracking for your model training sessions. When you train models in the notebook, model training information is automatically tracked with MLflow Tracking. For information about how this model training information is secured and managed, see Security and data management.\n\nThe default configuration for the mlflow.autolog() call is:\n\nCopy\nPython\nmlflow.autolog(\n    log_input_examples=False,\n    log_model_signatures=True,\n    log_models=True,\n    disable=False,\n    exclusive=False,\n    disable_for_unsupported_versions=True,\n    silent=False\n)\n\n\nYou can customize the autologging configuration.\n\nUsage\n\nTo use Databricks Autologging, train a machine learning model in a supported framework using an interactive Databricks Python notebook. Databricks Autologging automatically records model lineage information, parameters, and metrics to MLflow Tracking. You can also customize the behavior of Databricks Autologging.\n\nNote\n\nDatabricks Autologging is not applied to runs created using the MLflow fluent API with mlflow.start_run(). In these cases, you must call mlflow.autolog() to save autologged content to the MLflow run. See Track additional content.\n\nCustomize logging behavior\n\nTo customize logging, use mlflow.autolog(). This function provides configuration parameters to enable model logging (log_models), log datasets (log_datasets), collect input examples (log_input_examples), log model signatures (log_model_signatures), configure warnings (silent), and more.\n\nTrack additional content\n\nTo track additional metrics, parameters, files, and metadata with MLflow runs created by Databricks Autologging, follow these steps in a Databricks interactive Python notebook:\n\nCall mlflow.autolog() with exclusive=False.\n\nStart an MLflow run using mlflow.start_run(). You can wrap this call in with mlflow.start_run(); when you do this, the run is ended automatically after it completes.\n\nUse MLflow Tracking methods, such as mlflow.log_param(), to track pre-training content.\n\nTrain one or more machine learning models in a framework supported by Databricks Autologging.\n\nUse MLflow Tracking methods, such as mlflow.log_metric(), to track post-training content.\n\nIf you did not use with mlflow.start_run() in Step 2, end the MLflow run using mlflow.end_run().\n\nFor example:\n\nCopy\nPython\nimport mlflow\nmlflow.autolog(exclusive=False)\n\nwith mlflow.start_run():\n  mlflow.log_param(\"example_param\", \"example_value\")\n  # <your model training code here>\n  mlflow.log_metric(\"example_metric\", 5)\n\nDisable Databricks Autologging\n\nTo disable Databricks Autologging in a Databricks interactive Python notebook, call mlflow.autolog() with disable=True:\n\nCopy\nPython\nimport mlflow\nmlflow.autolog(disable=True)\n\n\nAdministrators can also disable Databricks Autologging for all clusters in a workspace from the Advanced tab of the admin settings page. Clusters must be restarted for this change to take effect.\n\nSupported environments and frameworks\n\nDatabricks Autologging is supported in interactive Python notebooks and is available for the following ML frameworks:\n\nscikit-learn\n\nApache Spark MLlib\n\nTensorFlow\n\nKeras\n\nPyTorch Lightning\n\nXGBoost\n\nLightGBM\n\nGluon\n\nFast.ai\n\nstatsmodels\n\nPaddlePaddle\n\nOpenAI\n\nLangChain\n\nFor more information about each of the supported frameworks, see MLflow automatic logging.\n\nMLflow Tracing enablement\n\nMLflow Tracing utilizes the autolog feature within respective model framework integrations to control the enabling or disabling of tracing support for integrations that support tracing.\n\nFor example, to enable tracing when using a LlamaIndex model, utilize mlflow.llama_index.autolog() with log_traces=True:\n\nCopy\nPython\nimport mlflow\nmlflow.llama_index.autolog(log_traces=True)\n\n\nThe supported integrations that have trace enablement within their autolog implementations are:\n\nOpenAI\n\nLangChain\n\nLangGraph\n\nLlamaIndex\n\nAutoGen\n\nSecurity and data management\n\nAll model training information tracked with Databricks Autologging is stored in MLflow Tracking and is secured by MLflow Experiment permissions. You can share, modify, or delete model training information using the MLflow Tracking API or UI.\n\nAdministration\n\nAdministrators can enable or disable Databricks Autologging for all interactive notebook sessions across their workspace in the Advanced tab of the admin settings page. Changes do not take effect until the cluster is restarted.\n\nLimitations\n\nDatabricks Autologging is not supported in Databricks jobs. To use autologging from jobs, you can explicitly call mlflow.autolog().\n\nDatabricks Autologging is enabled only on the driver node of your Databricks cluster. To use autologging from worker nodes, you must explicitly call mlflow.autolog() from within the code executing on each worker.\n\nThe XGBoost scikit-learn integration is not supported.\n\nApache Spark MLlib, Hyperopt, and automated MLflow tracking\n\nDatabricks Autologging does not change the behavior of existing automated MLflow tracking integrations for Apache Spark MLlib and Hyperopt.\n\nNote\n\nIn Databricks Runtime 10.1 ML, disabling the automated MLflow tracking integration for Apache Spark MLlib CrossValidator and TrainValidationSplit models also disables the Databricks Autologging feature for all Apache Spark MLlib models.\n\nWas this article helpful?\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nHow it works\nUsage\nSupported environments and frameworks\nMLflow Tracing enablement\nSecurity and data management\nAdministration\nLimitations\nApache Spark MLlib, Hyperopt, and automated MLflow tracking\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Log, load, register, and deploy MLflow models | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/models.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nLog model dependencies\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Log, load, register, and deploy MLflow models\nLog, load, register, and deploy MLflow models\n\nDecember 03, 2024\n\nAn MLflow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools—for example, batch inference on Apache Spark or real-time serving through a REST API. The format defines a convention that lets you save a model in different flavors (python-function, pytorch, sklearn, and so on), that can be understood by different model serving and inference platforms.\n\nTo learn how to log and score a streaming model, see How to save and load a streaming model.\n\nLog and load models\n\nWhen you log a model, MLflow automatically logs requirements.txt and conda.yaml files. You can use these files to recreate the model development environment and reinstall dependencies using virtualenv (recommended) or conda.\n\nImportant\n\nAnaconda Inc. updated their terms of service for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See Anaconda Commercial Edition FAQ for more information. Your use of any Anaconda channels is governed by their terms of service.\n\nMLflow models logged before v1.18 (Databricks Runtime 8.3 ML or earlier) were by default logged with the conda defaults channel (https://repo.anaconda.com/pkgs/) as a dependency. Because of this license change, Databricks has stopped the use of the defaults channel for models logged using MLflow v1.18 and above. The default channel logged is now conda-forge, which points at the community managed https://conda-forge.org/.\n\nIf you logged a model before MLflow v1.18 without excluding the defaults channel from the conda environment for the model, that model may have a dependency on the defaults channel that you may not have intended. To manually confirm whether a model has this dependency, you can examine channel value in the conda.yaml file that is packaged with the logged model. For example, a model’s conda.yaml with a defaults channel dependency may look like this:\n\nCopy\nYAML\nchannels:\n- defaults\ndependencies:\n- python=3.8.8\n- pip\n- pip:\n    - mlflow\n    - scikit-learn==0.23.2\n    - cloudpickle==1.6.0\n      name: mlflow-env\n\n\nBecause Databricks can not determine whether your use of the Anaconda repository to interact with your models is permitted under your relationship with Anaconda, Databricks is not forcing its customers to make any changes. If your use of the Anaconda.com repo through the use of Databricks is permitted under Anaconda’s terms, you do not need to take any action.\n\nIf you would like to change the channel used in a model’s environment, you can re-register the model to the model registry with a new conda.yaml. You can do this by specifying the channel in the conda_env parameter of log_model().\n\nFor more information on the log_model() API, see the MLflow documentation for the model flavor you are working with, for example, log_model for scikit-learn.\n\nFor more information on conda.yaml files, see the MLflow documentation.\n\nAPI commands\n\nTo log a model to the MLflow tracking server, use mlflow.<model-type>.log_model(model, ...).\n\nTo load a previously logged model for inference or further development, use mlflow.<model-type>.load_model(modelpath), where modelpath is one of the following:\n\na run-relative path (such as runs:/{run_id}/{model-path})\n\na Unity Catalog volumes path (such as dbfs:/Volumes/catalog_name/schema_name/volume_name/{path_to_artifact_root}/{model_path})\n\nan MLflow-managed artifact storage path beginning with dbfs:/databricks/mlflow-tracking/\n\na registered model path (such as models:/{model_name}/{model_stage}).\n\nFor a complete list of options for loading MLflow models, see Referencing Artifacts in the MLflow documentation.\n\nFor Python MLflow models, an additional option is to use mlflow.pyfunc.load_model() to load the model as a generic Python function. You can use the following code snippet to load the model and score data points.\n\nCopy\nPython\nmodel = mlflow.pyfunc.load_model(model_path)\nmodel.predict(model_input)\n\n\nAs an alternative, you can export the model as an Apache Spark UDF to use for scoring on a Spark cluster, either as a batch job or as a real-time Spark Streaming job.\n\nCopy\nPython\n# load input data table as a Spark DataFrame\ninput_data = spark.table(input_table_name)\nmodel_udf = mlflow.pyfunc.spark_udf(spark, model_path)\ndf = input_data.withColumn(\"prediction\", model_udf())\n\nLog model dependencies\n\nTo accurately load a model, you should make sure the model dependencies are loaded with the correct versions into the notebook environment. In Databricks Runtime 10.5 ML and above, MLflow warns you if a mismatch is detected between the current environment and the model’s dependencies.\n\nAdditional functionality to simplify restoring model dependencies is included in Databricks Runtime 11.0 ML and above. In Databricks Runtime 11.0 ML and above, for pyfunc flavor models, you can call mlflow.pyfunc.get_model_dependencies to retrieve and download the model dependencies. This function returns a path to the dependencies file which you can then install by using %pip install <file-path>. When you load a model as a PySpark UDF, specify env_manager=\"virtualenv\" in the mlflow.pyfunc.spark_udf call. This restores model dependencies in the context of the PySpark UDF and does not affect the outside environment.\n\nYou can also use this functionality in Databricks Runtime 10.5 or below by manually installing MLflow version 1.25.0 or above:\n\nCopy\nPython\n%pip install \"mlflow>=1.25.0\"\n\n\nFor additional information on how to log model dependencies (Python and non-Python) and artifacts, see Log model dependencies.\n\nLearn how to log model dependencies and custom artifacts for model serving:\n\nDeploy models with dependencies\n\nUse custom Python libraries with Model Serving\n\nPackage custom artifacts for Model Serving\n\nLog model dependencies\nAutomatically generated code snippets in the MLflow UI\n\nWhen you log a model in a Databricks notebook, Databricks automatically generates code snippets that you can copy and use to load and run the model. To view these code snippets:\n\nNavigate to the Runs screen for the run that generated the model. (See View notebook experiment for how to display the Runs screen.)\n\nScroll to the Artifacts section.\n\nClick the name of the logged model. A panel opens to the right showing code you can use to load the logged model and make predictions on Spark or pandas DataFrames.\n\nExamples\n\nFor examples of logging models, see the examples in Track machine learning training runs examples.\n\nRegister models in the Model Registry\n\nYou can register models in the MLflow Model Registry, a centralized model store that provides a UI and set of APIs to manage the full lifecycle of MLflow Models. For instructions on how to use the Model Registry to manage models in Databricks Unity Catalog, see Manage model lifecycle in Unity Catalog. To use the Workspace Model Registry, see Manage model lifecycle using the Workspace Model Registry (legacy).\n\nTo register a model using the API, use mlflow.register_model(\"runs:/{run_id}/{model-path}\", \"{registered-model-name}\").\n\nSave models to Unity Catalog volumes\n\nTo save a model locally, use mlflow.<model-type>.save_model(model, modelpath). modelpath must be a Unity Catalog volumes path. For example, if you use a Unity Catalog volumes location dbfs:/Volumes/catalog_name/schema_name/volume_name/my_project_models to store your project work, you must use the model path /dbfs/Volumes/catalog_name/schema_name/volume_name/my_project_models:\n\nCopy\nPython\nmodelpath = \"/dbfs/Volumes/catalog_name/schema_name/volume_name/my_project_models/model-%f-%f\" % (alpha, l1_ratio)\nmlflow.sklearn.save_model(lr, modelpath)\n\n\nFor MLlib models, use ML Pipelines.\n\nDownload model artifacts\n\nYou can download the logged model artifacts (such as model files, plots, and metrics) for a registered model with various APIs.\n\nPython API example:\n\nCopy\nPython\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n\nmodel_uri = MlflowClient.get_model_version_download_uri(model_name, model_version)\nModelsArtifactRepository(model_uri).download_artifacts(artifact_path=\"\")\n\n\nJava API example:\n\nCopy\nJava\nMlflowClient mlflowClient = new MlflowClient();\n// Get the model URI for a registered model version.\nString modelURI = mlflowClient.getModelVersionDownloadUri(modelName, modelVersion);\n\n// Or download the model artifacts directly.\nFile modelFile = mlflowClient.downloadModelVersion(modelName, modelVersion);\n\n\nCLI command example:\n\nCopy\nmlflow artifacts download --artifact-uri models:/<name>/<version|stage>\n\nDeploy models for online serving\n\nUse Mosaic AI Model Serving to host machine learning models registered in Unity Catalog model registry as REST endpoints. These endpoints are updated automatically based on the availability of model versions.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nLog and load models\nRegister models in the Model Registry\nSave models to Unity Catalog volumes\nDownload model artifacts\nDeploy models for online serving\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Manage model lifecycle in Unity Catalog | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/index.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nUpgrade ML workflows to target models in Unity Catalog\nUpgrade models to Unity Catalog\nManage model lifecycle using the Workspace Model Registry (legacy)\nWorkspace Model Registry example\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Manage model lifecycle in Unity Catalog\nManage model lifecycle in Unity Catalog\n\nDecember 04, 2024\n\nImportant\n\nThis article documents Models in Unity Catalog, which Databricks recommends for governing and deploying models. If your workspace is not enabled for Unity Catalog, the functionality on this page is not available. Instead, see Manage model lifecycle using the Workspace Model Registry (legacy). For guidance on how to upgrade from the Workspace Model Registry to Unity Catalog, see Migrate workflows and models to Unity Catalog.\n\nModels in Unity Catalog isn’t available in AWS GovCloud regions.\n\nThis article describes how to use Models in Unity Catalog as part of your machine learning workflow to manage the full lifecycle of ML models. Databricks provides a hosted version of MLflow Model Registry in Unity Catalog. Models in Unity Catalog extends the benefits of Unity Catalog to ML models, including centralized access control, auditing, lineage, and model discovery across workspaces. Models in Unity Catalog is compatible with the open-source MLflow Python client.\n\nFor an overview of Model Registry concepts, see ML lifecycle management using MLflow.\n\nRequirements\n\nUnity Catalog must be enabled in your workspace. See Get started using Unity Catalog to create a Unity Catalog Metastore, enable it in a workspace, and create a catalog. If Unity Catalog is not enabled, use the workspace model registry.\n\nYou must use a compute resource that has access to Unity Catalog. For ML workloads, this means that the access mode for the compute must be Single user. For more information, see Access modes.\n\nTo create new registered models, you need the following privileges:\n\nUSE SCHEMA and USE CATALOG privileges on the schema and its enclosing catalog.\n\nCREATE_MODEL privilege on the schema. To grant this privilege, use the Catalog Explorer UI or the following SQL GRANT command:\n\nCopy\nSQL\nGRANT CREATE_MODEL ON SCHEMA <schema-name> TO <principal>\n\n\nNote\n\nYour workspace must be attached to a Unity Catalog metastore that supports privilege inheritance. This is true for all metastores created after August 25, 2022. If running on an older metastore, follow docs to upgrade.\n\nInstall and configure MLflow client for Unity Catalog\n\nThis section includes instructions for installing and configuring the MLflow client for Unity Catalog.\n\nInstall MLflow Python client\n\nSupport for models in Unity Catalog is included in Databricks Runtime 13.2 ML and above.\n\nYou can also use models in Unity Catalog on Databricks Runtime 11.3 LTS and above by installing the latest version of the MLflow Python client in your notebook, using the following code.\n\nCopy\n%pip install --upgrade \"mlflow-skinny[databricks]\"\ndbutils.library.restartPython()\n\nConfigure MLflow client to access models in Unity Catalog\n\nIf your workspace’s default catalog is in Unity Catalog (rather than hive_metastore) and you are running a cluster using Databricks Runtime 13.3 LTS or above, models are automatically created in and loaded from the default catalog. You do not have to perform this step.\n\nFor other workspaces, the MLflow Python client creates models in the Databricks workspace model registry. To upgrade to models in Unity Catalog, use the following code in your notebooks to configure the MLflow client:\n\nCopy\nPython\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\n\n\nFor a small number of workspaces where both the default catalog was configured to a catalog in Unity Catalog prior to January 2024 and the workspace model registry was used prior to January 2024, you must manually set the default catalog to Unity Catalog using the command shown above.\n\nTrain and register Unity Catalog-compatible models\n\nPermissions required: To create a new registered model, you need the CREATE_MODEL and USE SCHEMA privileges on the enclosing schema, and USE CATALOG privilege on the enclosing catalog. To create new model versions under a registered model, you must be the owner of the registered model and have USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model.\n\nML model versions in UC must have a model signature. If you’re not already logging MLflow models with signatures in your model training workloads, you can either:\n\nUse Databricks autologging, which automatically logs models with signatures for many popular ML frameworks. See supported frameworks in the MLflow docs.\n\nWith MLflow 2.5.0 and above, you can specify an input example in your mlflow.<flavor>.log_model call, and the model signature is automatically inferred. For further information, refer to the MLflow documentation.\n\nThen, pass the three-level name of the model to MLflow APIs, in the form <catalog>.<schema>.<model>.\n\nThe examples in this section create and access models in the ml_team schema under the prod catalog.\n\nThe model training examples in this section create a new model version and register it in the prod catalog. Using the prod catalog doesn’t necessarily mean that the model version serves production traffic. The model version’s enclosing catalog, schema, and registered model reflect its environment (prod) and associated governance rules (for example, privileges can be set up so that only admins can delete from the prod catalog), but not its deployment status. To manage the deployment status, use model aliases.\n\nRegister a model to Unity Catalog using autologging\n\nTo register a model, use MLflow Client API register_model() method. See mlflow.register_model.\n\nCopy\nPython\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train a sklearn model on the iris dataset\nX, y = datasets.load_iris(return_X_y=True, as_frame=True)\nclf = RandomForestClassifier(max_depth=7)\nclf.fit(X, y)\n\n# Note that the UC model name follows the pattern\n# <catalog_name>.<schema_name>.<model_name>, corresponding to\n# the catalog, schema, and registered model name\n# in Unity Catalog under which to create the version\n# The registered model will be created if it doesn't already exist\nautolog_run = mlflow.last_active_run()\nmodel_uri = \"runs:/{}/model\".format(autolog_run.info.run_id)\nmlflow.register_model(model_uri, \"prod.ml_team.iris_model\")\n\nRegister a model using the API\nCopy\nPython\nmlflow.register_model(\n  \"runs:/<run_uuid>/model\", \"prod.ml_team.iris_model\"\n)\n\nRegister a model to Unity Catalog with automatically inferred signature\n\nSupport for automatically inferred signatures is available in MLflow version 2.5.0 and above, and is supported in Databricks Runtime 11.3 LTS ML and above. To use automatically inferred signatures, use the following code to install the latest MLflow Python client in your notebook:\n\nCopy\n%pip install --upgrade \"mlflow-skinny[databricks]\"\ndbutils.library.restartPython()\n\n\nThe following code shows an example of an automatically inferred signature.\n\nCopy\nPython\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\n\nwith mlflow.start_run():\n    # Train a sklearn model on the iris dataset\n    X, y = datasets.load_iris(return_X_y=True, as_frame=True)\n    clf = RandomForestClassifier(max_depth=7)\n    clf.fit(X, y)\n    # Take the first row of the training dataset as the model input example.\n    input_example = X.iloc[[0]]\n    # Log the model and register it as a new version in UC.\n    mlflow.sklearn.log_model(\n        sk_model=clf,\n        artifact_path=\"model\",\n        # The signature is automatically inferred from the input example and its predicted output.\n        input_example=input_example,\n        registered_model_name=\"prod.ml_team.iris_model\",\n    )\n\nRegister a model using the UI\n\nFollow these steps:\n\nFrom the experiment run page, click Register model in the upper-right corner of the UI.\n\nIn the dialog, select Unity Catalog, and select a destination model from the drop down list.\n\nClick Register.\n\nRegistering a model can take time. To monitor progress, navigate to the destination model in Unity Catalog and refresh periodically.\n\nDeploy models using aliases\n\nModel aliases allow you to assign a mutable, named reference to a particular version of a registered model. You can use aliases to indicate the deployment status of a model version. For example, you could allocate a “Champion” alias to the model version currently in production and target this alias in workloads that use the production model. You can then update the production model by reassigning the “Champion” alias to a different model version.\n\nSet and delete aliases on models\n\nPermissions required: Owner of the registered model, plus USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model.\n\nYou can set, update, and remove aliases for models in Unity Catalog by using Catalog Explorer. You can manage aliases across a registered model in the model details page and configure aliases for a specific model version in the model version details page.\n\nTo set, update, and delete aliases using the MLflow Client API, see the examples below:\n\nCopy\nPython\nfrom mlflow import MlflowClient\nclient = MlflowClient()\n\n# create \"Champion\" alias for version 1 of model \"prod.ml_team.iris_model\"\nclient.set_registered_model_alias(\"prod.ml_team.iris_model\", \"Champion\", 1)\n\n# reassign the \"Champion\" alias to version 2\nclient.set_registered_model_alias(\"prod.ml_team.iris_model\", \"Champion\", 2)\n\n# get a model version by alias\nclient.get_model_version_by_alias(\"prod.ml_team.iris_model\", \"Champion\")\n\n# delete the alias\nclient.delete_registered_model_alias(\"prod.ml_team.iris_model\", \"Champion\")\n\n\nFor more details on alias client APIs, see the MLflow API documentation.\n\nLoad model version by alias for inference workloads\n\nPermissions required: EXECUTE privilege on the registered model, plus USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model.\n\nBatch inference workloads can reference a model version by alias. The snippet below loads and applies the “Champion” model version for batch inference. If the “Champion” version is updated to reference a new model version, the batch inference workload automatically picks it up on its next execution. This allows you to decouple model deployments from your batch inference workloads.\n\nCopy\nPython\nimport mlflow.pyfunc\nmodel_version_uri = \"models:/prod.ml_team.iris_model@Champion\"\nchampion_version = mlflow.pyfunc.load_model(model_version_uri)\nchampion_version.predict(test_x)\n\n\nModel serving endpoints can also reference a model version by alias. You can write deployment workflows to get a model version by alias and update a model serving endpoint to serve that version, using the model serving REST API. For example:\n\nCopy\nPython\nimport mlflow\nimport requests\nclient = mlflow.tracking.MlflowClient()\nchampion_version = client.get_model_version_by_alias(\"prod.ml_team.iris_model\", \"Champion\")\n# Invoke the model serving REST API to update endpoint to serve the current \"Champion\" version\nmodel_name = champion_version.name\nmodel_version = champion_version.version\nrequests.request(...)\n\nLoad model version by version number for inference workloads\n\nYou can also load model versions by version number:\n\nCopy\nPython\nimport mlflow.pyfunc\n# Load version 1 of the model \"prod.ml_team.iris_model\"\nmodel_version_uri = \"models:/prod.ml_team.iris_model/1\"\nfirst_version = mlflow.pyfunc.load_model(model_version_uri)\nfirst_version.predict(test_x)\n\nShare models across workspaces\nShare models with users in the same region\n\nAs long as you have the appropriate privileges, you can access models in Unity Catalog from any workspace that is attached to the metastore containing the model. For example, you can access models from the prod catalog in a dev workspace, to facilitate comparing newly-developed models to the production baseline.\n\nTo collaborate with other users (share write privileges) on a registered model you created, you must grant ownership of the model to a group containing yourself and the users you’d like to collaborate with. Collaborators must also have the USE CATALOG and USE SCHEMA privileges on the catalog and schema containing the model. See Unity Catalog privileges and securable objects for details.\n\nShare models with users in another region or account\n\nTo share models with users in other regions or accounts, use the Delta Sharing Databricks-to-Databricks sharing flow. See Add models to a share (for providers) and Get access in the Databricks-to-Databricks model (for recipients). As a recipient, after you create a catalog from a share, you access models in that shared catalog the same way as any other model in Unity Catalog.\n\nTrack the data lineage of a model in Unity Catalog\n\nNote\n\nSupport for table to model lineage in Unity Catalog is available in MLflow 2.11.0 and above.\n\nWhen you train a model on a table in Unity Catalog, you can track the lineage of the model to the upstream dataset(s) it was trained and evaluated on. To do this, use mlflow.log_input. This saves the input table information with the MLflow run that generated the model. Data lineage is also automatically captured for models logged using feature store APIs. See Feature governance and lineage.\n\nWhen you register the model to Unity Catalog, lineage information is automatically saved and is visible in the Lineage tab of the model version UI in Catalog Explorer.\n\nThe following code shows an example.\n\nCopy\nPython\nimport mlflow\nimport pandas as pd\nimport pyspark.pandas as ps\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Write a table to Unity Catalog\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df.rename(\n  columns = {\n    'sepal length (cm)':'sepal_length',\n    'sepal width (cm)':'sepal_width',\n    'petal length (cm)':'petal_length',\n    'petal width (cm)':'petal_width'},\n  inplace = True\n)\niris_df['species'] = iris.target\nps.from_pandas(iris_df).to_table(\"prod.ml_team.iris\", mode=\"overwrite\")\n\n# Load a Unity Catalog table, train a model, and log the input table\ndataset = mlflow.data.load_delta(table_name=\"prod.ml_team.iris\", version=\"0\")\npd_df = dataset.df.toPandas()\nX = pd_df.drop(\"species\", axis=1)\ny = pd_df[\"species\"]\nwith mlflow.start_run():\n    clf = RandomForestRegressor(n_estimators=100)\n    clf.fit(X, y)\n    mlflow.log_input(dataset, \"training\")\n    # Take the first row of the training dataset as the model input example.\n    input_example = X.iloc[[0]]\n    # Log the model and register it as a new version in UC.\n    mlflow.sklearn.log_model(\n        sk_model=clf,\n        artifact_path=\"model\",\n        # The signature is automatically inferred from the input example and its predicted output.\n        input_example=input_example,\n        registered_model_name=\"prod.ml_team.iris_classifier\",\n    )\n\nControl access to models\n\nIn Unity Catalog, registered models are a subtype of the FUNCTION securable object. To grant access to a model registered in Unity Catalog, you use GRANT ON FUNCTION. For details, see Unity Catalog privileges and securable objects. For best practices on organizing models across catalogs and schemas, see Organize your data.\n\nYou can configure model permissions programmatically using the Grants REST API. When you configure model permissions, set securable_type to \"FUNCTION\" in REST API requests. For example, use PATCH /api/2.1/unity-catalog/permissions/function/{full_name} to update registered model permissions.\n\nView models in the UI\n\nPermissions required: To view a registered model and its model versions in the UI, you need EXECUTE privilege on the registered model, plus USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model\n\nYou can view and manage registered models and model versions in Unity Catalog using the Catalog Explorer.\n\nRename a model\n\nPermissions required: Owner of the registered model, CREATE_MODEL privilege on the schema containing the registered model, and USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model.\n\nTo rename a registered model, use the MLflow Client API rename_registered_model() method:\n\nCopy\nPython\nclient=MlflowClient()\nclient.rename_registered_model(\"<full-model-name>\", \"<new-model-name>\")\n\nCopy a model version\n\nYou can copy a model version from one model to another in Unity Catalog.\n\nCopy a model version using the UI\n\nFollow these steps:\n\nFrom the model version page, click Copy this version in the upper-right corner of the UI.\n\nSelect a destination model from the drop down list and click Copy.\n\nCopying a model can take time. To monitor progress, navigate to the destination model in Unity Catalog and refresh periodically.\n\nCopy a model version using the API\n\nTo copy a model version, use the MLflow’s copy_model_version() Python API:\n\nCopy\nPython\nclient = MlflowClient()\nclient.copy_model_version(\n  \"models:/<source-model-name>/<source-model-version>\",\n  \"<destination-model-name>\",\n)\n\nDelete a model or model version\n\nPermissions required: Owner of the registered model, plus USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model.\n\nYou can delete a registered model or a model version within a registered model using the Catalog Explorer UI or the API.\n\nWarning\n\nYou cannot undo this action. When you delete a model, all model artifacts stored by Unity Catalog and all the metadata associated with the registered model are deleted.\n\nDelete a model version or model using the UI\n\nTo delete a model or model version in Unity Catalog, follow these steps.\n\nOn the model page or model version page, click the kebab menu  in the upper-right corner.\n\nFrom the model page:\n\nFrom the model version page:\n\nSelect Delete.\n\nA confirmation dialog appears. Click Delete to confirm.\n\nDelete a model version or model using the API\n\nTo delete a model version, use the MLflow Client API delete_model_version() method:\n\nCopy\nPython\n# Delete versions 1,2, and 3 of the model\nclient = MlflowClient()\nversions=[1, 2, 3]\nfor version in versions:\n  client.delete_model_version(name=\"<model-name>\", version=version)\n\n\nTo delete a model, use the MLflow Client API delete_registered_model() method:\n\nCopy\nPython\nclient = MlflowClient()\nclient.delete_registered_model(name=\"<model-name>\")\n\nUse tags on models\n\nTags are key-value pairs that you associate with registered models and model versions, allowing you to label and categorize them by function or status. For example, you could apply a tag with key \"task\" and value \"question-answering\" (displayed in the UI as task:question-answering) to registered models intended for question answering tasks. At the model version level, you could tag versions undergoing pre-deployment validation with validation_status:pending and those cleared for deployment with validation_status:approved.\n\nPermissions required: Owner of or have APPLY_TAG privilege on the registered model, plus USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model.\n\nSee Add and update tags using Catalog Explorer on how to set and delete tags using the UI.\n\nTo set and delete tags using the MLflow Client API, see the examples below:\n\nCopy\nPython\nfrom mlflow import MlflowClient\nclient = MlflowClient()\n\n# Set registered model tag\nclient.set_registered_model_tag(\"prod.ml_team.iris_model\", \"task\", \"classification\")\n\n# Delete registered model tag\nclient.delete_registered_model_tag(\"prod.ml_team.iris_model\", \"task\")\n\n# Set model version tag\nclient.set_model_version_tag(\"prod.ml_team.iris_model\", \"1\", \"validation_status\", \"approved\")\n\n# Delete model version tag\nclient.delete_model_version_tag(\"prod.ml_team.iris_model\", \"1\", \"validation_status\")\n\n\nBoth registered model and model version tags must meet the platform-wide constraints.\n\nFor more details on tag client APIs, see the MLflow API documentation.\n\nAdd a description (comments) to a model or model version\n\nPermissions required: Owner of the registered model, plus USE SCHEMA and USE CATALOG privileges on the schema and catalog containing the model.\n\nYou can include a text description for any model or model version in Unity Catalog. For example, you can provide an overview of the problem or information about the methodology and algorithm used.\n\nFor models, you also have the option of using AI-generated comments. See Add AI-generated comments to Unity Catalog objects.\n\nAdd a description to a model using the UI\n\nTo add a description for a model, you can use AI-generated comments, or you can enter your own comments. You can edit AI-generated comments as necessary.\n\nTo add automatically generated comments, click the AI generate button.\n\nTo add your own comments, click Add. Enter your comments in the dialog, and click Save.\n\nAdd a description to a model version using the UI\n\nTo add a description to a model version in Unity Catalog, follow these steps:\n\nOn the model version page, click the pencil icon under Description.\n\nEnter your comments in the dialog, and click Save.\n\nAdd a description to a model or model version using the API\n\nTo update a registered model description, use the MLflow Client API update_registered_model() method:\n\nCopy\nPython\nclient = MlflowClient()\nclient.update_registered_model(\n  name=\"<model-name>\",\n  description=\"<description>\"\n)\n\n\nTo update a model version description, use the MLflow Client API update_model_version() method:\n\nCopy\nPython\nclient = MlflowClient()\nclient.update_model_version(\n  name=\"<model-name>\",\n  version=<model-version>,\n  description=\"<description>\"\n)\n\nList and search models\n\nTo get a list of registered models in Unity Catalog, use MLflow’s search_registered_models() Python API:\n\nCopy\nPython\nclient=MlflowClient()\nclient.search_registered_models()\n\n\nTo search for a specific model name and get information about that model’s versions, use search_model_versions():\n\nCopy\nPython\nfrom pprint import pprint\n\nclient=MlflowClient()\n[pprint(mv) for mv in client.search_model_versions(\"name='<model-name>'\")]\n\n\nNote\n\nNot all search API fields and operators are supported for models in Unity Catalog. See Limitations for details.\n\nDownload model files (advanced use case)\n\nIn most cases, to load models, you should use MLflow APIs like mlflow.pyfunc.load_model or mlflow.<flavor>.load_model (for example, mlflow.transformers.load_model for HuggingFace models).\n\nIn some cases you may need to download model files to debug model behavior or model loading issues. You can download model files using mlflow.artifacts.download_artifacts, as follows:\n\nCopy\nPython\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\nmodel_uri = f\"models:/{model_name}/{version}\" # reference model by version or alias\ndestination_path = \"/local_disk0/model\"\nmlflow.artifacts.download_artifacts(artifact_uri=model_uri, dst_path=destination_path)\n\nPromote a model across environments\n\nDatabricks recommends that you deploy ML pipelines as code. This eliminates the need to promote models across environments, as all production models can be produced through automated training workflows in a production environment.\n\nHowever, in some cases, it may be too expensive to retrain models across environments. Instead, you can copy model versions across registered models in Unity Catalog to promote them across environments.\n\nYou need the following privileges to execute the example code below:\n\nUSE CATALOG on the staging and prod catalogs.\n\nUSE SCHEMA on the staging.ml_team and prod.ml_team schemas.\n\nEXECUTE on staging.ml_team.fraud_detection.\n\nIn addition, you must be the owner of the registered model prod.ml_team.fraud_detection.\n\nThe following code snippet uses the copy_model_version MLflow Client API, available in MLflow version 2.8.0 and above.\n\nCopy\nPython\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\n\nclient = mlflow.tracking.MlflowClient()\nsrc_model_name = \"staging.ml_team.fraud_detection\"\nsrc_model_version = \"1\"\nsrc_model_uri = f\"models:/{src_model_name}/{src_model_version}\"\ndst_model_name = \"prod.ml_team.fraud_detection\"\ncopied_model_version = client.copy_model_version(src_model_uri, dst_model_name)\n\n\nAfter the model version is in the production environment, you can perform any necessary pre-deployment validation. Then, you can mark the model version for deployment using aliases.\n\nCopy\nPython\nclient = mlflow.tracking.MlflowClient()\nclient.set_registered_model_alias(name=\"prod.ml_team.fraud_detection\", alias=\"Champion\", version=copied_model_version.version)\n\n\nIn the example above, only users who can read from the staging.ml_team.fraud_detection registered model and write to the prod.ml_team.fraud_detection registered model can promote staging models to the production environment. The same users can also use aliases to manage which model versions are deployed within the production environment. You don’t need to configure any other rules or policies to govern model promotion and deployment.\n\nYou can customize this flow to promote the model version across multiple environments that match your setup, such as dev, qa, and prod. Access control is enforced as configured in each environment.\n\nExample notebook\n\nThis example notebook illustrates how to use Models in Unity Catalog APIs to manage models in Unity Catalog, including registering models and model versions, adding descriptions, loading and deploying models, using model aliases, and deleting models and model versions.\n\nModels in Unity Catalog example notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nMigrate workflows and models to Unity Catalog\n\nDatabricks recommends using Models in Unity Catalog for improved governance, easy sharing across workspaces and environments, and more flexible MLOps workflows. The table compares the capabilities of the Workspace Model Registry and Unity Catalog.\n\nCapability\n\n\t\n\nWorkspace Model Registry (legacy)\n\n\t\n\nModels in Unity Catalog (recommended)\n\n\n\n\nReference model versions by named aliases\n\n\t\n\nModel Registry Stages: Move model versions into one of four fixed stages to reference them by that stage. Cannot rename or add stages.\n\n\t\n\nModel Registry Aliases: Create up to 10 custom and reassignable named references to model versions for each registered model.\n\n\n\n\nCreate access-controlled environments for models\n\n\t\n\nModel Registry Stages: Use stages within one registered model to denote the environment of its model versions, with access controls for only two of the four fixed stages (Staging and Production).\n\n\t\n\nRegistered Models: Create a registered model for each environment in your MLOps workflow, utilizing three-level namespaces and permissions of Unity Catalog to express governance.\n\n\n\n\nPromote models across environments (deploy model)\n\n\t\n\nUse the transition_model_version_stage() MLflow Client API to move a model version to a different stage, potentially breaking workflows that reference the previous stage.\n\n\t\n\nUse the copy_model_version() MLflow Client API to copy a model version from one registered model to another.\n\n\n\n\nAccess and share models across workspaces\n\n\t\n\nManually export and import models across workspaces, or configure connections to remote model registries using personal access tokens and workspace secret scopes.\n\n\t\n\nOut of the box access to models across workspaces in the same account. No configuration required.\n\n\n\n\nConfigure permissions\n\n\t\n\nSet permissions at the workspace-level.\n\n\t\n\nSet permissions at the account-level, which applies consistent governance across workspaces.\n\n\n\n\nAccess models in the Databricks markplace\n\n\t\n\nUnavailable.\n\n\t\n\nLoad models from the Databricks marketplace into your Unity Catalog metastore and access them across workspaces.\n\nThe articles linked below describe how to migrate workflows (model training and batch inference jobs) and models from the Workspace Model Registry to Unity Catalog.\n\nUpgrade ML workflows to target models in Unity Catalog\n\nUpgrade models to Unity Catalog\n\nLimitations\n\nStages are not supported for models in Unity Catalog. Databricks recommends using the three-level namespace in Unity Catalog to express the environment a model is in, and using aliases to promote models for deployment. See Promote a model across environments for details.\n\nWebhooks are not supported for models in Unity Catalog. See suggested alternatives in the upgrade guide.\n\nSome search API fields and operators are not supported for models in Unity Catalog. This can be mitigated by calling the search APIs using supported filters and scanning the results. Following are some examples:\n\nThe order_by parameter is not supported in the search_model_versions or search_registered_models client APIs.\n\nTag-based filters (tags.mykey = 'myvalue') are not supported for search_model_versions or search_registered_models.\n\nOperators other than exact equality (for example, LIKE, ILIKE, !=) are not supported for search_model_versions or search_registered_models.\n\nSearching registered models by name (for example, MlflowClient().search_registered_models(filter_string=\"name='main.default.mymodel'\") is not supported. To fetch a particular registered model by name, use get_registered_model.\n\nEmail notifications and comment discussion threads on registered models and model versions are not supported in Unity Catalog.\n\nThe activity log is not supported for models in Unity Catalog. To track activity on models in Unity Catalog, use audit logs.\n\nsearch_registered_models might return stale results for models shared through Delta Sharing. To ensure the most recent results, use the Databricks CLI or SDK to list the models in a schema.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nInstall and configure MLflow client for Unity Catalog\nTrain and register Unity Catalog-compatible models\nDeploy models using aliases\nShare models across workspaces\nTrack the data lineage of a model in Unity Catalog\nControl access to models\nView models in the UI\nRename a model\nCopy a model version\nDelete a model or model version\nUse tags on models\nAdd a description (comments) to a model or model version\nList and search models\nDownload model files (advanced use case)\nPromote a model across environments\nExample notebook\nMigrate workflows and models to Unity Catalog\nLimitations\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Get started with MLflow experiments | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/quick-start-python.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Get started with MLflow experiments\nGet started with MLflow experiments\n\nNovember 22, 2024\n\nThis article gives an overview of how to use MLflow in Databricks to automatically log training runs and track parameters, metrics, and models. For more details about using MLflow to track model development, see Track ML and deep learning training runs.\n\nMLflow is an open source platform for managing the end-to-end machine learning lifecycle. MLflow provides simple APIs for logging metrics (for example, model loss), parameters (for example, learning rate), and fitted models, making it easy to analyze training results or deploy models later on.\n\nInstall MLflow\n\nIf you’re using Databricks Runtime for Machine Learning, MLflow is already installed. Otherwise, install the MLflow package from PyPI.\n\nAutomatically log training runs to MLflow\n\nWith Databricks Runtime 10.4 LTS ML and above, Databricks Autologging is enabled by default and automatically captures model parameters, metrics, files, and lineage information when you train models from a variety of popular machine learning libraries.\n\nWith Databricks Runtime 9.1 LTS ML, MLflow provides mlflow.<framework>.autolog() APIs to automatically log training code written in many ML frameworks. You can call this API before running training code to log model-specific metrics, parameters, and model artifacts.\n\nTensorFlow\nXGBoost\nLightGBM\nScikit-learn\nPySpark\n\nNote\n\nKeras models are also supported in mlflow.tensorflow.autolog().\n\nCopy\n# Also autoinstruments tf.keras\nimport mlflow.tensorflow\nmlflow.tensorflow.autolog()\n\nView results\n\nAfter executing your machine learning code, you can view results using the Experiment Runs sidebar. See View notebook experiment for instructions on how to view the experiment, run, and notebook revision used in the quickstart.\n\nTrack additional metrics, parameters, and models\n\nYou can log additional information by directly invoking the MLflow Tracking logging APIs.\n\nNumerical metrics\nCopy\nPython\n  import mlflow\n  mlflow.log_metric(\"accuracy\", 0.9)\n\nTraining parameters\nCopy\nPython\n  import mlflow\n  mlflow.log_param(\"learning_rate\", 0.001)\n\nModels\nScikit-learn\nPySpark\nXGBoost\nTensorFlow\nKeras\nPyTorch\nSpaCy\nCopy\nimport mlflow.sklearn\nmlflow.sklearn.log_model(model, \"myModel\")\n\nOther artifacts (files)\nCopy\nPython\n   import mlflow\n   mlflow.log_artifact(\"/tmp/my-file\", \"myArtifactPath\")\n\nExample notebooks\n\nNote\n\nWith Databricks Runtime 10.4 LTS ML and above, Databricks Autologging is enabled by default, and the code in these example notebooks is not required. The example notebooks in this section are designed for use with Databricks Runtime 9.1 LTS ML.\n\nThe recommended way to get started using MLflow tracking with Python is to use the MLflow autolog() API. With MLflow’s autologging capabilities, a single line of code automatically logs the resulting model, the parameters used to create the model, and a model score. The following notebook shows you how to set up a run using autologging.\n\nMLflow autologging quickstart Python notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\n\nIf you need more control over the metrics logged for each training run, or want to log additional artifacts such as tables or plots, you can use the MLflow logging API functions demonstrated in the following notebook.\n\nMLflow logging API quickstart Python notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLearn more\n\nMLflow overview\n\nTrack ML and deep learning training runs\n\nLog, load, register, and deploy MLflow models\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nInstall MLflow\nAutomatically log training runs to MLflow\nView results\nTrack additional metrics, parameters, and models\nExample notebooks\nLearn more\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "MLOps Stacks: model development process as code | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/mlops/mlops-stacks.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nHow does Databricks support CI/CD for machine learning?\nModel deployment patterns\nMLOps Stacks: model development process as code\nLLMOps\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  MLOps workflows on Databricks  MLOps Stacks: model development process as code\nMLOps Stacks: model development process as code\n\nOctober 30, 2024\n\nThis article describes how MLOps Stacks lets you implement the development and deployment process as code in a source-controlled repository. It also describes the benefits of model development on the Databricks Data Intelligence platform, a single platform that unifies every step of the model development and deployment process.\n\nWhat is MLOps Stacks?\n\nWith MLOps Stacks, the entire model development process is implemented, saved, and tracked as code in a source-controlled repository. Automating the process in this way facilitates more repeatable, predictable, and systematic deployments and makes it possible to integrate with your CI/CD process. Representing the model development process as code enables you to deploy the code instead of deploying the model. Deploying the code automates the ability to build the model, making it much easier to retrain the model when necessary.\n\nWhen you create a project using MLOps Stacks, you define the components of your ML development and deployment process such as notebooks to use for feature engineering, training, testing, and deployment, pipelines for training and testing, workspaces to use for each stage, and CI/CD workflows using GitHub Actions or Azure DevOps for automated testing and deployment of your code.\n\nThe environment created by MLOps Stacks implements the MLOps workflow recommended by Databricks. You can customize the code to create stacks to match your organization’s processes or requirements.\n\nHow does MLOps Stacks work?\n\nYou use the Databricks CLI to create an MLOps Stack. For step-by-step instructions, see Databricks Asset Bundles for MLOps Stacks.\n\nWhen you initiate an MLOps Stacks project, the software steps you through entering the configuration details and then creates a directory containing the files that compose your project. This directory, or stack, implements the production MLOps workflow recommended by Databricks. The components shown in the diagram are created for you, and you need only edit the files to add your custom code.\n\nIn the diagram:\n\nA: A data scientist or ML engineer initializes the project using databricks bundle init mlops-stacks. When you initialize the project, you can choose to set up the ML code components (typically used by data scientists), the CI/CD components (typically used by ML engineers), or both.\n\nB: ML engineers set up Databricks service principal secrets for CI/CD.\n\nC: Data scientists develop models on Databricks or on their local system.\n\nD: Data scientists create pull requests to update ML code.\n\nE: The CI/CD runner runs notebooks, creates jobs, and performs other tasks in the staging and production workspaces.\n\nYour organization can use the default stack, or customize it as needed to add, remove, or revise components to fit your organization’s practices. See the GitHub repository readme for details.\n\nMLOps Stacks is designed with a modular structure to allow the different ML teams to work independently on a project while following software engineering best practices and maintaining production-grade CI/CD. Production engineers configure ML infrastructure that allows data scientists to develop, test, and deploy ML pipelines and models to production.\n\nAs shown in the diagram, the default MLOps Stack includes the following three components:\n\nML code. MLOps Stacks creates a set of templates for an ML project including notebooks for training, batch inference, and so on. The standardized template allows data scientists to get started quickly, unifies project structure across teams, and enforces modularized code ready for testing.\n\nML resources as code. MLOps Stacks defines resources such as workspaces and pipelines for tasks like training and batch inference. Resources are defined in Databricks Asset Bundles to facilitate testing, optimization, and version control for the ML environment. For example, you can try a larger instance type for automated model retraining, and the change is automatically tracked for future reference.\n\nCI/CD. You can use GitHub Actions or Azure DevOps to test and deploy ML code and resources, ensuring that all production changes are performed through automation and that only tested code is deployed to prod.\n\nMLOps project flow\n\nA default MLOps Stacks project includes an ML pipeline with CI/CD workflows to test and deploy automated model training and batch inference jobs across development, staging, and production Databricks workspaces. MLOps Stacks is configurable, so you can modify the project structure to meet your organization’s processes.\n\nThe diagram shows the process that is implemented by the default MLOps Stack. In the development workspace, data scientists iterate on ML code and file pull requests (PRs). PRs trigger unit tests and integration tests in an isolated staging Databricks workspace. When a PR is merged to main, model training and batch inference jobs that run in staging immediately update to run the latest code. After you merge a PR into main, you can cut a new release branch as part of your scheduled release process and deploy the code changes to production.\n\nMLOps Stacks project structure\n\nAn MLOps Stack uses Databricks Asset Bundles – a collection of source files that serves as the end-to-end definition of a project. These source files include information about how they are to be tested and deployed. Collecting the files as a bundle makes it easy to co-version changes and use software engineering best practices such as source control, code review, testing, and CI/CD.\n\nThe diagram shows the files created for the default MLOps Stack. For details about the files included in the stack, see the documentation on the GitHub repository or Databricks Asset Bundles for MLOps Stacks.\n\nMLOps Stacks components\n\nA “stack” refers to the set of tools used in a development process. The default MLOps Stack takes advantage of the unified Databricks platform and uses the following tools:\n\nComponent\n\n\t\n\nTool in Databricks\n\n\n\n\nML model development code\n\n\t\n\nDatabricks notebooks, MLflow\n\n\n\n\nFeature development and management\n\n\t\n\nFeature engineering\n\n\n\n\nML model repository\n\n\t\n\nModels in Unity Catalog\n\n\n\n\nML model serving\n\n\t\n\nMosaic AI Model Serving\n\n\n\n\nInfrastructure-as-code\n\n\t\n\nDatabricks Asset Bundles\n\n\n\n\nOrchestrator\n\n\t\n\nDatabricks Jobs\n\n\n\n\nCI/CD\n\n\t\n\nGitHub Actions, Azure DevOps\n\n\n\n\nData and model performance monitoring\n\n\t\n\nLakehouse monitoring\n\nNext steps\n\nTo get started, see Databricks Asset Bundles for MLOps Stacks or the Databricks MLOps Stacks repository on GitHub.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat is MLOps Stacks?\nHow does MLOps Stacks work?\nMLOps project flow\nMLOps Stacks project structure\nMLOps Stacks components\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "MLOps workflows on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/mlops/mlops-workflow.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nHow does Databricks support CI/CD for machine learning?\nModel deployment patterns\nMLOps Stacks: model development process as code\nLLMOps\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  MLOps workflows on Databricks\nMLOps workflows on Databricks\n\nNovember 15, 2024\n\nThis article describes how you can use MLOps on the Databricks platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks platform that you can use as a model for your ML development-to-production process. For modifications of this workflow for LLMOps applications, see LLMOps workflows.\n\nFor more details, see The Big Book of MLOps.\n\nWhat is MLOps?\n\nMLOps is a set of processes and automated steps for managing code, data, and models to improve performance, stability, and long-term efficiency of ML systems. It combines DevOps, DataOps, and ModelOps.\n\nML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.\n\nGeneral recommendations for MLOps\n\nThis section includes some general recommendations for MLOps on Databricks with links for more information.\n\nCreate a separate environment for each stage\n\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n\nDevelopment\n\nStaging\n\nProduction\n\nOther configurations can also be used to meet the specific needs of your organization.\n\nAccess control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:\n\nUse Git for version control. Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use Databricks Git folders to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see Developer tools.\n\nStore data in a lakehouse architecture using Delta tables. Data should be stored in a lakehouse architecture in your cloud account. Both raw data and feature tables should be stored as Delta tables with access controls to determine who can read and modify them.\n\nManage model development with MLflow. You can use MLflow to track the model development process and save code snapshots, model parameters, metrics, and other metadata.\n\nUse Models in Unity Catalog to manage the model lifecycle. Use Models in Unity Catalog to manage model versioning, governance, and deployment status.\n\nDeploy code, not models\n\nIn most situations, Databricks recommends that during the ML development process, you promote code, rather than models, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see Model deployment patterns.\n\nRecommended MLOps workflow\n\nThe following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.\n\nThis section uses the terms “data scientist” and “ML engineer” as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.\n\nDevelopment stage\n\nThe focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n1. Data sources\n\nThe development environment is represented by the dev catalog in Unity Catalog. Data scientists have read-write access to the dev catalog as they create temporary data and feature tables in the development workspace. Models created in the development stage are registered to the dev catalog.\n\nIdeally, data scientists working in the development workspace also have read-only access to production data in the prod catalog. Allowing data scientists read access to production data, inference tables, and metric tables in the prod catalog enables them to analyze current production model predictions and performance. Data scientists should also be able to load production models for experimentation and analysis.\n\nIf it is not possible to grant read-only access to the prod catalog, a snapshot of production data can be written to the dev catalog to enable data scientists to develop and evaluate project code.\n\n2. Exploratory data analysis (EDA)\n\nData scientists explore and analyze data in an interactive, iterative process using notebooks. The goal is to assess whether the available data has the potential to solve the business problem. In this step, the data scientist begins identifying data preparation and featurization steps for model training. This ad hoc process is generally not part of a pipeline that will be deployed in other execution environments.\n\nAutoML accelerates this process by generating baseline models for a dataset. AutoML performs and records a set of trials and provides a Python notebook with the source code for each trial run, so you can review, reproduce, and modify the code. AutoML also calculates summary statistics on your dataset and saves this information in a notebook that you can review.\n\n3. Code\n\nThe code repository contains all of the pipelines, modules, and other project files for an ML project. Data scientists create new or updated pipelines in a development (“dev”) branch of the project repository. Starting from EDA and the initial phases of a project, data scientists should work in a repository to share code and track changes.\n\n4. Train model (development)\n\nData scientists develop the model training pipeline in the development environment using tables from the dev or prod catalogs.\n\nThis pipeline includes 2 tasks:\n\nTraining and tuning. The training process logs model parameters, metrics, and artifacts to the MLflow Tracking server. After training and tuning hyperparameters, the final model artifact is logged to the tracking server to record a link between the model, the input data it was trained on, and the code used to generate it.\n\nEvaluation. Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow Tracking server. The purpose of evaluation is to determine if the newly developed model performs better than the current production model. Given sufficient permissions, any production model registered to the prod catalog can be loaded into the development workspace and compared against a newly trained model.\n\nIf your organization’s governance requirements include additional information about the model, you can save it using MLflow tracking. Typical artifacts are plain text descriptions and model interpretations such as plots produced by SHAP. Specific governance requirements may come from a data governance officer or business stakeholders.\n\nThe output of the model training pipeline is an ML model artifact stored in the MLflow Tracking server for the development environment. If the pipeline is executed in the staging or production workspace, the model artifact is stored in the MLflow Tracking server for that workspace.\n\nWhen the model training is complete, register the model to Unity Catalog. Set up your pipeline code to register the model to the catalog corresponding to the environment that the model pipeline was executed in; in this example, the dev catalog.\n\nWith the recommended architecture, you deploy a multitask Databricks workflow in which the first task is the model training pipeline, followed by model validation and model deployment tasks. The model training task yields a model URI that the model validation task can use. You can use task values to pass this URI to the model.\n\n5. Validate and deploy model (development)\n\nIn addition to the model training pipeline, other pipelines such as model validation and model deployment pipelines are developed in the development environment.\n\nModel validation. The model validation pipeline takes the model URI from the model training pipeline, loads the model from Unity Catalog, and runs validation checks.\n\nValidation checks depend on the context. They can include fundamental checks such as confirming format and required metadata, and more complex checks that might be required for highly regulated industries, such as predefined compliance checks and confirming model performance on selected data slices.\n\nThe primary function of the model validation pipeline is to determine whether a model should proceed to the deployment step. If the model passes pre-deployment checks, it can be assigned the “Challenger” alias in Unity Catalog. If the checks fail, the process ends. You can configure your workflow to notify users of a validation failure. See Add email and system notifications for job events.\n\nModel deployment. The model deployment pipeline typically either directly promotes the newly trained “Challenger” model to “Champion” status using an alias update, or facilitates a comparison between the existing “Champion” model and the new “Challenger” model. This pipeline can also set up any required inference infrastructure, such as Model Serving endpoints. For a detailed discussion of the steps involved in the model deployment pipeline, see Production.\n\n6. Commit code\n\nAfter developing code for training, validation, deployment and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\nStaging stage\n\nThe focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.\n\nML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.\n\n1. Data\n\nThe staging environment should have its own catalog in Unity Catalog for testing ML pipelines and registering models to Unity Catalog. This catalog is shown as the “staging” catalog in the diagram. Assets written to this catalog are generally temporary and only retained until testing is complete. The development environment may also require access to the staging catalog for debugging purposes.\n\n2. Merge code\n\nData scientists develop the model training pipeline in the development environment using tables from the development or production catalogs.\n\nPull request. The deployment process begins when a pull request is created against the main branch of the project in source control.\n\nUnit tests (CI). The pull request automatically builds source code and triggers unit tests. If unit tests fail, the pull request is rejected.\n\nUnit tests are part of the software development process and are continuously executed and added to the codebase during the development of any code. Running unit tests as part of a CI pipeline ensures that changes made in a development branch do not break existing functionality.\n\n3. Integration tests (CI)\n\nThe CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.\n\nIf you are deploying an ML application with real-time inference, you should create and test serving infrastructure in the staging environment. This involves triggering the model deployment pipeline, which creates a serving endpoint in the staging environment and loads a model.\n\nTo reduce the time required to run integration tests, some steps can trade off between fidelity of testing and speed or cost. For example, if models are expensive or time-consuming to train, you might use small subsets of data or run fewer training iterations. For model serving, depending on production requirements, you might do full-scale load testing in integration tests, or you might just test small batch jobs or requests to a temporary endpoint.\n\n4. Merge to staging branch\n\nIf all tests pass, the new code is merged into the main branch of the project. If tests fail, the CI/CD system should notify users and post results on the pull request.\n\nYou can schedule periodic integration tests on the main branch. This is a good idea if the branch is updated frequently with concurrent pull requests from multiple users.\n\n5. Create a release branch\n\nAfter CI tests have passed and the dev branch is merged into the main branch, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\nProduction stage\n\nML engineers own the production environment where ML pipelines are deployed and executed. These pipelines trigger model training, validate and deploy new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, production pipeline status, and monitoring tables. This visibility allows them to identify and diagnose problems in production and to compare the performance of new models to models currently in production. You can grant data scientists read-only access to assets in the production catalog for these purposes.\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n1. Train model\n\nThis pipeline can be triggered by code changes or by automated retraining jobs. In this step, tables from the production catalog are used for the following steps.\n\nTraining and tuning. During the training process, logs are recorded to the production environment MLflow Tracking server. These logs include model metrics, parameters, tags, and the model itself. If you use feature tables, the model is logged to MLflow using the Databricks Feature Store client, which packages the model with feature lookup information that is used at inference time.\n\nDuring development, data scientists may test many algorithms and hyperparameters. In the production training code, it’s common to consider only the top-performing options. Limiting tuning in this way saves time and can reduce the variance from tuning in automated retraining.\n\nIf data scientists have read-only access to the production catalog, they may be able to determine the optimal set of hyperparameters for a model. In this case, the model training pipeline deployed in production can be executed using the selected set of hyperparameters, typically included in the pipeline as a configuration file.\n\nEvaluation. Model quality is evaluated by testing on held-out production data. The results of these tests are logged to the MLflow tracking server. This step uses the evaluation metrics specified by data scientists in the development stage. These metrics may include custom code.\n\nRegister model. When model training is complete, the model artifact is saved as a registered model version at the specified model path in the production catalog in Unity Catalog. The model training task yields a model URI that the model validation task can use. You can use task values to pass this URI to the model.\n\n2. Validate model\n\nThis pipeline uses the model URI from Step 1 and loads the model from Unity Catalog. It then executes a series of validation checks. These checks depend on your organization and use case, and can include things like basic format and metadata validations, performance evaluations on selected data slices, and compliance with organizational requirements such as compliance checks for tags or documentation.\n\nIf the model successfully passes all validation checks, you can assign the “Challenger” alias to the model version in Unity Catalog. If the model does not pass all validation checks, the process exits and users can be automatically notified. You can use tags to add key-value attributes depending on the outcome of these validation checks. For example, you could create a tag “model_validation_status” and set the value to “PENDING” as the tests execute, and then update it to “PASSED” or “FAILED” when the pipeline is complete.\n\nBecause the model is registered to Unity Catalog, data scientists working in the development environment can load this model version from the production catalog to investigate if the model fails validation. Regardless of the outcome, results are recorded to the registered model in the production catalog using annotations to the model version.\n\n3. Deploy model\n\nLike the validation pipeline, the model deployment pipeline depends on your organization and use case. This section assumes that you have assigned the newly validated model the “Challenger” alias, and that the existing production model has been assigned the “Champion” alias. The first step before deploying the new model is to confirm that it performs at least as well as the current production model.\n\nCompare “CHALLENGER” to “CHAMPION” model. You can perform this comparison offline or online. An offline comparison evaluates both models against a held-out data set and tracks results using the MLflow Tracking server. For real-time model serving, you might want to perform longer running online comparisons, such as A/B tests or a gradual rollout of the new model. If the “Challenger” model version performs better in the comparison, it replaces the current “Champion” alias.\n\nMosaic AI Model Serving and Databricks Lakehouse Monitoring allow you to automatically collect and monitor inference tables that contain request and response data for an endpoint.\n\nIf there is no existing “Champion” model, you might compare the “Challenger” model to a business heuristic or other threshold as a baseline.\n\nThe process described here is fully automated. If manual approval steps are required, you can set those up using workflow notifications or CI/CD callbacks from the model deployment pipeline.\n\nDeploy model. Batch or streaming inference pipelines can be set up to use the model with the “Champion” alias. For real-time use cases, you must set up the infrastructure to deploy the model as a REST API endpoint. You can create and manage this endpoint using Mosaic AI Model Serving. If an endpoint is already in use for the current model, you can update the endpoint with the new model. Mosaic AI Model Serving executes a zero-downtime update by keeping the existing configuration running until the new one is ready.\n\n4. Model Serving\n\nWhen configuring a Model Serving endpoint, you specify the name of the model in Unity Catalog and the version to serve. If the model version was trained using features from tables in Unity Catalog, the model stores the dependencies for the features and functions. Model Serving automatically uses this dependency graph to look up features from appropriate online stores at inference time. This approach can also be used to apply functions for data preprocessing or to compute on-demand features during model scoring.\n\nYou can create a single endpoint with multiple models and specify the endpoint traffic split between those models, allowing you to conduct online “Champion” versus “Challenger” comparisons.\n\n5. Inference: batch or streaming\n\nThe inference pipeline reads the latest data from the production catalog, executes functions to compute on-demand features, loads the “Champion” model, scores the data, and returns predictions. Batch or streaming inference is generally the most cost-effective option for higher throughput, higher latency use cases. For scenarios where low-latency predictions are required, but predictions can be computed offline, these batch predictions can be published to an online key-value store such as DynamoDB or Cosmos DB.\n\nThe registered model in Unity Catalog is referenced by its alias. The inference pipeline is configured to load and apply the “Champion” model version. If the “Champion” version is updated to a new model version, the inference pipeline automatically uses the new version for its next execution. In this way the model deployment step is decoupled from inference pipelines.\n\nBatch jobs typically publish predictions to tables in the production catalog, to flat files, or over a JDBC connection. Streaming jobs typically publish predictions either to Unity Catalog tables or to message queues like Apache Kafka.\n\n6. Lakehouse Monitoring\n\nLakehouse Monitoring monitors statistical properties, such as data drift and model performance, of input data and model predictions. You can create alerts based on these metrics or publish them in dashboards.\n\nData ingestion. This pipeline reads in logs from batch, streaming, or online inference.\n\nCheck accuracy and data drift. The pipeline computes metrics about the input data, the model’s predictions, and the infrastructure performance. Data scientists specify data and model metrics during development, and ML engineers specify infrastructure metrics. You can also define custom metrics with Lakehouse Monitoring.\n\nPublish metrics and set up alerts. The pipeline writes to tables in the production catalog for analysis and reporting. You should configure these tables to be readable from the development environment so data scientists have access for analysis. You can use Databricks SQL to create monitoring dashboards to track model performance, and set up the monitoring job or the dashboard tool to issue a notification when a metric exceeds a specified threshold.\n\nTrigger model retraining. When monitoring metrics indicate performance issues or changes in the input data, the data scientist may need to develop a new model version. You can set up SQL alerts to notify data scientists when this happens.\n\n7. Retraining\n\nThis architecture supports automatic retraining using the same model training pipeline above. Databricks recommends beginning with scheduled, periodic retraining and moving to triggered retraining when needed.\n\nScheduled. If new data is available on a regular basis, you can create a scheduled job to run the model training code on the latest available data. See Trigger types for Databricks Jobs\n\nTriggered. If the monitoring pipeline can identify model performance issues and send alerts, it can also trigger retraining. For example, if the distribution of incoming data changes significantly or if the model performance degrades, automatic retraining and redeployment can boost model performance with minimal human intervention. This can be achieved through a SQL alert to check whether a metric is anomalous (for example, check drift or model quality against a threshold). The alert can be configured to use a webhook destination, which can subsequently trigger the training workflow.\n\nIf the retraining pipeline or other pipelines exhibit performance issues, the data scientist may need to return to the development environment for additional experimentation to address the issues.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat is MLOps?\nGeneral recommendations for MLOps\nRecommended MLOps workflow\nDevelopment stage\nStaging stage\nProduction stage\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Track ML and deep learning training runs | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/tracking.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nTrack ML and deep learning training runs\nOrganize training runs with MLflow experiments\nManage training code with MLflow runs\nBuild dashboards with the MLflow Search API\nDatabricks Autologging\nAccess the MLflow tracking server from outside Databricks\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow  Track model development using MLflow  Track ML and deep learning training runs\nTrack ML and deep learning training runs\n\nNovember 26, 2024\n\nThe MLflow tracking component lets you log source properties, parameters, metrics, tags, and artifacts related to training a machine learning or deep learning model. For an example notebook to get started with MLflow, see Tutorial: End-to-end ML models on Databricks.\n\nMLflow tracking with experiments and runs\n\nMLflow tracking is based on two concepts, experiments and runs:\n\nNote\n\nStarting March 27, 2024, MLflow imposes a quota limit on the number of total parameters, tags, and metric steps for all existing and new runs, and the number of total runs for all existing and new experiments, see Resource limits. If you hit the runs per experiment quota, Databricks recommends you delete runs that you no longer need using the delete runs API in Python. If you hit other quota limits, Databricks recommends adjusting your logging strategy to keep under the limit. If you require an increase to this limit, reach out to your Databricks account team with a brief explanation of your use case, why the suggested mitigation approaches do not work, and the new limit you request.\n\nAn MLflow experiment is the primary unit of organization and access control for MLflow runs; all MLflow runs belong to an experiment. Experiments let you visualize, search for, and compare runs, as well as download run artifacts and metadata for analysis in other tools.\n\nAn MLflow run corresponds to a single execution of model code.\n\nOrganize training runs with MLflow experiments\nManage training code with MLflow runs\n\nThe MLflow Tracking API logs parameters, metrics, tags, and artifacts from a model run. The Tracking API communicates with an MLflow tracking server. When you use Databricks, a Databricks-hosted tracking server logs the data. The hosted MLflow tracking server has Python, Java, and R APIs.\n\nNote\n\nMLflow is installed on Databricks Runtime ML clusters. To use MLflow on a Databricks Runtime cluster, you must install the mlflow library. For instructions on installing a library onto a cluster, see Install a library on a cluster. The specific packages to install for MLflow are:\n\nFor Python, select Library Source PyPI and enter mlflow in the Package field.\n\nFor R, select Library Source CRAN and enter mlflow in the Package field.\n\nFor Scala, install these two packages:\n\nSelect Library Source Maven and enter org.mlflow:mlflow-client:1.11.0 in the Coordinates field.\n\nSelect Library Source PyPI and enter mlflow in the Package field.\n\nWhere MLflow runs are logged\n\nAll MLflow runs are logged to the active experiment, which can be set using any of the following ways:\n\nUse the mlflow.set_experiment() command.\n\nUse the experiment_id parameter in the mlflow.start_run() command.\n\nSet one of the MLflow environment variables MLFLOW_EXPERIMENT_NAME or MLFLOW_EXPERIMENT_ID.\n\nIf no active experiment is set, runs are logged to the notebook experiment.\n\nTo log your experiment results to a remotely hosted MLflow Tracking server in a workspace other than the one in which you are running your experiment, set the tracking URI to reference the remote workspace with mlflow.set_tracking_uri(), and set the path to your experiment in the remote workspace by using mlflow.set_experiment().\n\nCopy\nPython\nmlflow.set_tracking_uri(<uri-of-remote-workspace>)\nmlflow.set_experiment(\"path to experiment in remote workspace\")\n\n\nIf you are running experiments locally and want to log experiment results to the Databricks MLflow Tracking server, provide your Databricks workspace instance (DATABRICKS_HOST) and Databricks personal access token (DATABRICKS_TOKEN). Next, you can set the tracking URI to reference the workspace with mlflow.set_tracking_uri(), and set the path to your experiment by using mlflow.set_experiment(). See Perform Databricks personal access token authentication for details on where to find values for the DATABRICKS_HOST and DATABRICKS_TOKEN environment variables.\n\nThe following code example demonstrates setting these values:\n\nCopy\nPython\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://dbc-1234567890123456.cloud.databricks.com\" # set to your server URI\nos.environ[\"DATABRICKS_TOKEN\"] = \"dapixxxxxxxxxxxxx\"\n\nmlflow.set_tracking_uri(\"databricks\")\nmlflow.set_experiment(\"/your-experiment\")\n\nLogging example notebook\n\nThis notebook shows how to log runs to a notebook experiment and to a workspace experiment. Only MLflow runs initiated within a notebook can be logged to the notebook experiment. MLflow runs launched from any notebook or from the APIs can be logged to a workspace experiment. For information about viewing logged runs, see View notebook experiment and View workspace experiment.\n\nLog MLflow runs notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nAccess the MLflow tracking server from outside Databricks\n\nYou can also write to and read from the tracking server from outside Databricks, for example using the MLflow CLI. See Access the MLflow tracking server from outside Databricks.\n\nAnalyze MLflow runs programmatically\n\nYou can access MLflow run data programmatically using the following two DataFrame APIs:\n\nThe MLflow Python client search_runs API returns a pandas DataFrame.\n\nThe MLflow experiment data source returns an Apache Spark DataFrame.\n\nThis example demonstrates how to use the MLflow Python client to build a dashboard that visualizes changes in evaluation metrics over time, tracks the number of runs started by a specific user, and measures the total number of runs across all users:\n\nBuild dashboards with the MLflow Search API\nWhy model training metrics and outputs may vary\n\nMany of the algorithms used in ML have a random element, such as sampling or random initial conditions within the algorithm itself. When you train a model using one of these algorithms, the results might not be the same with each run, even if you start the run with the same conditions. Many libraries offer a seeding mechanism to fix the initial conditions for these stochastic elements. However, there may be other sources of variation that are not controlled by seeds. Some algorithms are sensitive to the order of the data, and distributed ML algorithms may also be affected by how the data is partitioned. Generally this variation is not significant and not important in the model development process.\n\nTo control variation caused by differences in ordering and partitioning, use the PySpark functions repartition and sortWithinPartitions.\n\nMLflow tracking examples\n\nThe following notebooks demonstrate how to train several types of models and track the training data in MLflow and how to store tracking data in Delta Lake.\n\nTrack scikit-learn model training with MLflow\n\nTrack Keras model training with MLflow\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nMLflow tracking with experiments and runs\nWhere MLflow runs are logged\nLogging example notebook\nAccess the MLflow tracking server from outside Databricks\nAnalyze MLflow runs programmatically\nWhy model training metrics and outputs may vary\nMLflow tracking examples\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "ML lifecycle management using MLflow | Databricks on AWS",
    "url": "https://docs.databricks.com/en/mlflow/index.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nMLOps workflows on Databricks\nGet started with MLflow experiments\nMLflow experiment tracking\nLog, load, register, and deploy MLflow models\nManage model lifecycle\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  ML lifecycle management using MLflow\nML lifecycle management using MLflow\n\nDecember 04, 2024\n\nThis article describes how MLflow is used in Databricks for machine learning lifecycle management. It also includes examples that introduce each MLflow component and links to content that describe how these components are hosted within Databricks.\n\nML lifecycle management in Databricks is provided by managed MLflow. Databricks provides a fully managed and hosted version of MLflow integrated with enterprise security features, high availability, and other Databricks workspace features such as experiment and run management and notebook revision capture.\n\nFirst-time users should begin with Get started with MLflow experiments, which demonstrates the basic MLflow tracking APIs.\n\nWhat is MLflow?\n\nMLflow is an open source platform for managing the end-to-end machine learning lifecycle. It has the following primary components:\n\nTracking: Allows you to track experiments to record and compare parameters and results.\n\nModels: Allow you to manage and deploy models from a variety of ML libraries to a variety of model serving and inference platforms.\n\nProjects: Allow you to package ML code in a reusable, reproducible form to share with other data scientists or transfer to production.\n\nModel Registry: Allows you to centralize a model store for managing models’ full lifecycle stage transitions: from staging to production, with capabilities for versioning and annotating. Databricks provides a managed version of the Model Registry in Unity Catalog.\n\nModel Serving: Allows you to host MLflow models as REST endpoints. Databricks provides a unified interface to deploy, govern, and query your served AI models.\n\nMLflow supports Java, Python, R, and REST APIs.\n\nNote\n\nIf you’re just getting started with Databricks, consider using MLflow on Databricks Community Edition, which provides a simple managed MLflow experience for lightweight experimentation. Remote execution of MLflow projects is not supported on Databricks Community Edition. We plan to impose moderate limits on the number of experiments and runs. For the initial launch of MLflow on Databricks Community Edition no limits are imposed.\n\nMLflow tracking\n\nMLflow on Databricks offers an integrated experience for tracking and securing training runs for machine learning and deep learning models.\n\nTrack model development using MLflow\n\nDatabricks Autologging\n\nModel lifecycle management\n\nMLflow Model Registry is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Databricks provides a hosted version of the MLflow Model Registry in Unity Catalog. Unity Catalog provides centralized model governance, cross-workspace access, lineage, and deployment. For details about managing the model lifecycle in Unity Catalog, see Manage model lifecycle in Unity Catalog.\n\nIf your workspace is not enabled for Unity Catalog, you can use the Workspace Model Registry.\n\nModel Registry concepts\n\nModel: An MLflow Model logged from an experiment or run that is logged with one of the model flavor’s mlflow.<model-flavor>.log_model methods. After a model is logged, you can register it with the Model Registry.\n\nRegistered model: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\nModel version: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\nModel alias: An alias is a mutable, named reference to a particular version of a registered model. Typical uses of aliases are to specify which model versions are deployed in a given environment in your model training workflows or to write inference workloads that target a specific alias. For example, you could assign the “Champion” alias of your “Fraud Detection” registered model to the model version that should serve the majority of production traffic, and then write inference workloads that target that alias (that is, make predictions using the “Champion” version).\n\nModel stage (workspace model registry only): A model version can be assigned one or more stages. MLflow provides predefined stages for the common use cases: None, Staging, Production, and Archived. With the appropriate permission you can transition a model version between stages or you can request a model stage transition. Model version stages are not used in Unity Catalog.\n\nDescription: You can annotate a model’s intent, including a description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\nExample notebooks\n\nFor an example notebook that illustrates how to use the Model Registry to manage the model lifecycle, see the following:\n\nExample notebook\n\nWorkspace Model Registry example\n\nModel deployment\n\nMosaic AI Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nModel serving supports serving:\n\nCustom models. These are Python models packaged in the MLflow format. They can be registered either in Unity Catalog or in the workspace model registry. Examples include scikit-learn, XGBoost, PyTorch, and Hugging Face transformer models.\n\nState-of-the-art open models made available by Foundation Model APIs. These models are curated foundation model architectures that support optimized inference. Base models, like Meta-Llama-3.1-70B-Instruct, BGE-Large, and Mistral-7B are available for immediate use with pay-per-token pricing, and workloads that require performance guarantees and fine-tuned model variants can be deployed with provisioned throughput.\n\nExternal models. These are models that are hosted outside of Databricks. Examples include generative AI models like, OpenAI’s GPT-4, Anthropic’s Claude, and others. Endpoints that serve external models can be centrally governed and customers can establish rate limits and access controls for them.\n\nYou also can deploy MLflow models for offline inference, see Deploy models for batch inference.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nWhat is MLflow?\nMLflow tracking\nModel lifecycle management\nModel deployment\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "AI and machine learning on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/index.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 06, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks\nAI and machine learning on Databricks\n\nNovember 15, 2024\n\nThis article describes the tools that Mosaic AI (formerly Databricks Machine Learning) provides to help you build AI and ML systems. The diagram shows how various products on Databricks platform help you implement your end to end workflows to build and deploy AI and ML systems\n\nGenerative AI on Databricks\n\nMosaic AI unifies the AI lifecycle from data collection and preparation, to model development and LLMOps, to serving and monitoring. The following features are specifically optimized to facilitate the development of generative AI applications:\n\nUnity Catalog for governance, discovery, versioning, and access control for data, features, models, and functions.\n\nMLflow for model development tracking.\n\nMosaic AI Model Serving for deploying LLMs. You can configure a model serving endpoint specifically for accessing generative AI models:\n\nState-of-the-art open LLMs using Foundation Model APIs.\n\nThird-party models hosted outside of Databricks. See External models in Mosaic AI Model Serving.\n\nMosaic AI Vector Search provides a queryable vector database that stores embedding vectors and can be configured to automatically sync to your knowledge base.\n\nLakehouse Monitoring for data monitoring and tracking model prediction quality and drift using automatic payload logging with inference tables.\n\nAI Playground for testing generative AI models from your Databricks workspace. You can prompt, compare and adjust settings such as system prompt and inference parameters.\n\nFoundation Model Fine-tuning (now part of Mosaic AI Model Training) for customizing a foundation model using your own data to optimize its performance for your specific application.\n\nMosaic AI Agent Framework for building and deploying production-quality agents like Retrieval Augmented Generation (RAG) applications.\n\nMosaic AI Agent Evaluation for evaluating the quality, cost, and latency of generative AI applications, including RAG applications and chains.\n\nWhat is generative AI?\n\nGenerative AI is a type of artificial intelligence focused on the ability of computers to use models to create content like images, text, code, and synthetic data.\n\nGenerative AI applications are built on top of generative AI models: large language models (LLMs) and foundation models.\n\nLLMs are deep learning models that consume and train on massive datasets to excel in language processing tasks. They create new combinations of text that mimic natural language based on their training data.\n\nGenerative AI models or foundation models are large ML models pre-trained with the intention that they are to be fine-tuned for more specific language understanding and generation tasks. These models are used to discern patterns within the input data.\n\nAfter these models have completed their learning processes, together they generate statistically probable outputs when prompted and they can be employed to accomplish various tasks, including:\n\nImage generation based on existing ones or utilizing the style of one image to modify or create a new one.\n\nSpeech tasks such as transcription, translation, question/answer generation, and interpretation of the intent or meaning of text.\n\nImportant\n\nWhile many LLMs or other generative AI models have safeguards, they can still generate harmful or inaccurate information.\n\nGenerative AI has the following design patterns:\n\nPrompt Engineering: Crafting specialized prompts to guide LLM behavior\n\nRetrieval Augmented Generation (RAG): Combining an LLM with external knowledge retrieval\n\nFine-tuning: Adapting a pre-trained LLM to specific data sets of domains\n\nPre-training: Training an LLM from scratch\n\nMachine learning on Databricks\n\nWith Mosaic AI, a single platform serves every step of ML development and deployment, from raw data to inference tables that save every request and response for a served model. Data scientists, data engineers, ML engineers and DevOps can do their jobs using the same set of tools and a single source of truth for the data.\n\nMosaic AI unifies the data layer and ML platform. All data assets and artifacts, such as models and functions, are discoverable and governed in a single catalog. Using a single platform for data and models makes it possible to track lineage from the raw data to the production model. Built-in data and model monitoring saves quality metrics to tables that are also stored in the platform, making it easier to identify the root cause of model performance problems. For more information about how Databricks supports the full ML lifecycle and MLOps, see MLOps workflows on Databricks and MLOps Stacks: model development process as code.\n\nSome of the key components of the data intelligence platform are:\n\nTasks\n\n\t\n\nComponent\n\n\n\n\nGovern and manage data, features, models, and functions. Also discovery, versioning, and lineage.\n\n\t\n\nUnity Catalog\n\n\n\n\nTrack changes to data, data quality, and model prediction quality\n\n\t\n\nLakehouse Monitoring, Inference tables\n\n\n\n\nFeature development and management\n\n\t\n\nFeature engineering and serving.\n\n\n\n\nTrain models\n\n\t\n\nAutoML, Databricks notebooks\n\n\n\n\nTrack model development\n\n\t\n\nMLflow tracking\n\n\n\n\nServe custom models\n\n\t\n\nMosaic AI Model Serving.\n\n\n\n\nBuild automated workflows and production-ready ETL pipelines\n\n\t\n\nDatabricks Jobs\n\n\n\n\nGit integration\n\n\t\n\nDatabricks Git folders\n\nDeep learning on Databricks\n\nConfiguring infrastructure for deep learning applications can be difficult. Databricks Runtime for Machine Learning takes care of that for you, with clusters that have built-in compatible versions of the most common deep learning libraries like TensorFlow, PyTorch, and Keras.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. It also supports libraries like Ray to parallelize compute processing for scaling ML workflows and ML applications.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. Mosaic AI Model Serving enables creation of scalable GPU endpoints for deep learning models with no extra configuration.\n\nFor machine learning applications, Databricks recommends using a cluster running Databricks Runtime for Machine Learning. See Create a cluster using Databricks Runtime ML.\n\nTo get started with deep learning on Databricks, see:\n\nBest practices for deep learning on Databricks\n\nDeep learning on Databricks\n\nReference solutions for deep learning\n\nNext steps\n\nTo get started, see:\n\nTutorials: Get started with AI and machine learning\n\nFor a recommended MLOps workflow on Databricks Mosaic AI, see:\n\nMLOps workflows on Databricks\n\nTo learn about key Databricks Mosaic AI features, see:\n\nWhat is AutoML?\n\nFeature engineering and serving\n\nModel serving with Databricks\n\nLakehouse Monitoring\n\nManage model lifecycle\n\nMLflow experiment tracking\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nGenerative AI on Databricks\nMachine learning on Databricks\nDeep learning on Databricks\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  }
]