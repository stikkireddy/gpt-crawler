[
  {
    "title": "Unstructured retrieval AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/unstructured-retrieval-tools.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCode interpreters\nStructured retrieval\nUnstructured retrieval\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create AI agent tools  Unstructured retrieval AI agent tools\nUnstructured retrieval AI agent tools\n\nDecember 16, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to create AI agent tools for unstructured data retrieval using the Mosaic AI Agent Framework. Unstructured retrievers enable agents to query unstructured data sources, such as a document corpus, using vector search indexes.\n\nTo learn more about agent tools, see Create AI agent tools.\n\nVector Search retriever tool with Unity Catalog functions\n\nThe following example creates a Unity Catalog function for a retriever tool that can query data from a Mosaic AI Vector Search index.\n\nThe Unity Catalog function databricks_docs_vector_search queries a hypothetical Vector Search index containing Databricks documentation. It wraps the Databricks SQL function vector_search() and uses the aliases page_content and metadata to match its output to the MLflow retriever schema.\n\nNote\n\nTo conform to the MLflow retriever schema, any additional metadata columns must be added to the metadata column using the SQL map function, rather than as top-level output keys.\n\nRun the following code in a notebook or SQL editor.\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.databricks_docs_vector_search (\n  -- The agent uses this comment to determine how to generate the query string parameter.\n  query STRING\n  COMMENT 'The query string for searching Databricks documentation.'\n) RETURNS TABLE\n-- The agent uses this comment to determine when to call this tool. It describes the types of documents and information contained within the index.\nCOMMENT 'Executes a search on Databricks documentation to retrieve text documents most relevant to the input query.' RETURN\nSELECT\n  chunked_text as page_content,\n  map('doc_uri', url, 'chunk_id', chunk_id) as metadata\nFROM\n  vector_search(\n    -- Specify your Vector Search index name here\n    index => 'catalog.schema.databricks_docs_index',\n    query => query,\n    num_results => 5\n  )\n\n\nThis retriever tool has the following caveats:\n\nMLflow traces this Unity Catalog function as a TOOL span type rather than a RETRIEVER span type. As a result, downstream Agent Framework applications like the agent review app and AI Playground will not show retriever-specific details such as links to chunks. For more information on span types, see MLflow Tracing Schema.\n\nSQL clients may limit the maximum number of rows or bytes returned. To prevent data truncation, you should truncate column values returned by the UDF. For example, you could use substring(chunked_text, 0, 8192) to reduce the size of large content columns and avoid row truncation during execution.\n\nSince this tool is a wrapper for the vector_search() function, it is subject to the same limitations as the vector_search() function. See Limitations.\n\nIf this example is unsuitable for your use case, create a vector search retriever tool using custom agent code instead.\n\nVector Search retriever with agent code (PyFunc)\n\nThe following example creates a Vector Search retriever for a PyFunc-flavored agent in agent code.\n\nThis example uses databricks-vectorsearch to create a basic retriever that performs a Vector Search similarity search with filters. It uses MLflow decorators to enable agent tracing.\n\nNote\n\nTo conform to the MLflow retriever schema, the retriever function should return a Document type and use the metadata field in the Document class to add additional attributes to the returned document, like like doc_uri and similarity_score.\n\nUse the following code in the agent module or agent notebook.\n\nCopy\nPython\nimport mlflow\nimport json\n\nfrom mlflow.entities import Document\nfrom typing import List, Dict, Any\nfrom dataclasses import asdict\nfrom databricks.vector_search.client import VectorSearchClient\n\nclass VectorSearchRetriever:\n    \"\"\"\n    Class using Databricks Vector Search to retrieve relevant documents.\n    \"\"\"\n    def __init__(self):\n        self.vector_search_client = VectorSearchClient(disable_notice=True)\n        # TODO: Replace this with the list of column names to return in the result when querying Vector Search\n        self.columns = [\"chunk_id\", \"text_column\", \"doc_uri\"]\n        self.vector_search_index = self.vector_search_client.get_index(\n            index_name=\"catalog.schema.chunked_docs_index\"\n        )\n        mlflow.models.set_retriever_schema(\n            name=\"vector_search\",\n            primary_key=\"chunk_id\",\n            text_column=\"text_column\",\n            doc_uri=\"doc_uri\"\n        )\n\n    @mlflow.trace(span_type=\"RETRIEVER\", name=\"vector_search\")\n    def __call__(\n        self,\n        query: str,\n        filters: Dict[Any, Any] = None,\n        score_threshold = None\n    ) -> List[Document]:\n        \"\"\"\n        Performs vector search to retrieve relevant chunks.\n        Args:\n            query: Search query.\n            filters: Optional filters to apply to the search. Filters must follow the Databricks Vector Search filter spec\n            score_threshold: Score threshold to use for the query.\n\n        Returns:\n            List of retrieved Documents.\n        \"\"\"\n\n        results = self.vector_search_index.similarity_search(\n            query_text=query,\n            columns=self.columns,\n            filters=filters,\n            num_results=5,\n            query_type=\"ann\"\n        )\n\n        documents = self.convert_vector_search_to_documents(\n            results, score_threshold\n        )\n        return [asdict(doc) for doc in documents]\n\n    @mlflow.trace(span_type=\"PARSER\")\n    def convert_vector_search_to_documents(\n        self, vs_results, score_threshold\n    ) -> List[Document]:\n\n        docs = []\n        column_names = [column[\"name\"] for column in vs_results.get(\"manifest\", {}).get(\"columns\", [])]\n        result_row_count = vs_results.get(\"result\", {}).get(\"row_count\", 0)\n\n        if result_row_count > 0:\n            for item in vs_results[\"result\"][\"data_array\"]:\n                metadata = {}\n                score = item[-1]\n\n                if score >= score_threshold:\n                    metadata[\"similarity_score\"] = score\n                    for i, field in enumerate(item[:-1]):\n                        metadata[column_names[i]] = field\n\n                    page_content = metadata.pop(\"text_column\", None)\n\n                    if page_content:\n                        doc = Document(\n                            page_content=page_content,\n                            metadata=metadata\n                        )\n                        docs.append(doc)\n\n        return docs\n\n\nTo run the retriever, run the following Python code. You can optionally include Vector Search filters in the request to filter results.\n\nCopy\nPython\nretriever = VectorSearchRetriever()\nquery = \"What is Databricks?\"\nfilters={\"text_column LIKE\": \"Databricks\"},\nresults = retriever(query, filters=filters, score_threshold=0.1)\n\nSet retriever schema\n\nTo ensure that retrievers are traced properly and render correctly in downstream applications, call mlflow.models.set_retriever_schema when you define your agent. Use set_retriever_schema to map the column names in the returned table to MLflow’s expected fields such as primary_key, text_column, and doc_uri.\n\nCopy\nPython\n# Define the retriever's schema by providing your column names\nmlflow.models.set_retriever_schema(\n    name=\"vector_search\",\n    primary_key=\"chunk_id\",\n    text_column=\"text_column\",\n    doc_uri=\"doc_uri\"\n    # other_columns=[\"column1\", \"column2\"],\n)\n\n\nYou can also specify additional columns in your retriever’s schema by providing a list of column names with the other_columns field.\n\nIf you have multiple retrievers, you can define multiple schemas by using unique names for each retriever schema.\n\nThe retriever schema set during agent creation affects downstream applications and workflows, such as the review app and evaluation sets. Specifically, the doc_uri column serves as the primary identifier for documents returned by the retriever.\n\nThe review app displays the doc_uri to help reviewers assess responses and trace document origins. See Review App UI.\n\nEvaluation sets use doc_uri to compare retriever results against predefined evaluation datasets to determine the retriever’s recall and precision. See Evaluation sets.\n\nTrace the retriever\n\nMLflow tracing adds observability by capturing detailed information about your agent’s execution. It provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to pinpoint the source of bugs and unexpected behaviors quickly.\n\nThis example uses the @mlflow.trace decorator to create a trace for the retriever and parser. For other options for setting up trace methods, see MLflow Tracing for agents.\n\nThe decorator creates a span that starts when the function is invoked and ends when it returns. MLflow automatically records the function’s input and output and any exceptions raised.\n\nNote\n\nLangChain, LlamaIndex, and OpenAI library users can use MLflow auto logging instead of manually defining traces with the decorator. See Use autologging to add traces to your agents.\n\nCopy\nPython\n...\n@mlflow.trace(span_type=\"RETRIEVER\", name=\"vector_search\")\ndef __call__(self, query: str) -> List[Document]:\n  ...\n\n\nTo ensure downstream applications such as Agent Evaluation and the AI Playground render the retriever trace correctly, make sure the decorator meets the following requirements:\n\nUse span_type=\"RETRIEVER\" and ensure the function returns List[Document] object. See Retriever spans.\n\nThe trace name and the retriever_schema name must match to configure the trace correctly.\n\nFilter Vector Search results\n\nYou can limit the search scope to a subset of data using a Vector Search filter.\n\nThe filters parameter in VectorSearchRetriever defines the filter conditions using the Databricks Vector Search filter specification.\n\nCopy\nPython\nfilters = {\"text_column LIKE\": \"Databricks\"}\n\n\nInside the __call__ method, the filters dictionary is passed directly to the similarity_search function:\n\nCopy\nPython\nresults = self.vector_search_index.similarity_search(\n    query_text=query,\n    columns=self.columns,\n    filters=filters,\n    num_results=5,\n    query_type=\"ann\"\n)\n\n\nAfter initial filtering, the score_threshold parameter provides additional filtering by setting a minimum similarity score.\n\nCopy\nPython\nif score >= score_threshold:\n    metadata[\"similarity_score\"] = score\n\n\nThe final result includes documents that meet the filters and score_threshold conditions.\n\nNext steps\n\nAfter you create a Unity Catalog function agent tool, add the tool to an AI agent. See Add Unity Catalog tools to agents.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nVector Search retriever tool with Unity Catalog functions\nVector Search retriever with agent code (PyFunc)\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Structured retrieval AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/structured-retrieval-tools.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCode interpreters\nStructured retrieval\nUnstructured retrieval\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create AI agent tools  Structured retrieval AI agent tools\nStructured retrieval AI agent tools\n\nDecember 13, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to create AI agents for structured data retrieval using the Mosaic AI Agent Framework. Structured retrievers enable agents to query structured data sources such as SQL tables.\n\nTo learn more about agent tools, see Create AI agent tools.\n\nTable query tool\n\nThe following example creates a tool that allows an agent to query structured customer data from a Unity Catalog table.\n\nIt defines a UC function called lookup_customer_info, which allows an AI agent to retrieve structured data from a hypothetical customer_data table.\n\nRun the following code in a SQL editor.\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.lookup_customer_info(\n  customer_name STRING COMMENT 'Name of the customer whose info to look up'\n)\nRETURNS STRING\nCOMMENT 'Returns metadata about a particular customer given the customer name, including the customer's email and ID. The\ncustomer ID can be used for other queries.'\nRETURN SELECT CONCAT(\n    'Customer ID: ', customer_id, ', ',\n    'Customer Email: ', customer_email\n  )\n  FROM main.default.customer_data\n  WHERE customer_name = customer_name\n  LIMIT 1;\n\nNext steps\n\nAfter you create an agent tool, add the tool to an AI agent. See Add Unity Catalog tools to agents.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nTable query tool\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Code interpreter AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/code-interpreter-tools.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCode interpreters\nStructured retrieval\nUnstructured retrieval\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create AI agent tools  Code interpreter AI agent tools\nCode interpreter AI agent tools\n\nDecember 19, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to create code interpreter tools for AI agents using the Mosaic AI Agent Framework. Code interpreters enable agents to execute arbitrary code provided by an interacting user, retrieved from a code base, or written by the agent.\n\nTo learn more about agent tools, see Create AI agent tools.\n\nBuilt-in Python executor tool\n\nDatabricks provides a built-in Python executor tool that lets AI agents run Python code. The Unity Catalog function system.ai.python_exe is available by default and can be used like any other Unity Catalog function-based tool.\n\nPython executor tool\n\nThe following example re-creates the built-in tool, system.ai.python_exe that allows an agent to execute Python code.\n\nRun the following code in a notebook cell. It uses the %sql notebook magic to create a Unity Catalog function called python_exec.\n\nCopy\nSQL\n%sql\nCREATE OR REPLACE FUNCTION main.default.python_exec (\n        code STRING COMMENT \"Python code to execute. Ensure that all variables are initialized within the code, and import any necessary standard libraries. The code must print the final result to stdout. Do not attempt to access files or external systems.\"\n    )\n    RETURNS STRING\n    LANGUAGE PYTHON\n    COMMENT \"Executes Python code in a stateless sandboxed environment and returns its stdout. The runtime cannot access files or read previous executions' output. All operations must be self-contained, using only standard Python libraries. Calls to other tools are prohibited.\"\n    AS $$\n        import sys\n        from io import StringIO\n        stdout = StringIO()\n        stderr = StringIO()\n        sys.stdout = stdout\n        sys.stderr = stderr\n        try:\n            exec(code, {})\n        except SyntaxError as e: # try escaping characters\n            code = code.encode('utf-8').decode('unicode_escape')\n            exec(code, {})\n        return stdout.getvalue() + stderr.getvalue()\n    $$\n\nNext steps\n\nAfter you create an agent tool, add the tool to an AI agent. See Add Unity Catalog tools to agents.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nBuilt-in Python executor tool\nPython executor tool\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Define an agent’s input and output schema | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/agent-schema.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Define an agent’s input and output schema\nDefine an agent’s input and output schema\n\nDecember 16, 2024\n\nAI agents must adhere to specific input and output schema requirements to be compatible with other features on Databricks. This article explains how to ensure your AI agent adheres to those requirements and how to customize your agent’s input and output schema while ensuring compatibility.\n\nMosaic AI uses MLflow Model Signatures to define an agent’s input and output schema requirements. The model signature tells internal and external components how to interact with your agent and validates that they adhere to the schema.\n\nOpenAI chat completion schema (recommended)\n\nDatabricks recommends using the OpenAI chat completion schema to define agent input and output. This schema is widely adopted and compatible with many agent frameworks and applications, including those in Databricks.\n\nSee OpenAI documentation for more information about the chat completion input schema and output schema.\n\nNote\n\nThe OpenAI chat completion schema is simply a standard for structuring agent inputs and outputs. Implementing this schema does not involve using OpenAI’s resources or models.\n\nMLflow provides convenient APIs for LangChain and PyFunc-flavored agents, helping you create agents compatible with the chat completion schema.\n\nImplement chat completion with LangChain\n\nIf your agent uses LangChain, use MLflow’s ChatCompletionOutputParser() to format your agent’s final output to be compatible with the chat completion schema. If you use LangGraph, see LangGraph custom schemas.\n\nCopy\nPython\n\n  from mlflow.langchain.output_parsers import ChatCompletionOutputParser\n\n  chain = (\n      {\n          \"user_query\": itemgetter(\"messages\")\n          | RunnableLambda(extract_user_query_string),\n          \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_chat_history),\n      }\n      | RunnableLambda(DatabricksChat)\n      | ChatCompletionOutputParser()\n  )\n\nImplement chat completion with PyFunc\n\nIf you use PyFunc, Databricks recommends writing your agent as a subclass of mlflow.pyfunc.ChatModel. This method provides the following benefits:\n\nAllows you to write agent code compatible with the chat completion schema using typed Python classes.\n\nWhen logging the agent, MLflow will automatically infer a chat completion-compatible signature, even without an input_example. This simplifies the process of registering and deploying the agent. See Infer Model Signature during logging.\n\nCopy\nPython\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, List, Generator\nfrom mlflow.pyfunc import ChatModel\nfrom mlflow.types.llm import (\n    # Non-streaming helper classes\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ChatCompletionChunk,\n    ChatMessage,\n    ChatChoice,\n    ChatParams,\n    # Helper classes for streaming agent output\n    ChatChoiceDelta,\n    ChatChunkChoice,\n)\n\nclass MyAgent(ChatModel):\n    \"\"\"\n    Defines a custom agent that processes ChatCompletionRequests\n    and returns ChatCompletionResponses.\n    \"\"\"\n    def predict(self, context, messages: list[ChatMessage], params: ChatParams) -> ChatCompletionResponse:\n        last_user_question_text = messages[-1].content\n        response_message = ChatMessage(\n            role=\"assistant\",\n            content=(\n                f\"I will always echo back your last question. Your last question was: {last_user_question_text}. \"\n            )\n        )\n        return ChatCompletionResponse(\n            choices=[ChatChoice(message=response_message)]\n        )\n\n    def _create_chat_completion_chunk(self, content) -> ChatCompletionChunk:\n        \"\"\"Helper for constructing a ChatCompletionChunk instance for wrapping streaming agent output\"\"\"\n        return ChatCompletionChunk(\n                choices=[ChatChunkChoice(\n                    delta=ChatChoiceDelta(\n                        role=\"assistant\",\n                        content=content\n                    )\n                )]\n            )\n\n    def predict_stream(\n        self, context, messages: List[ChatMessage], params: ChatParams\n    ) -> Generator[ChatCompletionChunk, None, None]:\n        last_user_question_text = messages[-1].content\n        yield self._create_chat_completion_chunk(f\"Echoing back your last question, word by word.\")\n        for word in last_user_question_text.split(\" \"):\n            yield self._create_chat_completion_chunk(word)\n\nagent = MyAgent()\nmodel_input = ChatCompletionRequest(\n    messages=[ChatMessage(role=\"user\", content=\"What is Databricks?\")]\n)\nresponse = agent.predict(context=None, model_input=model_input)\nprint(response)\n\nCustom inputs and outputs\n\nDatabricks recommends adhering to the OpenAI chat completions schema for most agent use cases.\n\nHowever, some scenarios may require additional inputs, such as client_type and session_id, or outputs like retrieval source links that should not be included in the chat history for future interactions.\n\nFor these scenarios, Mosaic AI Agent Framework supports augmenting OpenAI chat completion requests and responses with custom inputs and outputs.\n\nSee the following examples to learn how to create custom inputs and outputs for PyFunc and LangGraph agents.\n\nWarning\n\nThe Agent Evaluation review app does not currently support rendering traces for agents with additional input fields.\n\nPyFunc custom schemas\n\nThe following notebooks show a custom schema example using PyFunc.\n\nPyFunc custom schema agent notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nPyFunc custom schema driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLangGraph custom schemas\n\nThe following notebooks show a custom schema example using LangGraph. You can modify the wrap_output function in the notebooks to parse and extract information from the message stream.\n\nLangGraph custom schema agent notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLangGraph custom schema driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nProvide custom_inputs in the AI Playground and agent review app\n\nIf your agent accepts additional inputs using the custom_inputs field, you can manually provide these inputs in both the AI Playground and the agent review app.\n\nIn either the AI Playground or the Agent Review App, select the gear icon .\n\nEnable custom_inputs.\n\nProvide a JSON object that matches your agent’s defined input schema.\n\nLegacy input and output schemas\n\nThe SplitChatMessageRequest input schema and StringResponse output schema have been deprecated. If you are using either of these legacy schemas, Databricks recommends that you migrate to the recommended chat completion schema.\n\nSplitChatMessageRequest input schema (deprecated)\n\nSplitChatMessagesRequest allows you to pass the current query and history separately as agent input.\n\nCopy\nPython\n  question = {\n      \"query\": \"What is MLflow\",\n      \"history\": [\n          {\n              \"role\": \"user\",\n              \"content\": \"What is Retrieval-augmented Generation?\"\n          },\n          {\n              \"role\": \"assistant\",\n              \"content\": \"RAG is\"\n          }\n      ]\n  }\n\nStringResponse output schema (deprecated)\n\nStringResponse allows you to return the agent’s response as an object with a single string content field:\n\nCopy\n{\"content\": \"This is an example string response\"}\n\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nOpenAI chat completion schema (recommended)\nCustom inputs and outputs\nPyFunc custom schemas\nLangGraph custom schemas\nProvide custom_inputs in the AI Playground and agent review app\nLegacy input and output schemas\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "What are compound AI systems and AI agents? | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/ai-agents.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  What are compound AI systems and AI agents?\nWhat are compound AI systems and AI agents?\n\nNovember 30, 2024\n\nMosaic AI Agent Framework helps developers overcome the unique challenges of developing AI agents and compound AI systems. Learn what makes an AI application a compound AI system and an AI agent.\n\nCompound AI systems\n\nCompound AI systems are systems that tackle AI tasks by combining multiple interacting components. In contrast, an AI model is simply a statistical model, e.g., a Transformer that predicts the next token in text. Compound AI systems are an increasingly common design pattern for AI applications due to their performance and flexibility.\n\nFor more information, see The Shift from Models to Compound AI Systems .\n\nWhat are AI agents?\n\nThe industry is still defining AI agents, however it generally understood as an AI system where the model makes some or all of the planning decisions in contrast to hard-coded logic. These agents use large language models (LLMs) to make decisions and accomplish their objectives.\n\nMany AI agents applications are made of multiple systems, thus qualifying them as compound AI systems.\n\nAgency is a continuum, the more freedom we provide models to control the behavior of the system, the more agent-like the application becomes.\n\nWhat are tools?\n\nAI agents use tools to perform actions besides language generation, for example to retrieve structured or unstructured data, run code, or talk to remote services like sending an email or Slack message.\n\nOn Databricks, you can use Unity Catalog functions as tools, enabling easy discovery, governance, and sharing of tools. You can also define tools using open source agent authoring libraries like LangChain.\n\nIn typical agentic workflows, the agent LLM is given metadata about tools, which it uses to determine when and how to use the tool. So when defining tools, you must ensure that the tool, its parameters, and its return value are well-documented, so that the agent LLM can best use the tool.\n\nFrom LLMs to AI agents\n\nTo understand AI agents, it’s helpful to consider the evolution of AI systems.\n\nLLMs: Initially, large language models simply responded to prompts based on knowledge from a vast training dataset.\n\nLLMs + tool chains: Then, developers added hardcoded tools to expand the LLM’s capabilities. For example, retrieval augmented generation (RAG) expanded an LLM’s knowledge base with custom documentation sets, while API tools allowed LLMs to perform tasks like create support tickets or send emails.\n\nAI agents: Now, AI agents autonomously create plans and execute tasks based on their understanding of the problem. AI agents still use tools but it’s up to them to decide which tool to use and when. The key distinction is in the level of autonomy and decision-making capabilities compared to compound AI systems.\n\nFrom a development standpoint, AI applications, whether they are individual LLMs, LLMs with toolchains, or full AI agents face similar challenges. Mosaic AI Agent Framework helps developers manage the unique challenges of building and AI applications at all levels of complexity.\n\nExamples of AI agents\n\nHere are some examples of AI agents across industries:\n\nAI/BI: AI-powered chatbots and dashboards accept natural language prompts to perform analysis on a businesses’ data, drawing insights from the full lifecycle of their data. AI/BI agents parse requests, decide which data sources to, and how to communicate findings. AI/BI agents can improve over time through human feedback, offering tools to verify and refine its outputs.\n\nCustomer service: AI-powered chatbots, such as those used by customer service platforms, interact with users, understand natural language, and provide relevant responses or perform tasks. Companies use AI chatbots for customer service by answering queries, providing product information, and assisting with troubleshooting.\n\nManufacturing predictive maintenance: AI agents can go beyond simply predicting equipment failures, autonomously acting on them by ordering replacements, or scheduling maintenance to reduce downtime and increase productivity.\n\nNext steps\n\nLearn how to develop and evaluate AI agents:\n\nCreate an AI agent\n\nWhat is Mosaic AI Agent Evaluation?\n\nHands on AI agent tutorials:\n\nDemo: Mosaic AI Agent Framework and Agent Evaluation\n\nIntroduction: End-to-end generative AI agent tutorial\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nCompound AI systems\nWhat are AI agents?\nWhat are tools?\nExamples of AI agents\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Log and register AI agents | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/log-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Log and register AI agents\nLog and register AI agents\n\nDecember 18, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nLog AI agents using Mosaic AI Agent Framework. Logging an agent is the basis of the development process. Logging captures a “point in time” of the agent’s code and configuration so you can evaluate the quality of the configuration.\n\nRequirements\n\nCreate an AI agent before logging it.\n\nCode-based logging\n\nDatabricks recommends using MLflow’s Models from Code functionality when logging agents.\n\nIn this approach, the agent’s code is captured as a Python file, and the Python environment is captured as a list of packages. When the agent is deployed, the Python environment is restored, and the agent’s code is executed to load the agent into memory so it can be invoked when the endpoint is called.\n\nYou can couple this approach with the use of pre-deployment validation APIs like mlflow.models.predict() to ensure that the agent runs reliably when deployed for serving.\n\nThe code that logs the agent or agent must be in a separate notebook from the agent code. This notebook is called a driver notebook. For an example notebook, see Example notebooks.\n\nInfer Model Signature during logging\n\nDuring logging, you must define an MLflow Model Signature, which specifies the agent’s input and output schema. The signature validates inputs and outputs to ensure that the agent interacts correctly with downstream tools like AI Playground and the review app. It also guides other applications on how to use the agent effectively.\n\nDatabricks recommends using MLflow’s Model Signature inferencing capabilities to automatically generate the agent’s signature based on an input example you provide. This approach is more convenient than manually defining the signature.\n\nThe LangChain and PyFunc examples below use Model Signature inferencing.\n\nIf you would rather explicitly define a Model Signature yourself at logging time, see MLflow docs - How to log models with signatures.\n\nCode-based logging with LangChain\n\nThe following instructions and code sample show you how to log an agent with LangChain.\n\nCreate a notebook or Python file with your code. For this example, the notebook or file is named agent.py. The notebook or file must contain a LangChain agent, referred to here as lc_agent.\n\nInclude mlflow.models.set_model(lc_agent) in the notebook or file.\n\nCreate a new notebook to serve as the driver notebook (called driver.py in this example).\n\nIn the driver notebook, use the following code to run agent.py and log the results to an MLflow model:\n\nCopy\nPython\nmlflow.langchain.log_model(lc_model=\"/path/to/agent.py\", resources=list_of_databricks_resources)\n\n\nThe resources parameter declares Databricks-managed resources needed to serve the agent, such as a vector search index or serving endpoint that serves a foundation model. For more information, see Specify resources for automatic authentication passthrough.\n\nDeploy the model. See Deploy an agent for generative AI application.\n\nWhen the serving environment is loaded, agent.py is executed.\n\nWhen a serving request comes in, lc_agent.invoke(...) is called.\n\nCopy\nPython\n\nimport mlflow\n\ncode_path = \"/Workspace/Users/first.last/agent.py\"\nconfig_path = \"/Workspace/Users/first.last/config.yml\"\n\n# Input example used by MLflow to infer Model Signature\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Retrieval-augmented Generation?\",\n        }\n    ]\n}\n\n# example using langchain\nwith mlflow.start_run():\n  logged_agent_info = mlflow.langchain.log_model(\n    lc_model=code_path,\n    model_config=config_path, # If you specify this parameter, this configuration is used by agent code. The development_config is overwritten.\n    artifact_path=\"agent\", # This string is used as the path inside the MLflow model where artifacts are stored\n    input_example=input_example, # Must be a valid input to the agent\n    example_no_conversion=True, # Required\n  )\n\nprint(f\"MLflow Run: {logged_agent_info.run_id}\")\nprint(f\"Model URI: {logged_agent_info.model_uri}\")\n\n# To verify that the model has been logged correctly, load the agent and call `invoke`:\nmodel = mlflow.langchain.load_model(logged_agent_info.model_uri)\nmodel.invoke(example)\n\nCode-based logging with PyFunc\n\nThe following instructions and code sample show you how to log an agent with PyFunc.\n\nCreate a notebook or Python file with your code. For this example, the notebook or file is named agent.py. The notebook or file must contain a PyFunc class, named PyFuncClass.\n\nInclude mlflow.models.set_model(PyFuncClass) in the notebook or file.\n\nCreate a new notebook to serve as the driver notebook (called driver.py in this example).\n\nIn the driver notebook, use the following code to run agent.py and log the results to an MLflow model:\n\nCopy\nPython\nmlflow.pyfunc.log_model(python_model=\"/path/to/agent.py\", resources=list_of_databricks_resources)\n\n\nThe resources parameter declares Databricks-managed resources needed to serve the agent, such as a vector search index or serving endpoint that serves a foundation model. For more information, see Specify resources for automatic authentication passthrough.\n\nDeploy the model. See Deploy an agent for generative AI application.\n\nWhen the serving environment is loaded, agent.py is executed.\n\nWhen a serving request comes in, PyFuncClass.predict(...) is called.\n\nCopy\nPython\nimport mlflow\nfrom mlflow.models.resources import (\n    DatabricksServingEndpoint,\n    DatabricksVectorSearchIndex,\n)\n\ncode_path = \"/Workspace/Users/first.last/agent.py\"\nconfig_path = \"/Workspace/Users/first.last/config.yml\"\n\n# Input example used by MLflow to infer Model Signature\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Retrieval-augmented Generation?\",\n        }\n    ]\n}\n\nwith mlflow.start_run():\n  logged_agent_info = mlflow.pyfunc.log_model(\n    python_model=agent_notebook_path,\n    artifact_path=\"agent\",\n    input_example=input_example,\n    resources=resources_path,\n    example_no_conversion=True,\n    resources=[\n      DatabricksServingEndpoint(endpoint_name=\"databricks-mixtral-8x7b-instruct\"),\n      DatabricksVectorSearchIndex(index_name=\"prod.agents.databricks_docs_index\"),\n    ]\n  )\n\nprint(f\"MLflow Run: {logged_agent_info.run_id}\")\nprint(f\"Model URI: {logged_agent_info.model_uri}\")\n\n# To verify that the model has been logged correctly, load the agent and call `invoke`:\nmodel = mlflow.pyfunc.load_model(logged_agent_info.model_uri)\nmodel.invoke(example)\n\nSpecify resources for automatic authentication passthrough\n\nAI agents often need to authenticate to other resources to complete tasks. For example, an agent may need to access a Vector Search index to query unstructured data.\n\nAs described in Authentication for dependent resources, Model Serving supports authenticating to both Databricks-managed and external resources when you deploy the agent.\n\nFor the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront during logging. This enables automatic authentication passthrough when you deploy the agent - Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n\nTo enable automatic authentication passthrough, specify dependent resources using the resources parameter of the log_model() API, as shown in the following code.\n\nCopy\nPython\nimport mlflow\nfrom mlflow.models.resources import (\n    DatabricksVectorSearchIndex,\n    DatabricksServingEndpoint,\n    DatabricksSQLWarehouse,\n    DatabricksFunction,\n    DatabricksGenieSpace,\n    DatabricksTable,\n)\n\nwith mlflow.start_run():\n  logged_agent_info = mlflow.pyfunc.log_model(\n    python_model=agent_notebook_path,\n    artifact_path=\"agent\",\n    input_example=input_example,\n    example_no_conversion=True,\n    # Specify resources for automatic authentication passthrough\n    resources=[\n      DatabricksVectorSearchIndex(index_name=\"prod.agents.databricks_docs_index\"),\n      DatabricksServingEndpoint(endpoint_name=\"databricks-mixtral-8x7b-instruct\"),\n      DatabricksServingEndpoint(endpoint_name=\"databricks-bge-large-en\"),\n      DatabricksSQLWarehouse(warehouse_id=\"your_warehouse_id\"),\n      DatabricksFunction(function_name=\"ml.tools.python_exec\"),\n      DatabricksGenieSpace(genie_space_id=\"your_genie_space_id\"),\n      DatabricksTable(table_name=\"your_table_name\"),\n    ]\n  )\n\n\nDatabricks recommends you manually specify resources for all agent flavors.\n\nNote\n\nIf you do not specify resources when logging LangChain agents using mlflow.langchain.log_model(...), MLflow performs best-effort automatic inference of resources. However, this may not capture all dependencies, resulting in authorization errors when serving or querying the agent.\n\nThe following table lists the Databricks resources that support automatic authentication passthrough and the minimum mlflow version required to log the resource.\n\nResource type\n\n\t\n\nMinimum mlflow version required to log the resource\n\n\n\n\nVector Search index\n\n\t\n\nRequires mlflow 2.13.1 or above\n\n\n\n\nModel serving endpoint\n\n\t\n\nRequires mlflow 2.13.1 or above\n\n\n\n\nSQL warehouse\n\n\t\n\nRequires mlflow 2.16.1 or above\n\n\n\n\nUnity Catalog function\n\n\t\n\nRequires mlflow 2.16.1 or above\n\n\n\n\nGenie space\n\n\t\n\nRequires mlflow 2.17.1 or above\n\n\n\n\nUnity Catalog table\n\n\t\n\nRequires mlflow 2.18.0 or above\n\nRegister the agent to Unity Catalog\n\nBefore you deploy the agent, you must register the agent to Unity Catalog. Registering the agent packages it as a model in Unity Catalog. As a result, you can use Unity Catalog permissions for authorization for resources in the agent.\n\nCopy\nPython\nimport mlflow\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog_name = \"test_catalog\"\nschema_name = \"schema\"\nmodel_name = \"agent_name\"\n\nmodel_name = catalog_name + \".\" + schema_name + \".\" + model_name\nuc_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=model_name)\n\nNext steps\n\nAdd traces to an AI agent.\n\nDeploy an AI agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nCode-based logging\nInfer Model Signature during logging\nCode-based logging with LangChain\nCode-based logging with PyFunc\nSpecify resources for automatic authentication passthrough\nRegister the agent to Unity Catalog\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Deploy an agent for generative AI application | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/deploy-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Deploy an agent for generative AI application\nDeploy an agent for generative AI application\n\nDecember 06, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to deploy your AI agent using the deploy() function from the Agent Framework Python API.\n\nRequirements\n\nMLflow 2.13.1 or above to deploy agents using the the deploy() API from databricks.agents.\n\nRegister an AI agent to Unity Catalog. See Register the agent to Unity Catalog.\n\nDeploying agents from outside a Databricks notebook requires databricks-agents SDK version 0.12.0 or above.\n\nInstall the the databricks-agents SDK.\n\nCopy\nPython\n%pip install databricks-agents\ndbutils.library.restartPython()\n\nDeploy an agent using deploy()\n\nThe deploy() function does the following:\n\nCreates CPU model serving endpoints for your agent that can be integrated into your user-facing application.\n\nTo reduce cost for idle endpoints (at the expense of increased time to serve initial queries), you can enable scale to zero for your serving endpoint by passing scale_to_zero_enabled=True to deploy(). See Endpoint scaling expectations.\n\nInference tables are enabled on these Model Serving endpoints. See Inference tables for monitoring and debugging models.\n\nDatabricks automatically provides short-lived service principal credentials to agent code running in the endpoint. The credentials have the minimum permissions to access Databricks-managed resources as defined during model logging. Before generating these credentials, Databricks ensures that the endpoint owner has the appropriate permissions to prevent privilege escalation and unauthorized access. See Authentication for dependent resources.\n\nIf you have resource dependencies that are not Databricks-managed, for example, using Pinecone, you can pass in environment variables with secrets to the deploy() API. See Configure access to resources from model serving endpoints.\n\nEnables the Review App for your agent. The Review App lets stakeholders chat with the agent and give feedback using the Review App UI.\n\nLogs every request to the Review App or REST API to an inference table. The data logged includes query requests, responses, and intermediate trace data from MLflow Tracing.\n\nCreates a feedback model with the same catalog and schema as the agent you are trying to deploy. This feedback model is the mechanism that makes it possible to accept feedback from the Review App and log it to an inference table. This model is served in the same CPU model serving endpoint as your deployed agent. Because this serving endpoint has inference tables enabled, it is possible to log feedback from the Review App to an inference table.\n\nNote\n\nDeployments can take up to 15 minutes to complete. Raw JSON payloads take 10 - 30 minutes to arrive, and the formatted logs are processed from the raw payloads about every hour.\n\nCopy\nPython\n\nfrom databricks.agents import deploy\nfrom mlflow.utils import databricks_utils as du\n\ndeployment = deploy(model_fqn, uc_model_info.version)\n\n# query_endpoint is the URL that can be used to make queries to the app\ndeployment.query_endpoint\n\n# Copy deployment.rag_app_url to browser and start interacting with your RAG application.\ndeployment.rag_app_url\n\nAgent-enhanced inference tables\n\nThe deploy() creates three inference tables for each deployment to log requests and responses to and from the agent serving endpoint. Users can expect the data to be in the payload table within an hour of interacting with their deployment.\n\nPayload request logs and assessment logs might take longer to populate, but are ultimately derived from the raw payload table. You can extract request and assessment logs from the payload table yourself. Deletions and updates to the payload table are not reflected in the payload request logs or the payload assessment logs.\n\nTable\n\n\t\n\nExample Unity Catalog table name\n\n\t\n\nWhat is in each table\n\n\n\n\nPayload\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload\n\n\t\n\nRaw JSON request and response payloads\n\n\n\n\nPayload request logs\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload_request_logs\n\n\t\n\nFormatted request and responses, MLflow traces\n\n\n\n\nPayload assessment logs\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload_assessment_logs\n\n\t\n\nFormatted feedback, as provided in the Review App, for each request\n\nThe following shows the schema for the request logs table.\n\nColumn name\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nclient_request_id\n\n\t\n\nString\n\n\t\n\nClient request ID, usually null.\n\n\n\n\ndatabricks_request_id\n\n\t\n\nString\n\n\t\n\nDatabricks request ID.\n\n\n\n\ndate\n\n\t\n\nDate\n\n\t\n\nDate of request.\n\n\n\n\ntimestamp_ms\n\n\t\n\nLong\n\n\t\n\nTimestamp in milliseconds.\n\n\n\n\ntimestamp\n\n\t\n\nTimestamp\n\n\t\n\nTimestamp of the request.\n\n\n\n\nstatus_code\n\n\t\n\nInteger\n\n\t\n\nStatus code of endpoint.\n\n\n\n\nexecution_time_ms\n\n\t\n\nLong\n\n\t\n\nTotal execution milliseconds.\n\n\n\n\nconversation_id\n\n\t\n\nString\n\n\t\n\nConversation id extracted from request logs.\n\n\n\n\nrequest\n\n\t\n\nString\n\n\t\n\nThe last user query from the user’s conversation. This is extracted from the RAG request.\n\n\n\n\nresponse\n\n\t\n\nString\n\n\t\n\nThe last response to the user. This is extracted from the RAG request.\n\n\n\n\nrequest_raw\n\n\t\n\nString\n\n\t\n\nString representation of request.\n\n\n\n\nresponse_raw\n\n\t\n\nString\n\n\t\n\nString representation of response.\n\n\n\n\ntrace\n\n\t\n\nString\n\n\t\n\nString representation of trace extracted from the databricks_options of response Struct.\n\n\n\n\nsampling_fraction\n\n\t\n\nDouble\n\n\t\n\nSampling fraction.\n\n\n\n\nrequest_metadata\n\n\t\n\nMap[String, String]\n\n\t\n\nA map of metadata related to the model serving endpoint associated with the request. This map contains the endpoint name, model name, and model version used for your endpoint.\n\n\n\n\nschema_version\n\n\t\n\nString\n\n\t\n\nInteger for the schema version.\n\nThe following is the schema for the assessment logs table.\n\nColumn name\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nrequest_id\n\n\t\n\nString\n\n\t\n\nDatabricks request ID.\n\n\n\n\nstep_id\n\n\t\n\nString\n\n\t\n\nDerived from retrieval assessment.\n\n\n\n\nsource\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the information on who created the assessment.\n\n\n\n\ntimestamp\n\n\t\n\nTimestamp\n\n\t\n\nTimestamp of request.\n\n\n\n\ntext_assessment\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the data for any feedback on the agent’s responses from the review app.\n\n\n\n\nretrieval_assessment\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the data for any feedback on the documents retrieved for a response.\n\nAuthentication for dependent resources\n\nAI agents often need to authenticate to other resources to complete tasks. For example, an agent may need to access a Vector Search index to query unstructured data.\n\nYour agent can use one of the following methods to authenticate to dependent resources when you serve it behind a Model Serving endpoint:\n\nAutomatic authentication passthrough: Declare Databricks resource dependencies for your agent during logging. Databricks can automatically provision, rotate, and manage short-lived credentials when your agent is deployed to securely access resources. Databricks recommends using automatic authentication passthrough where possible.\n\nManual authentication: Manually specify long-lived credentials during agent deployment. Use manual authentication for Databricks resources that do not support automatic authentication passthrough, or for external API access.\n\nAutomatic authentication passthrough\n\nModel Serving supports automatic authentication passthrough for the most common types of Databricks resources used by agents.\n\nTo enable automatic authentication passthrough, you must specify dependencies during agent logging.\n\nThen, when you serve the agent behind an endpoint, Databricks performs the following steps:\n\nPermission verification: Databricks verifies that the endpoint creator can access all dependencies specified during agent logging.\n\nService principal creation and grants: A service principal is created for the agent model version and is automatically granted read access to agent resources.\n\nNote\n\nThe system-generated service principal does not appear in API or UI listings. If the agent model version is removed from the endpoint, the service principal is also deleted.\n\nCredential provisioning and rotation: Short-lived credentials (an M2M OAuth token) for the service principal are injected into the endpoint, allowing agent code to access Databricks resources. Databricks also rotates the credentials, ensuring that your agent has continued, secure access to dependent resources.\n\nThis authentication behavior is similar to the “Run as owner” behavior for Databricks dashboards - downstream resources like Unity Catalog tables are accessed using the credentials of a service principal with least-privilege access to dependent resources.\n\nThe following table lists the Databricks resources that support automatic authentication passthrough and the permissions the endpoint creator must have when deploying the agent.\n\nNote\n\nUnity Catalog resources also require USE SCHEMA on the parent schema and USE CATALOG on the parent catalog.\n\nResource type\n\n\t\n\nPermission\n\n\n\n\nSQL Warehouse\n\n\t\n\nUse Endpoint\n\n\n\n\nModel Serving endpoint\n\n\t\n\nCan Query\n\n\n\n\nUnity Catalog Function\n\n\t\n\nEXECUTE\n\n\n\n\nGenie space\n\n\t\n\nCan Run\n\n\n\n\nVector Search index\n\n\t\n\nCan Use\n\n\n\n\nUnity Catalog Table\n\n\t\n\nSELECT\n\nManual authentication\n\nYou can also manually provide credentials using secrets-based environment variables. Manual authentication can be helpful in the following scenarios:\n\nThe dependent resource does not support automatic authentication passthrough.\n\nThe agent is accessing an external resource or API.\n\nThe agent needs to use credentials other than those of the agent deployer.\n\nFor example, to use the Databricks SDK in your agent to access other dependent resources, you can set the environment variables described in Databricks client unified authentication.\n\nGet deployed applications\n\nThe following shows how to get your deployed agents.\n\nCopy\nPython\nfrom databricks.agents import list_deployments, get_deployments\n\n# Get the deployment for specific model_fqn and version\ndeployment = get_deployments(model_name=model_fqn, model_version=model_version.version)\n\ndeployments = list_deployments()\n# Print all the current deployments\ndeployments\n\nProvide feedback on a deployed agent (experimental)\n\nWhen you deploy your agent with agents.deploy(), agent framework also creates and deploys a “feedback” model version within the same endpoint, which you can query to provide feedback on your agent application. Feedback entries appear as request rows within the inference table associated with your agent serving endpoint.\n\nNote that this behavior is experimental: Databricks may provide a first-class API for providing feedback on a deployed agent in the future, and future functionality may require migrating to this API.\n\nLimitations of this API include:\n\nThe feedback API lacks input validation - it always responds successfully, even if passed invalid input.\n\nThe feedback API requires passing in the Databricks-generated request_id of the agent endpoint request on which you wish to provide feedback. To get the databricks_request_id, include {\"databricks_options\": {\"return_trace\": True}} in your original request to the agent serving endpoint. The agent endpoint response will then include the databricks_request_id associated with the request so that you can pass that request ID back to the feedback API when providing feedback on the agent response.\n\nFeedback is collected using inference tables. See inference table limitations.\n\nThe following example request provides feedback on the agent endpoint named “your-agent-endpoint-name”, and assumes that the DATABRICKS_TOKEN environment variable is set to a Databricks REST API token.\n\nCopy\nBash\ncurl \\\n  -u token:$DATABRICKS_TOKEN \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '\n      {\n          \"dataframe_records\": [\n              {\n                  \"source\": {\n                      \"id\": \"user@company.com\",\n                      \"type\": \"human\"\n                  },\n                  \"request_id\": \"573d4a61-4adb-41bd-96db-0ec8cebc3744\",\n                  \"text_assessments\": [\n                      {\n                          \"ratings\": {\n                              \"answer_correct\": {\n                                  \"value\": \"positive\"\n                              },\n                              \"accurate\": {\n                                  \"value\": \"positive\"\n                              }\n                          },\n                          \"free_text_comment\": \"The answer used the provided context to talk about Delta Live Tables\"\n                      }\n                  ],\n                  \"retrieval_assessments\": [\n                      {\n                          \"ratings\": {\n                              \"groundedness\": {\n                                  \"value\": \"positive\"\n                              }\n                          }\n                      }\n                  ]\n              }\n          ]\n      }' \\\nhttps://<workspace-host>.databricks.com/serving-endpoints/<your-agent-endpoint-name>/served-models/feedback/invocations\n\n\nYou can pass additional or different key-value pairs in the text_assessments.ratings and retrieval_assessments.ratings fields to provide different types of feedback. In the example, the feedback payload indicates that the agent’s response to the request with ID 573d4a61-4adb-41bd-96db-0ec8cebc3744 was correct, accurate, and grounded in context fetched by a retriever tool.\n\nAdditional resources\n\nWhat is Mosaic AI Agent Evaluation?\n\nGet feedback about the quality of an agentic application\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nDeploy an agent using deploy()\nAgent-enhanced inference tables\nAuthentication for dependent resources\nGet deployed applications\nProvide feedback on a deployed agent (experimental)\nAdditional resources\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Create an AI agent | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/create-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent\nCreate an AI agent\n\nDecember 18, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows you how to create a tool-calling AI agent using the Mosaic AI Agent Framework.\n\nLearn how to give an agent tools and start chatting with them to test and prototype the agent. Once you’re done prototyping the agent, export the Python code that defines the agent to iterate and deploy your AI agent.\n\nRequirements\n\nUnderstand the concepts of AI agents and tools as described in What are compound AI systems and AI agents?\n\nDatabricks recommends installing the latest version of the MLflow Python client when developing agents.\n\nCreate AI agent tools\n\nThe first step is to create a tool to give to your agent. Agents use tools to perform actions besides language generation, for example to retrieve structured or unstructured data, execute code, or talk to remote services (e.g. send an email or Slack message).\n\nFor this guide, you can use the built-in Unity Catalog function, system.ai.python_exec, to give your agent the ability to execute arbitrary Python code.\n\nTo learn more about creating your own agent tools, see Create AI agent tools.\n\nPrototype tool-calling agents in AI Playground\n\nNow that you have tool, use the AI Playground to give the tool to an agent and interact with it to validate and test behavior. The AI Playground provides a sandbox to prototype tool-calling agents.\n\nNote\n\nUnity Catalog, and serverless compute, Mosaic AI Agent Framework, and either pay-per-token foundation models or external models must be available in the current workspace to prototype agents in AI Playground.\n\nTo prototype a tool-calling endpoint.\n\nFrom Playground, select a model with the Tools enabled label.\n\nSelect Tools and specify your Unity Catalog function names in the dropdown:\n\nChat to test out the current combination of LLM, tools, and system prompt, and try variations.\n\nExport and deploy AI Playground agents\n\nAfter prototyping and refining the AI agent in AI Playground, you can export it to Python notebooks for further development or deploy it directly as a Model Serving endpoint\n\nClick Export to generate Python notebooks that help you develop and deploy the AI agent.\n\nAfter exporting the agent code, you see three files saved to your workspace:\n\nagent notebook: Contains Python code defining your agent using LangChain.\n\ndriver notebook: Contains Python code to log, trace, register, and deploy the AI agent using Mosaic AI Agent Framework.\n\nconfig.yml: Contains configuration information about your agent including tool definitions.\n\nOpen the agent notebook to see the LangChain code defining your agent, use this notebook to test and iterate on the agent programmatically such as defining more tools or adjusting the agent’s parameters.\n\nNote\n\nThe exported code might have different behavior from your AI playground session. Databricks recommends that you run the exported notebooks to iterate and debug further, evaluate agent quality, and then deploy the agent to share with others.\n\nOnce you’re happy with the agent’s outputs, you can run the driver notebook to log and deploy your agent to a Model Serving endpoint.\n\nDefine an agent in code\n\nIn addition to generating agent code from AI Playground, you can also define an agent in code yourself, using frameworks like LangChain or Python code. In order to deploy an agent using Agent Framework, its input must conform to one of the supported input and output formats.\n\nUse parameters to configure the agent\n\nIn the Agent Framework, you can use parameters to control how agents are executed. This allows you to quickly iterate by varying characteristics of your agent without changing the code. Parameters are key-value pairs that you define in a Python dictionary or a .yaml file.\n\nTo configure the code, create a ModelConfig, a set of key-value parameters. ModelConfig is either a Python dictionary or a .yaml file. For example, you can use a dictionary during development and then convert it to a .yaml file for production deployment and CI/CD. For details about ModelConfig, see the MLflow documentation.\n\nAn example ModelConfig is shown below.\n\nCopy\nYAML\nllm_parameters:\n  max_tokens: 500\n  temperature: 0.01\nmodel_serving_endpoint: databricks-dbrx-instruct\nvector_search_index: ml.docs.databricks_docs_index\nprompt_template: 'You are a hello world bot. Respond with a reply to the user''s\n  question that indicates your prompt template came from a YAML file. Your response\n  must use the word \"YAML\" somewhere. User''s question: {question}'\nprompt_template_input_vars:\n- question\n\n\nTo call the configuration from your code, use one of the following:\n\nCopy\nPython\n# Example for loading from a .yml file\nconfig_file = \"configs/hello_world_config.yml\"\nmodel_config = mlflow.models.ModelConfig(development_config=config_file)\n\n# Example of using a dictionary\nconfig_dict = {\n    \"prompt_template\": \"You are a hello world bot. Respond with a reply to the user's question that is fun and interesting to the user. User's question: {question}\",\n    \"prompt_template_input_vars\": [\"question\"],\n    \"model_serving_endpoint\": \"databricks-dbrx-instruct\",\n    \"llm_parameters\": {\"temperature\": 0.01, \"max_tokens\": 500},\n}\n\nmodel_config = mlflow.models.ModelConfig(development_config=config_dict)\n\n# Use model_config.get() to retrieve a parameter value\nvalue = model_config.get('sample_param')\n\nSet retriever schema\n\nAI agents often use retrievers, a type of agent tool that finds and returns relevant documents using a Vector Search index. For more information on retrievers, see Unstructured retrieval AI agent tools.\n\nTo ensure that retrievers are traced properly, call mlflow.models.set_retriever_schema when you define your agent in code. Use set_retriever_schema to map the column names in the returned table to MLflow’s expected fields such as primary_key, text_column, and doc_uri.\n\nCopy\nPython\n# Define the retriever's schema by providing your column names\n# These strings should be read from a config dictionary\nmlflow.models.set_retriever_schema(\n    name=\"vector_search\",\n    primary_key=\"chunk_id\",\n    text_column=\"text_column\",\n    doc_uri=\"doc_uri\"\n    # other_columns=[\"column1\", \"column2\"],\n)\n\n\nNote\n\nThe doc_uri column is especially important when evaluating the retriever’s performance. doc_uri is the main identifier for documents returned by the retriever, allowing you to compare them against ground truth evaluation sets. See Evaluation sets\n\nYou can also specify additional columns in your retriever’s schema by providing a list of column names with the other_columns field.\n\nIf you have multiple retrievers, you can define multiple schemas by using unique names for each retriever schema.\n\nSupported input and output formats\n\nAgent Framework uses MLflow Model Signatures to define input and output schemas for agents. Mosaic AI Agent Framework features require a minimum set of input/output fields to interact with features such as the Review App and the AI Playground. For more information, see Define an agent’s input and output schema.\n\nExample notebooks\n\nThese notebooks create a simple “Hello, world” chain to illustrate how to create a chain application in Databricks. The first example creates a simple chain. The second example notebook illustrates how to use parameters to minimize code changes during development.\n\nSimple chain notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nSimple chain driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nParameterized chain notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nParameterized chain driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nNext steps\n\nLog an AI agent.\n\nAdd traces to an AI agent.\n\nDeploy an AI agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nCreate AI agent tools\nPrototype tool-calling agents in AI Playground\nExport and deploy AI Playground agents\nDefine an agent in code\nUse parameters to configure the agent\nSupported input and output formats\nExample notebooks\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Create AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nCode interpreters\nStructured retrieval\nUnstructured retrieval\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create AI agent tools\nCreate AI agent tools\n\nDecember 18, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article provides an overview of building AI agent tools using the Mosaic AI Agent Framework.\n\nAgent Framework helps developers create tools that AI agents can use to perform actions beyond language generation, such as retrieving structured or unstructured data or executing code.\n\nFor an introduction to AI agents, see What are compound AI systems and AI agents?.\n\nUnity Catalog function tools vs. agent code tools\n\nTo create tools and add them to agents with Mosaic AI Agent Framework, you can use any combination of the following methods:\n\nUnity Catalog functions: Unity Catalog functions are defined and managed within Unity Catalog, offering built-in security and compliance features. Writing your tool as a Unity Catalog function grants easier discoverability, governance, and reuse. Unity Catalog functions work especially well for applying transformations and aggregations on large datasets.\n\nAgent code tools: These tools are defined in the same code that defines the AI agent. This approach is useful when calling REST APIs, using arbitrary code or libraries, or executing low-latency tools. However, this approach lacks the built-in discoverability and governance provided by Unity Catalog functions.\n\nBoth methods are compatible with agents written in custom Python code or using agent-authoring libraries like LangGraph.\n\nTo see examples of Unity Catalog function tools and agent code tools, see Agent tool examples\n\nImprove tool-calling with documentation\n\nClear and detailed documentation helps AI agents understand when and how to use the tools you provide. When creating tools, document tool parameters and return values thoroughly to ensure that your AI agent uses the tools correctly and at the right time:\n\nFor Unity Catalog functions, use COMMENT to describe the tool and parameters.\n\nExample of effective tool documentation\n\nThe following example shows effective COMMENT strings for a Unity Catalog function tool that queries a structured table.\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.lookup_customer_info(\n  customer_name STRING COMMENT 'Name of the customer whose info to look up.'\n)\nRETURNS STRING\nCOMMENT 'Returns metadata about a specific customer including their email and ID.'\nRETURN SELECT CONCAT(\n    'Customer ID: ', customer_id, ', ',\n    'Customer Email: ', customer_email\n  )\n  FROM main.default.customer_data\n  WHERE customer_name = customer_name\n  LIMIT 1;\n\nExample of ineffective tool documentation\n\nThe following example shows ineffective COMMENT strings that miss key information, such as the return values.\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.lookup_customer_info(\n  customer_name STRING COMMENT 'Name of the customer.'\n)\nRETURNS STRING\nCOMMENT 'Returns info about a customer.'\nRETURN SELECT CONCAT(\n    'Customer ID: ', customer_id, ', ',\n    'Customer Email: ', customer_email\n  )\n  FROM main.default.customer_data\n  WHERE customer_name = customer_name\n  LIMIT 1;\n\nAgent tool examples\n\nSee the following articles for examples of agent tools:\n\nCode interpreter tools allow agents to execute arbitrary code such as Python.\n\nStructured data retrieval tools allow your agent to query structured data sources like SQL tables.\n\nUnstructured data retrieval tools allow your agent to query unstructured data sources like a text corpora to perform retrieval augmented generation.\n\nAdd Unity Catalog tools to agents\n\nOnce you create the Unity Catalog tools, add them to your agent. LangChain agents can leverage the UCFunctionToolkit to incorporate UC tools.\n\nExport tool-calling agents from the AI Playground\n\nThe AI Playground provides a convenient way to add Unity Catalog tools to an LLM, test the agent, and export its code.\n\nTo use the AI Playground to export agents, your workspace must meet the following requirements:\n\nUnity Catalog must be enabled.\n\nServerless compute must be enabled.\n\nEither Pay-per-token foundation models or External models must be enabled.\n\nUse the following steps to export tool-calling agents code:\n\nFrom the AI Playground, select a model with the Tools enabled label.\n\nSelect Tools and click Add a tool.\n\nIn the dropdown menu, select a Unity Catalog function:\n\nUse the Playground to chat and test the current combination of LLM, tools, and system prompt. Try variations to get a feel for how the current setup works.\n\nAfter adding tools, export the agent to Python notebooks:\n\nClick Export to generate Python notebooks that define and deploy the agent.\n\nAfter exporting the agent code, you will see three files saved to your workspace:\n\nagent notebook: Contains Python code defining your agent using LangChain.\n\ndriver notebook: Contains Python code to log, trace, register, and deploy the AI agent using Mosaic AI Agent Framework.\n\nconfig.yml: Contains configuration information about your agent, including tool definitions.\n\nOpen the agent notebook to see the LangChain code defining your agent. Use this notebook to test and iterate on the agent programmatically, such as defining more tools.\n\nWhen you’re happy with the agent’s outputs, run the driver notebook to log and deploy your agent to a Model Serving endpoint.\n\nNext steps\n\nTrace an agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nUnity Catalog function tools vs. agent code tools\nImprove tool-calling with documentation\nAgent tool examples\nAdd Unity Catalog tools to agents\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Introduction to building gen AI apps on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/build-genai-apps.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nAgent tools\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks\nIntroduction to building gen AI apps on Databricks\n\nDecember 13, 2024\n\nMosaic AI provides a comprehensive platform to build, deploy, and manage GenAI applications. This article guides you through the essential components and processes involved in developing GenAI applications on Databricks.\n\nDeploy and query gen AI models\n\nFor simple use cases, you can directly serve and query gen AI models, including high quality open-source models, as well as third-party models from LLM providers such as OpenAI and Anthropic.\n\nMosaic AI Model Serving supports serving and querying generative AI models using the following capabilities:\n\nFoundation Model APIs. This functionality makes state-of-the-art open models and fine-tuned model variants available to your model serving endpoint. These models are curated foundation model architectures that support optimized inference. Base models, like DBRX Instruct, Meta-Llama-3.1-70B-Instruct, GTE-Large, and Mistral-7B are available for immediate use with pay-per-token pricing, and workloads that require performance guarantees, like fine-tuned model variants, can be deployed with provisioned throughput.\n\nExternal models. These are generative AI models that are hosted outside of Databricks. Endpoints that serve external models can be centrally governed and customers can establish rate limits and access control for them. Examples include foundation models like OpenAI’s GPT-4, Anthropic’s Claude, and others.\n\nSee Create generative AI model serving endpoints.\n\nMosaic AI Agent Framework\n\nMosaic AI Agent Framework comprises a set of tools on Databricks designed to help developers build, deploy, and evaluate production-quality agents like Retrieval Augmented Generation (RAG) applications.\n\nIt is compatible with third-party frameworks like LangChain and LlamaIndex, allowing you to develop with your preferred framework and while leveraging Databricks’ managed Unity Catalog, Agent Evaluation Framework, and other platform benefits.\n\nQuickly iterate on agent development using the following features:\n\nCreate and log agents using any library and MLflow. Parameterize your agents to experiment and iterate on agent development quickly.\n\nAgent tracing lets you log, analyze, and compare traces across your agent code to debug and understand how your agent responds to requests.\n\nImprove agent quality using DSPy. DSPy can automate prompt engineering and fine-tuning to improve the quality of your GenAI agents.\n\nDeploy agents to production with native support for token streaming and request/response logging, plus a built-in review app to get user feedback for your agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nDeploy and query gen AI models\nMosaic AI Agent Framework\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "AI and machine learning on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/index.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 20, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks\nAI and machine learning on Databricks\n\nNovember 15, 2024\n\nThis article describes the tools that Mosaic AI (formerly Databricks Machine Learning) provides to help you build AI and ML systems. The diagram shows how various products on Databricks platform help you implement your end to end workflows to build and deploy AI and ML systems\n\nGenerative AI on Databricks\n\nMosaic AI unifies the AI lifecycle from data collection and preparation, to model development and LLMOps, to serving and monitoring. The following features are specifically optimized to facilitate the development of generative AI applications:\n\nUnity Catalog for governance, discovery, versioning, and access control for data, features, models, and functions.\n\nMLflow for model development tracking.\n\nMosaic AI Model Serving for deploying LLMs. You can configure a model serving endpoint specifically for accessing generative AI models:\n\nState-of-the-art open LLMs using Foundation Model APIs.\n\nThird-party models hosted outside of Databricks. See External models in Mosaic AI Model Serving.\n\nMosaic AI Vector Search provides a queryable vector database that stores embedding vectors and can be configured to automatically sync to your knowledge base.\n\nLakehouse Monitoring for data monitoring and tracking model prediction quality and drift using automatic payload logging with inference tables.\n\nAI Playground for testing generative AI models from your Databricks workspace. You can prompt, compare and adjust settings such as system prompt and inference parameters.\n\nFoundation Model Fine-tuning (now part of Mosaic AI Model Training) for customizing a foundation model using your own data to optimize its performance for your specific application.\n\nMosaic AI Agent Framework for building and deploying production-quality agents like Retrieval Augmented Generation (RAG) applications.\n\nMosaic AI Agent Evaluation for evaluating the quality, cost, and latency of generative AI applications, including RAG applications and chains.\n\nWhat is generative AI?\n\nGenerative AI is a type of artificial intelligence focused on the ability of computers to use models to create content like images, text, code, and synthetic data.\n\nGenerative AI applications are built on top of generative AI models: large language models (LLMs) and foundation models.\n\nLLMs are deep learning models that consume and train on massive datasets to excel in language processing tasks. They create new combinations of text that mimic natural language based on their training data.\n\nGenerative AI models or foundation models are large ML models pre-trained with the intention that they are to be fine-tuned for more specific language understanding and generation tasks. These models are used to discern patterns within the input data.\n\nAfter these models have completed their learning processes, together they generate statistically probable outputs when prompted and they can be employed to accomplish various tasks, including:\n\nImage generation based on existing ones or utilizing the style of one image to modify or create a new one.\n\nSpeech tasks such as transcription, translation, question/answer generation, and interpretation of the intent or meaning of text.\n\nImportant\n\nWhile many LLMs or other generative AI models have safeguards, they can still generate harmful or inaccurate information.\n\nGenerative AI has the following design patterns:\n\nPrompt Engineering: Crafting specialized prompts to guide LLM behavior\n\nRetrieval Augmented Generation (RAG): Combining an LLM with external knowledge retrieval\n\nFine-tuning: Adapting a pre-trained LLM to specific data sets of domains\n\nPre-training: Training an LLM from scratch\n\nMachine learning on Databricks\n\nWith Mosaic AI, a single platform serves every step of ML development and deployment, from raw data to inference tables that save every request and response for a served model. Data scientists, data engineers, ML engineers and DevOps can do their jobs using the same set of tools and a single source of truth for the data.\n\nMosaic AI unifies the data layer and ML platform. All data assets and artifacts, such as models and functions, are discoverable and governed in a single catalog. Using a single platform for data and models makes it possible to track lineage from the raw data to the production model. Built-in data and model monitoring saves quality metrics to tables that are also stored in the platform, making it easier to identify the root cause of model performance problems. For more information about how Databricks supports the full ML lifecycle and MLOps, see MLOps workflows on Databricks and MLOps Stacks: model development process as code.\n\nSome of the key components of the data intelligence platform are:\n\nTasks\n\n\t\n\nComponent\n\n\n\n\nGovern and manage data, features, models, and functions. Also discovery, versioning, and lineage.\n\n\t\n\nUnity Catalog\n\n\n\n\nTrack changes to data, data quality, and model prediction quality\n\n\t\n\nLakehouse Monitoring, Inference tables\n\n\n\n\nFeature development and management\n\n\t\n\nFeature engineering and serving.\n\n\n\n\nTrain models\n\n\t\n\nAutoML, Databricks notebooks\n\n\n\n\nTrack model development\n\n\t\n\nMLflow tracking\n\n\n\n\nServe custom models\n\n\t\n\nMosaic AI Model Serving.\n\n\n\n\nBuild automated workflows and production-ready ETL pipelines\n\n\t\n\nDatabricks Jobs\n\n\n\n\nGit integration\n\n\t\n\nDatabricks Git folders\n\nDeep learning on Databricks\n\nConfiguring infrastructure for deep learning applications can be difficult. Databricks Runtime for Machine Learning takes care of that for you, with clusters that have built-in compatible versions of the most common deep learning libraries like TensorFlow, PyTorch, and Keras.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. It also supports libraries like Ray to parallelize compute processing for scaling ML workflows and ML applications.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. Mosaic AI Model Serving enables creation of scalable GPU endpoints for deep learning models with no extra configuration.\n\nFor machine learning applications, Databricks recommends using a cluster running Databricks Runtime for Machine Learning. See Create a cluster using Databricks Runtime ML.\n\nTo get started with deep learning on Databricks, see:\n\nBest practices for deep learning on Databricks\n\nDeep learning on Databricks\n\nReference solutions for deep learning\n\nNext steps\n\nTo get started, see:\n\nTutorials: Get started with AI and machine learning\n\nFor a recommended MLOps workflow on Databricks Mosaic AI, see:\n\nMLOps workflows on Databricks\n\nTo learn about key Databricks Mosaic AI features, see:\n\nWhat is AutoML?\n\nFeature engineering and serving\n\nModel serving with Databricks\n\nLakehouse Monitoring\n\nManage model lifecycle\n\nMLflow experiment tracking\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nGenerative AI on Databricks\nMachine learning on Databricks\nDeep learning on Databricks\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  }
]