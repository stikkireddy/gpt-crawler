[
  {
    "title": "Create a vector search retriever tool | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/agent-retriever.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nCreate AI agent tools\nCreate a retriever tool\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent  Create a vector search retriever tool\nCreate a vector search retriever tool\n\nNovember 08, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nLearn how to use Mosaic AI Agent Framework to create retrievers. A retriever is a type of agent tool that finds and returns relevant documents using a Vector Search index. Retrievers are a core component of RAG (Retrieval Augmented Generation) applications.\n\nRequirements\n\nMLflow Document is only available on MLflow version 2.14.0 and above.\n\nAn existing Vector Search Index.\n\nPyFunc retriever example\n\nThe following example uses databricks-vectorsearch to create a basic retriever that performs a Vector Search similarity search with filters. It uses MLflow decorators to enable agent tracing.\n\nThe retriever function should return a Document type. Use the metadata field in the Document class to add additional attributes to the returned document, like like doc_uri and similarity_score.\n\nUse the following code in the agent module or agent notebook.\n\nCopy\nPython\nimport mlflow\nimport json\n\nfrom mlflow.entities import Document\nfrom typing import List, Dict, Any\nfrom dataclasses import asdict\nfrom databricks.vector_search.client import VectorSearchClient\n\nclass VectorSearchRetriever:\n    \"\"\"\n    Class using Databricks Vector Search to retrieve relevant documents.\n    \"\"\"\n    def __init__(self):\n        self.vector_search_client = VectorSearchClient(disable_notice=True)\n        # TODO: Replace this with the list of column names to return in the result when querying Vector Search\n        self.columns = [\"chunk_id\", \"text_column\", \"doc_uri\"]\n        self.vector_search_index = self.vector_search_client.get_index(\n            index_name=\"catalog.schema.chunked_docs_index\"\n        )\n        mlflow.models.set_retriever_schema(\n            name=\"vector_search\",\n            primary_key=\"chunk_id\",\n            text_column=\"text_column\",\n            doc_uri=\"doc_uri\"\n        )\n\n    @mlflow.trace(span_type=\"RETRIEVER\", name=\"vector_search\")\n    def __call__(\n        self,\n        query: str,\n        filters: Dict[Any, Any] = None,\n        score_threshold = None\n    ) -> List[Document]:\n        \"\"\"\n        Performs vector search to retrieve relevant chunks.\n        Args:\n            query: Search query.\n            filters: Optional filters to apply to the search. Filters must follow the Databricks Vector Search filter spec\n            score_threshold: Score threshold to use for the query.\n\n        Returns:\n            List of retrieved Documents.\n        \"\"\"\n\n        results = self.vector_search_index.similarity_search(\n            query_text=query,\n            columns=self.columns,\n            filters=filters,\n            num_results=5,\n            query_type=\"ann\"\n        )\n\n        documents = self.convert_vector_search_to_documents(\n            results, score_threshold\n        )\n        return [asdict(doc) for doc in documents]\n\n    @mlflow.trace(span_type=\"PARSER\")\n    def convert_vector_search_to_documents(\n        self, vs_results, score_threshold\n    ) -> List[Document]:\n\n        docs = []\n        column_names = [column[\"name\"] for column in vs_results.get(\"manifest\", {}).get(\"columns\", [])]\n        result_row_count = vs_results.get(\"result\", {}).get(\"row_count\", 0)\n\n        if result_row_count > 0:\n            for item in vs_results[\"result\"][\"data_array\"]:\n                metadata = {}\n                score = item[-1]\n\n                if score >= score_threshold:\n                    metadata[\"similarity_score\"] = score\n                    for i, field in enumerate(item[:-1]):\n                        metadata[column_names[i]] = field\n\n                    page_content = metadata.pop(\"text_column\", None)\n\n                    if page_content:\n                        doc = Document(\n                            page_content=page_content,\n                            metadata=metadata\n                        )\n                        docs.append(doc)\n\n        return docs\n\n\nTo run the retriever, run the following Python code. You can optionally include Vector Search filters in the request to filter results.\n\nCopy\nPython\nretriever = VectorSearchRetriever()\nquery = \"What is Databricks?\"\nfilters={\"text_column LIKE\": \"Databricks\"},\nresults = retriever(query, filters=filters, score_threshold=0.1)\n\nSet retriever schema\n\nTo ensure that retrievers are traced properly, call mlflow.models.set_retriever_schema when you define your agent in code. Use set_retriever_schema to map the column names in the returned table to MLflow’s expected fields such as primary_key, text_column, and doc_uri.\n\nCopy\nPython\n# Define the retriever's schema by providing your column names\nmlflow.models.set_retriever_schema(\n    name=\"vector_search\",\n    primary_key=\"chunk_id\",\n    text_column=\"text_column\",\n    doc_uri=\"doc_uri\"\n    # other_columns=[\"column1\", \"column2\"],\n)\n\n\nNote\n\nThe doc_uri column is especially important when evaluating the retriever’s performance. doc_uri is the main identifier for documents returned by the retriever, allowing you to compare them against ground truth evaluation sets. See Evaluation sets.\n\nYou can also specify additional columns in your retriever’s schema by providing a list of column names with the other_columns field.\n\nIf you have multiple retrievers, you can define multiple schemas by using unique names for each retriever schema.\n\nTrace the retriever\n\nMLflow tracing adds observability by capturing detailed information about your agent’s execution. It provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to pinpoint the source of bugs and unexpected behaviors easily.\n\nThis example uses the @mlflow.trace decorator to create a trace for the retriever and parser. For other options for setting up trace methods, see MLflow Tracing for agents.\n\nThe decorator creates a span that starts when the function is invoked and ends when it returns. MLflow automatically records the function’s input and output and any exceptions raised from it.\n\nNote\n\nLangChain, LlamaIndex, and OpenAI library users can use MLflow autologging instead of manually defining traces with the decorator. See Use autologging to add traces to your agents.\n\nCopy\nPython\n...\n@mlflow.trace(span_type=\"RETRIEVER\", name=\"vector_search\")\ndef __call__(self, query: str) -> List[Document]:\n  ...\n\n\nTo ensure downstream applications such as Agent Evaluation and the AI Playground render the retriever trace correctly, make sure the decorator meets the following requirements:\n\nUse span_type=\"RETRIEVER\" and ensure the function returns List[Document] object. See Retriever spans.\n\nThe trace name and the retriever_schema name must match to configure the trace correctly.\n\nFilter Vector Search results\n\nYou can limit the search scope to a subset of data using a Vector Search filter.\n\nThe filters parameter in VectorSearchRetriever defines the filter conditions using the Databricks Vector Search filter specification.\n\nCopy\nPython\nfilters = {\"text_column LIKE\": \"Databricks\"}\n\n\nInside the __call__ method, the filters dictionary is passed directly to the similarity_search function:\n\nCopy\nPython\nresults = self.vector_search_index.similarity_search(\n    query_text=query,\n    columns=self.columns,\n    filters=filters,\n    num_results=5,\n    query_type=\"ann\"\n)\n\n\nAfter initial filtering, the score_threshold parameter provides additional filtering by setting a minimum similarity score.\n\nCopy\nPython\nif score >= score_threshold:\n    metadata[\"similarity_score\"] = score\n\n\nThe final result includes documents that meet the filters and score_threshold conditions.\n\nRetriever example applications\n\nSee the genai-cookbook GitHub repository for AI agent examples that use retrievers:\n\nLangChain retriever example: Demo: Mosaic AI Agent Framework and Agent Evaluation\n\nPyFunc retriever example: Agent app sample code\n\nNext steps\n\nCreate an AI agent\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nPyFunc retriever example\nSet retriever schema\nTrace the retriever\nFilter Vector Search results\nRetriever example applications\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Create AI agent tools | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nCreate AI agent tools\nCreate a retriever tool\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent  Create AI agent tools\nCreate AI agent tools\n\nNovember 08, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows you how to create tools for AI agents using the Mosaic AI Agent Framework. To learn more, see What are compound AI systems and AI agents?.\n\nAI agents use tools to perform actions besides language generation, such as retrieving structured or unstructured data, executing code, or talking to remote services (for example, sending an email or Slack message).\n\nTo provide tools to an agent with Mosaic AI Agent Framework, you can use any combination of the following methods:\n\nCreate tools with Unity Catalog functions: Unity Catalog functions provide easy discovery, governance, and sharing. Unity Catalog functions work especially well for applying transformations and aggregations on large datasets.\n\nCreate tools with agent code: This approach is useful when making calls to REST APIs, using arbitrary code or libraries, or executing tools with very low latency. However, this approach lacks the built-in discoverability and governance provided by Unity Catalog functions. Weigh this tradeoff when building your agent to determine which approach is best.\n\nBoth approaches work for agents written in custom Python code or using an agent-authoring library like LangGraph.\n\nImprove tool-calling with documentation\n\nAI agents use your documentation to understand when and how to use the tools you provide. So, when defining tools, be sure to add clear and detailed documentation for the tool, its parameters, and its return values.\n\nFor Unity Catalog functions, use COMMENT to describe the tool.\n\nRequirements\n\nDatabricks recommends installing the latest version of the MLflow Python client when developing agents. See Authentication for dependent resources for information on mlflow version requirements.\n\nCreate tools with Unity Catalog functions\n\nThe following examples show you how to create agent tools with Unity Catalog functions.\n\nPython executor tool\n\nThis example creates a tool to execute Python code.\n\nAn LLM can use this tool to execute Python code provided by a user or written by the LLM.\n\nRun the following code in a notebook cell. It uses the %sql notebook magic to create a Unity Catalog function called python_exec.\n\nCopy\nSQL\n%sql\nCREATE OR REPLACE FUNCTION\nmain.default.python_exec (\ncode STRING COMMENT 'Python code to execute. Remember to print the final result to stdout.'\n)\nRETURNS STRING\nLANGUAGE PYTHON\nDETERMINISTIC\nCOMMENT 'Executes Python code in the sandboxed environment and returns its stdout. The runtime is stateless, and you can not read the output of the previous tool executions. No such variables, \"rows\", or \"observations\" were defined. Calling another tool inside a Python code is NOT allowed. Use standard Python libraries only.'\nAS $$\nimport sys\nfrom io import StringIO\nsys_stdout = sys.stdout\nredirected_output = StringIO()\nsys.stdout = redirected_output\nexec(code)\nsys.stdout = sys_stdout\nreturn redirected_output.getvalue()\n$$\n\nTable query tool\n\nThis example creates a tool to query customer data from a Unity Catalog table.\n\nIt creates a Unity Catalog function called lookup_customer_info that an LLM can use to retrieve structured data from a hypothetical customer_data table.\n\nRun the following code in a SQL editor.\n\nCopy\nSQL\nCREATE OR REPLACE FUNCTION main.default.lookup_customer_info(\n  customer_name STRING COMMENT 'Name of the customer whose info to look up'\n)\nRETURNS STRING\nCOMMENT 'Returns metadata about a particular customer given the customer name, including the customer email and ID. The\ncustomer ID can be used for other queries.'\nRETURN SELECT CONCAT(\n    'Customer ID: ', customer_id, ', ',\n    'Customer Email: ', customer_email\n  )\n  FROM main.default.customer_data\n  WHERE customer_name = customer_name\n  LIMIT 1;\n\nAssign Unity Catalog tools to agents\n\nAfter you create the Unity Catalog tools, assign the tools to your agent so it can use them.\n\nThis section describes how to create a tool-calling agent using the AI playground.\n\nExport tool-calling agents from the AI Playground\n\nUse the AI Playground to assign Unity Catalog tools to an LLM, test the agent, and then export the code that defines the tool-calling agent.\n\nTo use the AI Playground to export agents, your workspace must meet the the following requirements:\n\nUnity Catalog enabled\n\nServerless compute enabled\n\nPay-per-token foundation models or External models enabled\n\nFrom Playground, select a model with the Tools enabled label.\n\nSelect Tools and click Add a tool.\n\nIn the dropdown menu, select a Unity Catalog function:\n\nUse the Playground to chat and test the current combination of LLM, tools, and system prompt. Try variations to get a feel for how the current setup works.\n\nAfter adding tools, export the agent to Python notebooks:\n\nClick Export to generate Python notebooks that define and deploy the agent.\n\nAfter exporting the agent code, you will see three files saved to your workspace:\n\nagent notebook: Contains Python code defining your agent using LangChain.\n\ndriver notebook: Contains Python code to log, trace, register, and deploy the AI agent using Mosaic AI Agent Framework.\n\nconfig.yml: Contains configuration information about your agent, including tool definitions.\n\nOpen the agent notebook to see the LangChain code defining your agent. Use this notebook to test and iterate on the agent programmatically, such as defining more tools.\n\nWhen you’re happy with the agent’s outputs, you can run the driver notebook to log and deploy your agent to a Model Serving endpoint.\n\nCreate tools with agent code\n\nYou can also create tools in the agent’s code rather than in Unity Catalog functions.\n\nThis approach is useful when calling REST APIs, using arbitrary code or libraries, or executing low-latency tools. However, defining tools in the agent’s code lacks the discoverability and governance provided by Unity Catalog functions.\n\nFor an example of a vector search retrieval tool defined in agent code, see Create a vector search retriever tool.\n\nNext steps\n\nCreate an agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nImprove tool-calling with documentation\nRequirements\nCreate tools with Unity Catalog functions\nAssign Unity Catalog tools to agents\nCreate tools with agent code\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "What are compound AI systems and AI agents? | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/ai-agents.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  What are compound AI systems and AI agents?\nWhat are compound AI systems and AI agents?\n\nNovember 30, 2024\n\nMosaic AI Agent Framework helps developers overcome the unique challenges of developing AI agents and compound AI systems. Learn what makes an AI application a compound AI system and an AI agent.\n\nCompound AI systems\n\nCompound AI systems are systems that tackle AI tasks by combining multiple interacting components. In contrast, an AI model is simply a statistical model, e.g., a Transformer that predicts the next token in text. Compound AI systems are an increasingly common design pattern for AI applications due to their performance and flexibility.\n\nFor more information, see The Shift from Models to Compound AI Systems .\n\nWhat are AI agents?\n\nThe industry is still defining AI agents, however it generally understood as an AI system where the model makes some or all of the planning decisions in contrast to hard-coded logic. These agents use large language models (LLMs) to make decisions and accomplish their objectives.\n\nMany AI agents applications are made of multiple systems, thus qualifying them as compound AI systems.\n\nAgency is a continuum, the more freedom we provide models to control the behavior of the system, the more agent-like the application becomes.\n\nWhat are tools?\n\nAI agents use tools to perform actions besides language generation, for example to retrieve structured or unstructured data, run code, or talk to remote services like sending an email or Slack message.\n\nOn Databricks, you can use Unity Catalog functions as tools, enabling easy discovery, governance, and sharing of tools. You can also define tools using open source agent authoring libraries like LangChain.\n\nIn typical agentic workflows, the agent LLM is given metadata about tools, which it uses to determine when and how to use the tool. So when defining tools, you must ensure that the tool, its parameters, and its return value are well-documented, so that the agent LLM can best use the tool.\n\nFrom LLMs to AI agents\n\nTo understand AI agents, it’s helpful to consider the evolution of AI systems.\n\nLLMs: Initially, large language models simply responded to prompts based on knowledge from a vast training dataset.\n\nLLMs + tool chains: Then, developers added hardcoded tools to expand the LLM’s capabilities. For example, retrieval augmented generation (RAG) expanded an LLM’s knowledge base with custom documentation sets, while API tools allowed LLMs to perform tasks like create support tickets or send emails.\n\nAI agents: Now, AI agents autonomously create plans and execute tasks based on their understanding of the problem. AI agents still use tools but it’s up to them to decide which tool to use and when. The key distinction is in the level of autonomy and decision-making capabilities compared to compound AI systems.\n\nFrom a development standpoint, AI applications, whether they are individual LLMs, LLMs with toolchains, or full AI agents face similar challenges. Mosaic AI Agent Framework helps developers manage the unique challenges of building and AI applications at all levels of complexity.\n\nExamples of AI agents\n\nHere are some examples of AI agents across industries:\n\nAI/BI: AI-powered chatbots and dashboards accept natural language prompts to perform analysis on a businesses’ data, drawing insights from the full lifecycle of their data. AI/BI agents parse requests, decide which data sources to, and how to communicate findings. AI/BI agents can improve over time through human feedback, offering tools to verify and refine its outputs.\n\nCustomer service: AI-powered chatbots, such as those used by customer service platforms, interact with users, understand natural language, and provide relevant responses or perform tasks. Companies use AI chatbots for customer service by answering queries, providing product information, and assisting with troubleshooting.\n\nManufacturing predictive maintenance: AI agents can go beyond simply predicting equipment failures, autonomously acting on them by ordering replacements, or scheduling maintenance to reduce downtime and increase productivity.\n\nNext steps\n\nLearn how to develop and evaluate AI agents:\n\nCreate an AI agent\n\nWhat is Mosaic AI Agent Evaluation?\n\nHands on AI agent tutorials:\n\nDemo: Mosaic AI Agent Framework and Agent Evaluation\n\nIntroduction: End-to-end generative AI agent tutorial\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nCompound AI systems\nWhat are AI agents?\nWhat are tools?\nExamples of AI agents\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Define an agent’s input and output schema | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/agent-schema.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Define an agent’s input and output schema\nDefine an agent’s input and output schema\n\nNovember 20, 2024\n\nMLflow Model Signatures define the input and output schema requirements for your AI agent. The Model Signature tells internal and external components how to interact with your agent. The Model Signature is a validation check to ensure that inputs adhere to schema requirements.\n\nFor example, to use the Agent Evaluation review app, your agent must adhere to the Agent Evaluation input schema.\n\nSupported input schemas\n\nMosaic AI Agent Framework supports the following input schemas.\n\nOpenAI chat completion schema\n\nNote\n\nDatabricks recommends the OpenAI chat completion schema because it’s widely used and interoperable with many agent frameworks and applications. If the OpenAI chat completion schema does not meet your needs, you can define your own schema. See Custom agent schemas.\n\n(Recommended) Databricks recommends using the OpenAI chat completion schema. The OpenAI chat completion schema should have an array of objects as a messages parameter. This format is best for RAG applications.\n\nCopy\nPython\nquestion = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Retrieval-Augmented Generation?\",\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"RAG, or Retrieval Augmented Generation, is a generative AI design pattern that combines a large language model (LLM) with external knowledge retrieval. This approach allows for real-time data connection to generative AI applications, improving their accuracy and quality by providing context from your data to the LLM during inference. Databricks offers integrated tools that support various RAG scenarios, such as unstructured data, structured data, tools & function calling, and agents.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"How to build RAG for unstructured data\",\n        },\n    ]\n}\n\nSplitChatMessageRequest\n\nSplitChatMessagesRequest is recommended for multi-turn chat applications, especially when you want to manage the current query and history separately.\n\nCopy\nPython\n  question = {\n      \"query\": \"What is MLflow\",\n      \"history\": [\n          {\n              \"role\": \"user\",\n              \"content\": \"What is Retrieval-augmented Generation?\"\n          },\n          {\n              \"role\": \"assistant\",\n              \"content\": \"RAG is\"\n          }\n      ]\n  }\n\nLangchain Expression Language\n\nIf your agent uses LangChain, you can write your chain in LangChain Expression Language. In your chain definition code, you can use an itemgetter to get the messages or query or history objects depending on your input format.\n\nSupported output schemas\n\nMosaic AI Agent Framework supports the following output schemas.\n\nChatCompletionResponse\n\n(Recommended) ChatCompletionResponse is recommended for customers with OpenAI response format interoperability.\n\nLangChain - ChatCompletionsOutputParser\n\nIf your agent uses LangChain, use ChatCompletionsOutputParser() from MLflow as your final chain step. This formats the LangChain AI message into an agent-compatible format.\n\nCopy\nPython\n\n  from mlflow.langchain.output_parsers import ChatCompletionsOutputParser\n\n  chain = (\n      {\n          \"user_query\": itemgetter(\"messages\")\n          | RunnableLambda(extract_user_query_string),\n          \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_chat_history),\n      }\n      | RunnableLambda(DatabricksChat)\n      | ChatCompletionsOutputParser()\n  )\n\nPyFunc - annotate input and output classes\n\nIf you are using PyFunc, Databricks recommends using type hints to annotate the predict() function with input and output data classes that are subclasses of classes defined in mlflow.models.rag_signatures.\n\nYou can construct an output object from the data class inside predict(). The returned object must be transformed into a dictionary representation to ensure it can be serialized.\n\nCopy\nPython\n\n  from mlflow.models.rag_signatures import ChatCompletionRequest, ChatCompletionResponse, ChainCompletionChoice, Message\n\n  class RAGModel(PythonModel):\n    ...\n      def predict(self, context, model_input: ChatCompletionRequest) -> ChatCompletionResponse:\n        ...\n        return asdict(ChatCompletionResponse(\n            choices=[ChainCompletionChoice(message=Message(content=text))]\n        ))\n\nExplicit and inferred signatures\n\nMLflow can infer the input and output schema of your agent at runtime and create a signature automatically. If you use supported input and output schemas, the inferred signatures are compatible with the Agent Framework. For information about supported schemas, see Supported input schemas.\n\nHowever, if you use a custom agent schema, you must explicitly define your Model Signature according to the instructions in Custom agent schemas.\n\nCustom agent schemas\n\nYou can customize an agent’s schema to pass and return additional fields to and from the agent by creating a subclass of a supported input/output schema. Then, add the extra keys custom_inputs and custom_outputs to contain the additional fields. See code examples for Pyfunc and Langchain and a UI-based method for using custom inputs.\n\nTo use the databricks-agents SDK, Databricks client UIs such as the AI Playground and the Review App, and other Mosaic AI Agent Framework features, your agent’s schema must fulfill the following requirements:\n\nThe agent must use mlflow version 2.17.1 or above.\n\nIn the agent notebook, mark additional fields added in your subclass as Optional and assign default values.\n\nIn the driver notebook, construct a ModelSignature using infer_signature with instances of your subclasses.\n\nIn the driver notebook, construct an input example by calling asdict on your subclass.\n\nPyFunc custom schemas\n\nIn addition to the requirements above, PyFunc-based agents must also meet the following requirements to interact with Mosaic AI agent features.\n\nPyFunc custom schema requirements\n\nIn the agent notebook, the predict and predict stream functions must meet the following requirements:\n\nHave type hints for your input subclass.\n\nUse dot notation to access dataclass fields (for example, use model_input.custom_input.id instead of model_input[\"custom_inputs\"]).\n\nReturn a dictionary. You can call asdict on an instance of your subclass to format the return as a dictionary.\n\nThe following notebooks show a custom schema example using PyFunc.\n\nPyFunc custom schema agent notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nPyFunc custom schema driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLangchain custom schemas\n\nThe following notebooks show a custom schema example using LangChain. You can modify the wrap_output function in the notebooks to parse and extract information from the message stream.\n\nLangchain custom schema agent notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nLangchain custom schema driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nProvide custom_inputs in the AI Playground and agent review app\n\nIf you define a custom agent schema with additional inputs using the custom_inputs field, you can manually provide these inputs in both the AI Playground and the agent review app. If no custom inputs are provided, the agent uses the default values specified in your schema.\n\nIn either the AI Playground or the Agent Review App, select the gear icon .\n\nEnable custom_inputs.\n\nProvide a JSON object that matches your agent’s defined input schema.\n\nThe JSON object must match the agent’s input schema. For example, if you have a custom_inputs dataclass defined as follows:\n\nCopy\nPython\n@dataclass\nclass CustomInputs():\n  id: int = 0\n  user: str = \"default\"\n\n\nThen the JSON string that you enter in the custom_inputs field must provide values for id and user, as shown in the following example:\n\nCopy\nJSON\n{\n  \"id\": 123\n  \"user\": \"dev_test\",\n}\n\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nSupported input schemas\nSupported output schemas\nExplicit and inferred signatures\nCustom agent schemas\nPyFunc custom schemas\nLangchain custom schemas\nProvide custom_inputs in the AI Playground and agent review app\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Deploy an agent for generative AI application | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/deploy-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Deploy an agent for generative AI application\nDeploy an agent for generative AI application\n\nNovember 19, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows how to deploy your AI agent using the deploy() function from the Agent Framework Python API.\n\nRequirements\n\nMLflow 2.13.1 or above to deploy agents using the the deploy() API from databricks.agents.\n\nRegister an AI agent to Unity Catalog. See Register the chain to Unity Catalog.\n\nInstall the the databricks-agents SDK.\n\nCopy\nPython\n%pip install databricks-agents\ndbutils.library.restartPython()\n\nDeploy an agent using deploy()\n\nThe deploy() function does the following:\n\nCreates CPU model serving endpoints for your agent that can be integrated into your user-facing application.\n\nTo reduce cost for idle endpoints (at the expense of increased time to serve initial queries), you can enable scale to zero for your serving endpoint by passing scale_to_zero_enabled=True to deploy(). See Endpoint scaling expectations.\n\nInference tables are enabled on these model serving endpoints. See Inference tables for monitoring and debugging models.\n\nAuthentication credentials are automatically passed to all Databricks-managed resources required by the agent as specified when logging the model. Databricks creates a service principal that has access to these resources, and automatically passes that into the endpoint. See Authentication for dependent resources.\n\nIf you have resource dependencies that are not Databricks-managed, for example using Pinecone, you can pass in environment variables with secrets to the deploy() API. See Configure access to resources from model serving endpoints.\n\nEnables the Review App for your agent. The Review App allows your stakeholders to chat with the agent and give feedback using the Review App UI.\n\nLogs every request to the Review App or REST API to an inference table. The data logged includes query requests, responses, and intermediate trace data from MLflow Tracing.\n\nCreates a feedback model with the same catalog and schema as the agent you are trying to deploy. This feedback model is the mechanism that makes it possible to accept feedback from the Review App and log it to an inference table. This model is served in the same CPU model serving endpoint as your deployed agent. Because this serving endpoint has inference tables enabled, it is possible to log feedback from the Review App to an inference table.\n\nNote\n\nDeployments can take up to 15 minutes to complete. Raw JSON payloads take 10 - 30 minutes to arrive, and the formatted logs are processed from the raw payloads about every hour.\n\nCopy\nPython\n\nfrom databricks.agents import deploy\nfrom mlflow.utils import databricks_utils as du\n\ndeployment = deploy(model_fqn, uc_model_info.version)\n\n# query_endpoint is the URL that can be used to make queries to the app\ndeployment.query_endpoint\n\n# Copy deployment.rag_app_url to browser and start interacting with your RAG application.\ndeployment.rag_app_url\n\nAgent-enhanced inference tables\n\nThe deploy() creates three inference tables for each deployment to log requests and responses to and from the agent serving endpoint. Users can expect the data to be in the payload table within an hour of interacting with their deployment.\n\nPayload request logs and assessment logs might take longer to populate, but are ultimately derived from the raw payload table. You can extract request and assessment logs from the payload table yourself. Deletions and updates to the payload table are not reflected in the payload request logs or the payload assessment logs.\n\nTable\n\n\t\n\nExample Unity Catalog table name\n\n\t\n\nWhat is in each table\n\n\n\n\nPayload\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload\n\n\t\n\nRaw JSON request and response payloads\n\n\n\n\nPayload request logs\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload_request_logs\n\n\t\n\nFormatted request and responses, MLflow traces\n\n\n\n\nPayload assessment logs\n\n\t\n\n{catalog_name}.{schema_name}.{model_name}_payload_assessment_logs\n\n\t\n\nFormatted feedback, as provided in the Review App, for each request\n\nThe following shows the schema for the request logs table.\n\nColumn name\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nclient_request_id\n\n\t\n\nString\n\n\t\n\nClient request ID, usually null.\n\n\n\n\ndatabricks_request_id\n\n\t\n\nString\n\n\t\n\nDatabricks request ID.\n\n\n\n\ndate\n\n\t\n\nDate\n\n\t\n\nDate of request.\n\n\n\n\ntimestamp_ms\n\n\t\n\nLong\n\n\t\n\nTimestamp in milliseconds.\n\n\n\n\ntimestamp\n\n\t\n\nTimestamp\n\n\t\n\nTimestamp of the request.\n\n\n\n\nstatus_code\n\n\t\n\nInteger\n\n\t\n\nStatus code of endpoint.\n\n\n\n\nexecution_time_ms\n\n\t\n\nLong\n\n\t\n\nTotal execution milliseconds.\n\n\n\n\nconversation_id\n\n\t\n\nString\n\n\t\n\nConversation id extracted from request logs.\n\n\n\n\nrequest\n\n\t\n\nString\n\n\t\n\nThe last user query from the user’s conversation. This is extracted from the RAG request.\n\n\n\n\nresponse\n\n\t\n\nString\n\n\t\n\nThe last response to the user. This is extracted from the RAG request.\n\n\n\n\nrequest_raw\n\n\t\n\nString\n\n\t\n\nString representation of request.\n\n\n\n\nresponse_raw\n\n\t\n\nString\n\n\t\n\nString representation of response.\n\n\n\n\ntrace\n\n\t\n\nString\n\n\t\n\nString representation of trace extracted from the databricks_options of response Struct.\n\n\n\n\nsampling_fraction\n\n\t\n\nDouble\n\n\t\n\nSampling fraction.\n\n\n\n\nrequest_metadata\n\n\t\n\nMap[String, String]\n\n\t\n\nA map of metadata related to the model serving endpoint associated with the request. This map contains the endpoint name, model name, and model version used for your endpoint.\n\n\n\n\nschema_version\n\n\t\n\nString\n\n\t\n\nInteger for the schema version.\n\nThe following is the schema for the assessment logs table.\n\nColumn name\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nrequest_id\n\n\t\n\nString\n\n\t\n\nDatabricks request ID.\n\n\n\n\nstep_id\n\n\t\n\nString\n\n\t\n\nDerived from retrieval assessment.\n\n\n\n\nsource\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the information on who created the assessment.\n\n\n\n\ntimestamp\n\n\t\n\nTimestamp\n\n\t\n\nTimestamp of request.\n\n\n\n\ntext_assessment\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the data for any feedback on the agent’s responses from the review app.\n\n\n\n\nretrieval_assessment\n\n\t\n\nStruct\n\n\t\n\nA struct field containing the data for any feedback on the documents retrieved for a response.\n\nPermission requirements for dependent resources\n\nWhen deploying a model with dependent resources, the creator of the endpoint must have the following permissions depending on the resource type:\n\nResource type\n\n\t\n\nPermission\n\n\n\n\nSql Warehouse\n\n\t\n\nUse Endpoint\n\n\n\n\nModel Serving Endpoint\n\n\t\n\nCan Query\n\n\n\n\nUnity Catalog Function\n\n\t\n\nExecute\n\n\n\n\nGenie space\n\n\t\n\nExecute\n\n\n\n\nVector Search Index\n\n\t\n\nReadVectorIndex\n\n\n\n\nUnity Catalog Table\n\n\t\n\nCan Read\n\nAuthentication for dependent resources\n\nWhen creating the model serving endpoint for agent deployment, Databricks verifies that the creator of the endpoint has permissions to access all resources on which the agent depends.\n\nFor LangChain flavored agents, dependent resources are automatically inferred during agent creation and logging. Those resources are logged in the resources.yaml file in the logged model artifact. During deployment, databricks.agents.deploy automatically creates the M2M OAuth tokens required to access and communicate with these inferred resource dependencies.\n\nFor PyFunc flavored agents, you must manually specify any resource dependencies during logging of the deployed agent in the resources parameter. See Specify resources for PyFunc or LangChain agent. During deployment, databricks.agents.deploy creates an M2M OAuth token with access to the resources specified in the resources parameter, and deploys it to the deployed agent.\n\nAutomatic authentication passthrough\n\nThe following table lists the features that support automatic authentication passthrough. Automatic authentication passthrough uses the credentials of the deployment creator to automatically authenticate against supported features.\n\nFeature\n\n\t\n\nMinimum mlflow version\n\n\n\n\nVector search indexes\n\n\t\n\nRequires mlflow 2.13.1 or above\n\n\n\n\nModel Serving endpoints\n\n\t\n\nRequires mlflow 2.13.1 or above\n\n\n\n\nSQL warehouses\n\n\t\n\nRequires mlflow 2.16.1 or above\n\n\n\n\nUnity Catalog Functions\n\n\t\n\nRequires mlflow 2.16.1 or above\n\nManual authentication\n\nIf you have a dependent resource that does not support automatic authentication passthrough, or if you want to use credentials other than those of the deployment creator, you can manually provide credentials using secrets-based environment variables. For example, if using the Databricks SDK in your agent to access other types of dependent resources, you can set the environment variables described in Databricks client unified authentication.\n\nGet deployed applications\n\nThe following shows how to get your deployed agents.\n\nCopy\nPython\nfrom databricks.agents import list_deployments, get_deployments\n\n# Get the deployment for specific model_fqn and version\ndeployment = get_deployments(model_name=model_fqn, model_version=model_version.version)\n\ndeployments = list_deployments()\n# Print all the current deployments\ndeployments\n\nProvide feedback on a deployed agent (experimental)\n\nWhen you deploy your agent with agents.deploy(), agent framework also creates and deploys a “feedback” model version within the same endpoint, which you can query to provide feedback on your agent application. Feedback entries appear as request rows within the inference table associated with your agent serving endpoint.\n\nNote that this behavior is experimental: Databricks may provide a first-class API for providing feedback on a deployed agent in the future, and future functionality may require migrating to this API.\n\nLimitations of this API include:\n\nThe feedback API lacks input validation - it always responds successfully, even if passed invalid input.\n\nThe feedback API requires passing in the Databricks-generated request_id of the agent endpoint request on which you wish to provide feedback. To get the databricks_request_id, include {\"databricks_options\": {\"return_trace\": True}} in your original request to the agent serving endpoint. The agent endpoint response will then include the databricks_request_id associated with the request, so that you can pass that request ID back to the feedback API when providing feedback on the agent response.\n\nFeedback is collected using inference tables. See inference table limitations.\n\nThe following example request provides feedback on the agent endpoint named “your-agent-endpoint-name”, and assumes that the DATABRICKS_TOKEN environment variable is set to a Databricks REST API token.\n\nCopy\nBash\ncurl \\\n  -u token:$DATABRICKS_TOKEN \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '\n      {\n          \"dataframe_records\": [\n              {\n                  \"source\": {\n                      \"id\": \"user@company.com\",\n                      \"type\": \"human\"\n                  },\n                  \"request_id\": \"573d4a61-4adb-41bd-96db-0ec8cebc3744\",\n                  \"text_assessments\": [\n                      {\n                          \"ratings\": {\n                              \"answer_correct\": {\n                                  \"value\": \"positive\"\n                              },\n                              \"accurate\": {\n                                  \"value\": \"positive\"\n                              }\n                          },\n                          \"free_text_comment\": \"The answer used the provided context to talk about Delta Live Tables\"\n                      }\n                  ],\n                  \"retrieval_assessments\": [\n                      {\n                          \"ratings\": {\n                              \"groundedness\": {\n                                  \"value\": \"positive\"\n                              }\n                          }\n                      }\n                  ]\n              }\n          ]\n      }' \\\nhttps://<workspace-host>.databricks.com/serving-endpoints/<your-agent-endpoint-name>/served-models/feedback/invocations\n\n\nYou can pass additional or different key-value pairs in the text_assessments.ratings and retrieval_assessments.ratings fields to provide different types of feedback. In the example, the feedback payload indicates that the agent’s response to the request with ID 573d4a61-4adb-41bd-96db-0ec8cebc3744 was correct, accurate, and grounded in context fetched by a retriever tool.\n\nAdditional resources\n\nWhat is Mosaic AI Agent Evaluation?\n\nGet feedback about the quality of an agentic application\n\nWas this article helpful?\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nDeploy an agent using deploy()\nAgent-enhanced inference tables\nPermission requirements for dependent resources\nAuthentication for dependent resources\nGet deployed applications\nProvide feedback on a deployed agent (experimental)\nAdditional resources\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Log and register AI agents | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/log-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Log and register AI agents\nLog and register AI agents\n\nOctober 29, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nLog AI agents using Mosaic AI Agent Framework. Logging an agent is the basis of the development process. Logging captures a “point in time” of the agent’s code and configuration so you can evaluate the quality of the configuration.\n\nRequirements\n\nCreate an AI agent before logging it.\n\nCode-based vs. serialization-based logging\n\nYou can use code-based MLflow logging or serialization-based MLflow logging. Databricks recommends that you use code-based logging.\n\nCode-based MLflow logging: The chain’s code is captured as a Python file. The Python environment is captured as a list of packages. When the chain is deployed, the Python environment is restored, and the chain’s code is executed to load the chain into memory so it can be invoked when the endpoint is called.\n\nSerialization-based MLflow logging: The chain’s code and current state in the Python environment is serialized to disk, often using libraries such as pickle or joblib. When the chain is deployed, the Python environment is restored, and the serialized object is loaded into memory so it can be invoked when the endpoint is called.\n\nThe table shows the advantages and disadvantages of each method.\n\nMethod\n\n\t\n\nAdvantages\n\n\t\n\nDisadvantages\n\n\n\n\nCode-based MLflow logging\n\n\t\n\nOvercomes inherent limitations of serialization, which is not supported by many popular GenAI libraries.\n\nSaves a copy of the original code for later reference.\n\nNo need to restructure your code into a single object that can be serialized.\n\n\t\n\nlog_model(...) must be called from a different notebook than the chain’s code (called a driver notebook).\n\n\n\n\nSerialization-based MLflow logging\n\n\t\n\nlog_model(...) can be called from the same notebook where the model is defined.\n\n\t\n\nOriginal code is not available.\n\nAll libraries and objects used in the chain must support serialization.\n\nFor code-based logging, the code that logs your agent or chain must be in a separate notebook from your chain code. This notebook is called a driver notebook. For an example notebook, see Example notebooks.\n\nCode-based logging with LangChain\n\nCreate a notebook or Python file with your code. For purposes of this example, the notebook or file is named chain.py. The notebook or file must contain a LangChain chain, referred to here as lc_chain.\n\nInclude mlflow.models.set_model(lc_chain) in the notebook or file.\n\nCreate a new notebook to serve as the driver notebook (called driver.py in this example).\n\nIn the driver notebook, use mlflow.lang_chain.log_model(lc_model=”/path/to/chain.py”)to run chain.py and log the results to an MLflow model.\n\nDeploy the model. See Deploy an agent for generative AI application. The deployment of your agent might depend on other Databricks resources such as a vector search index and model serving endpoints. For LangChain agents:\n\nThe MLflow log_model infers the dependencies required by the chain and logs them to the MLmodel file in the logged model artifact. Starting with Mlflow version 2.17.0, you can override these inferred dependencies. See Specify resources for PyFunc or LangChain agent.\n\nDuring deployment, databricks.agents.deploy automatically creates the M2M OAuth tokens required to access and communicate with these inferred resource dependencies.\n\nWhen the serving environment is loaded, chain.py is executed.\n\nWhen a serving request comes in, lc_chain.invoke(...) is called.\n\nCopy\nPython\n\nimport mlflow\n\ncode_path = \"/Workspace/Users/first.last/chain.py\"\nconfig_path = \"/Workspace/Users/first.last/config.yml\"\n\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Retrieval-augmented Generation?\",\n        }\n    ]\n}\n\n# example using LangChain\nwith mlflow.start_run():\n  logged_chain_info = mlflow.langchain.log_model(\n    lc_model=code_path,\n    model_config=config_path, # If you specify this parameter, this is the configuration that is used for training the model. The development_config is overwritten.\n    artifact_path=\"chain\", # This string is used as the path inside the MLflow model where artifacts are stored\n    input_example=input_example, # Must be a valid input to your chain\n    example_no_conversion=True, # Required\n  )\n\nprint(f\"MLflow Run: {logged_chain_info.run_id}\")\nprint(f\"Model URI: {logged_chain_info.model_uri}\")\n\n# To verify that the model has been logged correctly, load the chain and call `invoke`:\nmodel = mlflow.langchain.load_model(logged_chain_info.model_uri)\nmodel.invoke(example)\n\nCode-based logging with PyFunc\n\nCreate a notebook or Python file with your code. For purposes of this example, the notebook or file is named chain.py. The notebook or file must contain a PyFunc class, referred to here as PyFuncClass.\n\nInclude mlflow.models.set_model(PyFuncClass) in the notebook or file.\n\nCreate a new notebook to serve as the driver notebook (called driver.py in this example).\n\nIn the driver notebook, use mlflow.pyfunc.log_model(python_model=”/path/to/chain.py”, resources=”/path/to/resources.yaml”) to run chain.py and log the results to an MLflow model. The resources parameter declares any resources needed to serve the model such as a vector search index or serving endpoint that serves a foundation model. For an example resources file for PyFunc, see Specify resources for PyFunc or LangChain agent.\n\nDeploy the model. See Deploy an agent for generative AI application.\n\nWhen the serving environment is loaded, chain.py is executed.\n\nWhen a serving request comes in, PyFuncClass.predict(...) is called.\n\nCopy\nPython\nimport mlflow\n\ncode_path = \"/Workspace/Users/first.last/chain.py\"\nconfig_path = \"/Workspace/Users/first.last/config.yml\"\n\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Retrieval-augmented Generation?\",\n        }\n    ]\n}\n\n# example using PyFunc model\n\nresources_path = \"/Workspace/Users/first.last/resources.yml\"\n\nwith mlflow.start_run():\n  logged_chain_info = mlflow.pyfunc.log_model(\n    python_model=chain_notebook_path,\n    artifact_path=\"chain\",\n    input_example=input_example,\n    resources=resources_path,\n    example_no_conversion=True,\n  )\n\nprint(f\"MLflow Run: {logged_chain_info.run_id}\")\nprint(f\"Model URI: {logged_chain_info.model_uri}\")\n\n# To verify that the model has been logged correctly, load the chain and call `invoke`:\nmodel = mlflow.pyfunc.load_model(logged_chain_info.model_uri)\nmodel.invoke(example)\n\nSpecify resources for PyFunc or LangChain agent\n\nYou can specify resources, such as a vector search index and a serving endpoint, that are required to serve the model.\n\nFor LangChain, resources are automatically detected and logged with the model using a best-effort approach. Starting with MLflow version 2.17.0, you can override these automatically inferred resources using code similar to that shown below. This is recommended for production use cases as it allows you to ensure that agents are logged with the necessary dependencies.\n\nWhen deploying a pyfunc flavored agent, you must manually add any resource dependencies of the deployed agent. An M2M OAuth token with access to all the specified resources in the resources parameter is created and provided to the deployed agent.\n\nNote\n\nYou can override the resources your endpoint has permission to by manually specifying the resources when logging the chain.\n\nThe following code specifies dependencies using the resources parameter.\n\nCopy\nPython\nimport mlflow\nfrom mlflow.models.resources import (\n    DatabricksFunction,\n    DatabricksServingEndpoint,\n    DatabricksSQLWarehouse,\n    DatabricksVectorSearchIndex,\n)\n\nwith mlflow.start_run():\n  logged_chain_info = mlflow.pyfunc.log_model(\n    python_model=chain_notebook_path,\n    artifact_path=\"chain\",\n    input_example=input_example,\n    example_no_conversion=True,\n    resources=[\n      DatabricksServingEndpoint(endpoint_name=\"databricks-mixtral-8x7b-instruct\"),\n      DatabricksServingEndpoint(endpoint_name=\"databricks-bge-large-en\"),\n      DatabricksVectorSearchIndex(index_name=\"prod.agents.databricks_docs_index\"),\n      DatabricksSQLWarehouse(warehouse_id=\"your_warehouse_id\"),\n      DatabricksFunction(function_name=\"ml.tools.python_exec\"),\n    ]\n  )\n\n\nYou can also add resources by specifying them in a resources.yaml file. You can reference that file path in the resources parameter. An M2M OAuth token with access to all the specified resources in the resources.yaml is created and provided to the deployed agent.\n\nThe following is an example resources.yaml file that defines model serving endpoints and a vector search index.\n\nCopy\nYAML\napi_version: \"1\"\ndatabricks:\n  vector_search_index:\n    - name: \"catalog.schema.my_vs_index\"\n  serving_endpoint:\n    - name: databricks-dbrx-instruct\n    - name: databricks-bge-large-en\n\nRegister the chain to Unity Catalog\n\nBefore you deploy the chain, you must register the chain to Unity Catalog. When you register the chain, it is packaged as a model in Unity Catalog, and you can use Unity Catalog permissions for authorization for resources in the chain.\n\nCopy\nPython\nimport mlflow\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog_name = \"test_catalog\"\nschema_name = \"schema\"\nmodel_name = \"chain_name\"\n\nmodel_name = catalog_name + \".\" + schema_name + \".\" + model_name\nuc_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=model_name)\n\nNext steps\n\nAdd traces to an AI agent.\n\nDeploy an AI agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nCode-based vs. serialization-based logging\nCode-based logging with LangChain\nCode-based logging with PyFunc\nRegister the chain to Unity Catalog\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Create an AI agent | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/create-agent.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nCreate AI agent tools\nCreate a retriever tool\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks  Create an AI agent\nCreate an AI agent\n\nNovember 08, 2024\n\nPreview\n\nThis feature is in Public Preview.\n\nThis article shows you how to create a tool-calling AI agent using the Mosaic AI Agent Framework.\n\nLearn how to give an agent tools and start chatting with them to test and prototype the agent. Once you’re done prototyping the agent, export the Python code that defines the agent to iterate and deploy your AI agent.\n\nRequirements\n\nUnderstand the concepts of AI agents and tools as described in What are compound AI systems and AI agents?\n\nDatabricks recommends installing the latest version of the MLflow Python client when developing agents. See Authentication for dependent resources for information on mlflow version requirements.\n\nCreate AI agent tools\n\nThe first step is to create a tool to give to your agent. Agents use tools to perform actions besides language generation, for example to retrieve structured or unstructured data, execute code, or talk to remote services (e.g. send an email or Slack message).\n\nTo learn more about creating agent tools, see Create AI agent tools.\n\nFor this guide, create a Unity Catalog function that executes Python code. An agent can use this tool to run Python given them by a user, or written by the agent itself.\n\nRun the following code in a notebook cell. It uses the %sql notebook magic to create a Unity Catalog function called python_exec.\n\nCopy\nSQL\n%sql\nCREATE OR REPLACE FUNCTION\nmain.default.python_exec (\ncode STRING COMMENT 'Python code to execute. Remember to print the final result to stdout.'\n)\nRETURNS STRING\nLANGUAGE PYTHON\nDETERMINISTIC\nCOMMENT 'Executes Python code in the sandboxed environment and returns its stdout. The runtime is stateless and you can not read output of the previous tool executions. i.e. No such variables \"rows\", \"observation\" defined. Calling another tool inside a Python code is NOT allowed. Use standard python libraries only.'\nAS $$\nimport sys\nfrom io import StringIO\nsys_stdout = sys.stdout\nredirected_output = StringIO()\nsys.stdout = redirected_output\nexec(code)\nsys.stdout = sys_stdout\nreturn redirected_output.getvalue()\n$$\n\nPrototype tool-calling agents in AI Playground\n\nAfter creating the Unity Catalog function, use the AI Playground to give the tool to an LLM and test the agent. The AI Playground provides a sandbox to prototype tool-calling agents.\n\nOnce you’re happy with the AI agent, you can export it to develop it further in Python or deploy it as a Model Serving endpoint as is.\n\nNote\n\nUnity Catalog, and serverless compute, Mosaic AI Agent Framework, and either pay-per-token foundation models or external models must be available in the current workspace to prototype agents in AI Playground.\n\nTo prototype a tool-calling endpoint.\n\nFrom Playground, select a model with the Tools enabled label.\n\nSelect Tools and specify your Unity Catalog function names in the dropdown:\n\nChat to test out the current combination of LLM, tools, and system prompt, and try variations.\n\nExport and deploy AI Playground agents\n\nAfter adding tools and testing the agent, export the Playground agent to Python notebooks:\n\nClick Export to generate Python notebooks that help you develop and deploy the AI agent.\n\nAfter exporting the agent code, you see three files saved to your workspace:\n\nagent notebook: Contains Python code defining your agent using LangChain.\n\ndriver notebook: Contains Python code to log, trace, register, and deploy the AI agent using Mosaic AI Agent Framework.\n\nconfig.yml: Contains configuration information about your agent including tool definitions.\n\nOpen the agent notebook to see the LangChain code defining your agent, use this notebook to test and iterate on the agent programmatically such as defining more tools or adjusting the agent’s parameters.\n\nNote\n\nThe exported code might have different behavior from your AI playground session. Databricks recommends that you run the exported notebooks to iterate and debug further, evaluate agent quality, and then deploy the agent to share with others.\n\nOnce you’re happy with the agent’s outputs, you can run the driver notebook to log and deploy your agent to a Model Serving endpoint.\n\nDefine an agent in code\n\nIn addition to generating agent code from AI Playground, you can also define an agent in code yourself, using frameworks like LangChain or Python code. In order to deploy an agent using Agent Framework, its input must conform to one of the supported input and output formats.\n\nUse parameters to configure the agent\n\nIn the Agent Framework, you can use parameters to control how agents are executed. This allows you to quickly iterate by varying characteristics of your agent without changing the code. Parameters are key-value pairs that you define in a Python dictionary or a .yaml file.\n\nTo configure the code, create a ModelConfig, a set of key-value parameters. ModelConfig is either a Python dictionary or a .yaml file. For example, you can use a dictionary during development and then convert it to a .yaml file for production deployment and CI/CD. For details about ModelConfig, see the MLflow documentation.\n\nAn example ModelConfig is shown below.\n\nCopy\nYAML\nllm_parameters:\n  max_tokens: 500\n  temperature: 0.01\nmodel_serving_endpoint: databricks-dbrx-instruct\nvector_search_index: ml.docs.databricks_docs_index\nprompt_template: 'You are a hello world bot. Respond with a reply to the user''s\n  question that indicates your prompt template came from a YAML file. Your response\n  must use the word \"YAML\" somewhere. User''s question: {question}'\nprompt_template_input_vars:\n- question\n\n\nTo call the configuration from your code, use one of the following:\n\nCopy\nPython\n# Example for loading from a .yml file\nconfig_file = \"configs/hello_world_config.yml\"\nmodel_config = mlflow.models.ModelConfig(development_config=config_file)\n\n# Example of using a dictionary\nconfig_dict = {\n    \"prompt_template\": \"You are a hello world bot. Respond with a reply to the user's question that is fun and interesting to the user. User's question: {question}\",\n    \"prompt_template_input_vars\": [\"question\"],\n    \"model_serving_endpoint\": \"databricks-dbrx-instruct\",\n    \"llm_parameters\": {\"temperature\": 0.01, \"max_tokens\": 500},\n}\n\nmodel_config = mlflow.models.ModelConfig(development_config=config_dict)\n\n# Use model_config.get() to retrieve a parameter value\nvalue = model_config.get('sample_param')\n\nSet retriever schema\n\nAI agents often use retrievers, a type of agent tool that finds and returns relevant documents using a Vector Search index. For more information on retrievers, see Create a vector search retriever tool.\n\nTo ensure that retrievers are traced properly, call mlflow.models.set_retriever_schema when you define your agent in code. Use set_retriever_schema to map the column names in the returned table to MLflow’s expected fields such as primary_key, text_column, and doc_uri.\n\nCopy\nPython\n# Define the retriever's schema by providing your column names\n# These strings should be read from a config dictionary\nmlflow.models.set_retriever_schema(\n    name=\"vector_search\",\n    primary_key=\"chunk_id\",\n    text_column=\"text_column\",\n    doc_uri=\"doc_uri\"\n    # other_columns=[\"column1\", \"column2\"],\n)\n\n\nNote\n\nThe doc_uri column is especially important when evaluating the retriever’s performance. doc_uri is the main identifier for documents returned by the retriever, allowing you to compare them against ground truth evaluation sets. See Evaluation sets\n\nYou can also specify additional columns in your retriever’s schema by providing a list of column names with the other_columns field.\n\nIf you have multiple retrievers, you can define multiple schemas by using unique names for each retriever schema.\n\nSupported input and output formats\n\nAgent Framework uses MLflow Model Signatures to define input and output schemas for agents. Mosaic AI Agent Framework features require a minimum set of input/output fields to interact with features such as the Review App and the AI Playground. For more information, see Define an agent’s input and output schema.\n\nExample notebooks\n\nThese notebooks create a simple “Hello, world” chain to illustrate how to create a chain application in Databricks. The first example creates a simple chain. The second example notebook illustrates how to use parameters to minimize code changes during development.\n\nSimple chain notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nSimple chain driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nParameterized chain notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nParameterized chain driver notebook\n\nOpen notebook in new tab\n Copy link for import\n\nLoading notebook...\nNext steps\n\nLog an AI agent.\n\nAdd traces to an AI agent.\n\nDeploy an AI agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nRequirements\nCreate AI agent tools\nPrototype tool-calling agents in AI Playground\nExport and deploy AI Playground agents\nDefine an agent in code\nUse parameters to configure the agent\nSupported input and output formats\nExample notebooks\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "Introduction to building gen AI apps on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/generative-ai/agent-framework/build-genai-apps.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nCreate AI agents\nTrace agents\nLog and register agents\nDeploy agents\nDefine agent schemas\nCompound AI systems and AI agents\nRAG on Databricks\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks  Introduction to building gen AI apps on Databricks\nIntroduction to building gen AI apps on Databricks\n\nNovember 08, 2024\n\nMosaic AI provides a comprehensive platform to build, deploy, and manage GenAI applications. This article guides you through the essential components and processes involved in developing GenAI applications on Databricks.\n\nDeploy and query gen AI models\n\nFor simple use cases, you can directly serve and query gen AI models, including high quality open-source models, as well as third-party models from LLM providers such as OpenAI and Anthropic.\n\nMosaic AI Model Serving supports serving and querying generative AI models using the following capabilities:\n\nFoundation Model APIs. This functionality makes state-of-the-art open models and fine-tuned model variants available to your model serving endpoint. These models are curated foundation model architectures that support optimized inference. Base models, like DBRX Instruct, Meta-Llama-3.1-70B-Instruct, GTE-Large, and Mistral-7B are available for immediate use with pay-per-token pricing, and workloads that require performance guarantees, like fine-tuned model variants, can be deployed with provisioned throughput.\n\nExternal models. These are generative AI models that are hosted outside of Databricks. Endpoints that serve external models can be centrally governed and customers can establish rate limits and access control for them. Examples include foundation models like OpenAI’s GPT-4, Anthropic’s Claude, and others.\n\nSee Create generative AI model serving endpoints.\n\nMosaic AI Agent Framework\n\nMosaic AI Agent Framework comprises a set of tools on Databricks designed to help developers build, deploy, and evaluate production-quality agents like Retrieval Augmented Generation (RAG) applications.\n\nIt is compatible with third-party frameworks like LangChain and LlamaIndex, allowing you to develop with your preferred framework and while leveraging Databricks’ managed Unity Catalog, Agent Evaluation Framework, and other platform benefits.\n\nQuickly iterate on agent development using the following features:\n\nCreate and log agents using any library and MLflow. Parameterize your agents to experiment and iterate on agent development quickly.\n\nAgent tracing lets you log, analyze, and compare traces across your agent code to debug and understand how your agent responds to requests.\n\nImprove agent quality using DSPy. DSPy can automate prompt engineering and fine-tuning to improve the quality of your GenAI agents.\n\nDeploy agents to production with native support for token streaming and request/response logging, plus a built-in review app to get user feedback for your agent.\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nDeploy and query gen AI models\nMosaic AI Agent Framework\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  },
  {
    "title": "AI and machine learning on Databricks | Databricks on AWS",
    "url": "https://docs.databricks.com/en/machine-learning/index.html",
    "html": "Help Center\nDocumentation\nKnowledge Base\nCOMMUNITY\nSUPPORT\nFEEDBACK\nTRY DATABRICKS\nEnglish\nAmazon Web Services\nDatabricks on AWS\n\nGET STARTED\n\nGet started\nWhat is Databricks?\nDatabricksIQ\nRelease notes\n\nLOAD & MANAGE DATA\n\nWork with database objects\nConnect to data sources\nConnect to compute\nDiscover data\nQuery data\nIngest data\nWork with files\nTransform data\nSchedule and orchestrate workflows\nMonitor data and AI assets\nShare data securely\n\nWORK WITH DATA\n\nData engineering\nAI and machine learning\nTutorials\nAI playground\nAI functions in SQL\nServe models\nTrain models\nServe data for AI\nEvaluate AI\nBuild gen AI apps\nMLOps and MLflow\nGen AI model maintenance policy\nIntegrations\nGraph and network analysis\nReference solutions\nGenerative AI tutorial\nBusiness intelligence\nData warehousing\nNotebooks\nDelta Lake\nDevelopers\nTechnology partners\n\nADMINISTRATION\n\nAccount and workspace administration\nSecurity and compliance\nData governance (Unity Catalog)\nLakehouse architecture\n\nREFERENCE & RESOURCES\n\nReference\nResources\nWhat’s coming?\nDocumentation archive\n\nUpdated Dec 02, 2024\n\nSend us feedback\n\nDocumentation  AI and machine learning on Databricks\nAI and machine learning on Databricks\n\nNovember 15, 2024\n\nThis article describes the tools that Mosaic AI (formerly Databricks Machine Learning) provides to help you build AI and ML systems. The diagram shows how various products on Databricks platform help you implement your end to end workflows to build and deploy AI and ML systems\n\nGenerative AI on Databricks\n\nMosaic AI unifies the AI lifecycle from data collection and preparation, to model development and LLMOps, to serving and monitoring. The following features are specifically optimized to facilitate the development of generative AI applications:\n\nUnity Catalog for governance, discovery, versioning, and access control for data, features, models, and functions.\n\nMLflow for model development tracking.\n\nMosaic AI Model Serving for deploying LLMs. You can configure a model serving endpoint specifically for accessing generative AI models:\n\nState-of-the-art open LLMs using Foundation Model APIs.\n\nThird-party models hosted outside of Databricks. See External models in Mosaic AI Model Serving.\n\nMosaic AI Vector Search provides a queryable vector database that stores embedding vectors and can be configured to automatically sync to your knowledge base.\n\nLakehouse Monitoring for data monitoring and tracking model prediction quality and drift using automatic payload logging with inference tables.\n\nAI Playground for testing generative AI models from your Databricks workspace. You can prompt, compare and adjust settings such as system prompt and inference parameters.\n\nFoundation Model Fine-tuning (now part of Mosaic AI Model Training) for customizing a foundation model using your own data to optimize its performance for your specific application.\n\nMosaic AI Agent Framework for building and deploying production-quality agents like Retrieval Augmented Generation (RAG) applications.\n\nMosaic AI Agent Evaluation for evaluating the quality, cost, and latency of generative AI applications, including RAG applications and chains.\n\nWhat is generative AI?\n\nGenerative AI is a type of artificial intelligence focused on the ability of computers to use models to create content like images, text, code, and synthetic data.\n\nGenerative AI applications are built on top of generative AI models: large language models (LLMs) and foundation models.\n\nLLMs are deep learning models that consume and train on massive datasets to excel in language processing tasks. They create new combinations of text that mimic natural language based on their training data.\n\nGenerative AI models or foundation models are large ML models pre-trained with the intention that they are to be fine-tuned for more specific language understanding and generation tasks. These models are used to discern patterns within the input data.\n\nAfter these models have completed their learning processes, together they generate statistically probable outputs when prompted and they can be employed to accomplish various tasks, including:\n\nImage generation based on existing ones or utilizing the style of one image to modify or create a new one.\n\nSpeech tasks such as transcription, translation, question/answer generation, and interpretation of the intent or meaning of text.\n\nImportant\n\nWhile many LLMs or other generative AI models have safeguards, they can still generate harmful or inaccurate information.\n\nGenerative AI has the following design patterns:\n\nPrompt Engineering: Crafting specialized prompts to guide LLM behavior\n\nRetrieval Augmented Generation (RAG): Combining an LLM with external knowledge retrieval\n\nFine-tuning: Adapting a pre-trained LLM to specific data sets of domains\n\nPre-training: Training an LLM from scratch\n\nMachine learning on Databricks\n\nWith Mosaic AI, a single platform serves every step of ML development and deployment, from raw data to inference tables that save every request and response for a served model. Data scientists, data engineers, ML engineers and DevOps can do their jobs using the same set of tools and a single source of truth for the data.\n\nMosaic AI unifies the data layer and ML platform. All data assets and artifacts, such as models and functions, are discoverable and governed in a single catalog. Using a single platform for data and models makes it possible to track lineage from the raw data to the production model. Built-in data and model monitoring saves quality metrics to tables that are also stored in the platform, making it easier to identify the root cause of model performance problems. For more information about how Databricks supports the full ML lifecycle and MLOps, see MLOps workflows on Databricks and MLOps Stacks: model development process as code.\n\nSome of the key components of the data intelligence platform are:\n\nTasks\n\n\t\n\nComponent\n\n\n\n\nGovern and manage data, features, models, and functions. Also discovery, versioning, and lineage.\n\n\t\n\nUnity Catalog\n\n\n\n\nTrack changes to data, data quality, and model prediction quality\n\n\t\n\nLakehouse Monitoring, Inference tables\n\n\n\n\nFeature development and management\n\n\t\n\nFeature engineering and serving.\n\n\n\n\nTrain models\n\n\t\n\nAutoML, Databricks notebooks\n\n\n\n\nTrack model development\n\n\t\n\nMLflow tracking\n\n\n\n\nServe custom models\n\n\t\n\nMosaic AI Model Serving.\n\n\n\n\nBuild automated workflows and production-ready ETL pipelines\n\n\t\n\nDatabricks Jobs\n\n\n\n\nGit integration\n\n\t\n\nDatabricks Git folders\n\nDeep learning on Databricks\n\nConfiguring infrastructure for deep learning applications can be difficult. Databricks Runtime for Machine Learning takes care of that for you, with clusters that have built-in compatible versions of the most common deep learning libraries like TensorFlow, PyTorch, and Keras.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. It also supports libraries like Ray to parallelize compute processing for scaling ML workflows and ML applications.\n\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. Mosaic AI Model Serving enables creation of scalable GPU endpoints for deep learning models with no extra configuration.\n\nFor machine learning applications, Databricks recommends using a cluster running Databricks Runtime for Machine Learning. See Create a cluster using Databricks Runtime ML.\n\nTo get started with deep learning on Databricks, see:\n\nBest practices for deep learning on Databricks\n\nDeep learning on Databricks\n\nReference solutions for deep learning\n\nNext steps\n\nTo get started, see:\n\nTutorials: Get started with AI and machine learning\n\nFor a recommended MLOps workflow on Databricks Mosaic AI, see:\n\nMLOps workflows on Databricks\n\nTo learn about key Databricks Mosaic AI features, see:\n\nWhat is AutoML?\n\nFeature engineering and serving\n\nModel serving with Databricks\n\nLakehouse Monitoring\n\nManage model lifecycle\n\nMLflow experiment tracking\n\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\n\nSend us feedback | Privacy Notice | Terms of Use | Modern Slavery Statement | California Privacy | Your Privacy Choices \n\nIn this article:\nGenerative AI on Databricks\nMachine learning on Databricks\nDeep learning on Databricks\nNext steps\nWe Care About Your Privacy\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. Click “Accept All” to enable all cookies or “Reject All” to reject cookies. You can also manage your cookie settings by clicking “Manage Preferences.”\nReject All Accept All\nManage Preferences"
  }
]